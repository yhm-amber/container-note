[datafusion.site/apache.org]: https://datafusion.apache.org/ "DataFusion is an extensible query engine written in Rust that uses Apache Arrow as its in-memory format. // DataFusion 是一个用 Rust 编写的可扩展查询引擎，它使用 Apache Arrow 作为其内存中的格式。 /// “Out of the box,” DataFusion offers SQL and Dataframe APIs, excellent performance, built-in support for CSV, Parquet, JSON, and Avro, extensive customization, and a great community. Python Bindings are also available. Ballista is Apache DataFusion extension enabling the parallelized execution of workloads across multiple nodes in a distributed environment. // “开箱即用”的情况下，DataFusion 提供了 SQL 和 Dataframe API，出色的性能，对 CSV、Parquet、JSON 和 Avro 的内置支持，广泛的定制功能，以及一个优秀的社区。Python 绑定也是可用的。Ballista 是 Apache DataFusion 的扩展，能够在分布式环境中的多个节点上并行执行工作负载。"
[datafusion<arrow-mem>.src/gh]: https://github.com/apache/datafusion.git "(Apache-2.0) (Languages: Rust 99.2%, Other 0.8%) Apache DataFusion SQL Query Engine // Apache DataFusion SQL 查询引擎 /// DataFusion is great for building projects such as domain specific query engines, new database platforms and data pipelines, query languages and more. It lets you start quickly from a fully working engine, and then customize those features specific to your use. // DataFusion 非常适合构建特定领域的查询引擎、新的数据库平台和数据管道、查询语言等。它让你能够从一个完全可工作的引擎快速开始，然后根据你的使用需求定制这些功能。"
[datafusion-engine.cargo/crates]: https://crates.io/crates/datafusion "(: cargo install -- datafusion) (Apache-2.0) (20K SLoC) (250 KiB) DataFusion is an in-memory query engine that uses Apache Arrow as the memory model // DataFusion 是一个使用 Apache Arrow 作为内存模型的内存查询引擎 (src: gh:apache/datafusion.git)"
[datafusion-cli.cargo/crates]: https://crates.io/crates/datafusion "(: cargo install -- datafusion-cli) (Apache-2.0) (3.9K SLoC) (75.6 KiB) Command Line Client for DataFusion query engine. // DataFusion 查询引擎的命令行客户端。 (src: gh:apache/datafusion.git)"
[datafusion-functions.cargo/crates]: https://crates.io/crates/datafusion "(: cargo add -- datafusion-functions) (Apache-2.0) (27K SLoC) (202 KiB) Function packages for the DataFusion query engine // DataFusion 查询引擎的函数包 /// DataFusion is an extensible query execution framework, written in Rust, that uses Apache Arrow as its in-memory format. // DataFusion 是一个用 Rust 编写的可扩展查询执行框架，它使用 Apache Arrow 作为其内存中的格式。 /// This crate contains packages of function that can be used to customize the functionality of DataFusion. // 这个 crates 包含可以用来定制 DataFusion 功能的函数包。 /// Most projects should use the datafusion crate directly, which re-exports this module. If you are already using the datafusion crate, there is no reason to use this crate directly in your project as well. // 大多数项目应该直接使用 datafusion 这个 crates，它重新导出了这个模块。如果你已经在使用 datafusion 这个 crates，那么在你的项目中也没有理由直接使用这个 crates。 (src: gh:apache/datafusion.git)"
[datafusion-python<rust-impl>.src/gh]: https://github.com/apache/datafusion-python/ "(Apache-2.0) (Languages: Python 55.9%, Rust 41.7%, Shell 2.0%, Dockerfile 0.4%) Apache DataFusion Python Bindings // Apache DataFusion Python 绑定 /// This is a Python library that binds to Apache Arrow in-memory query engine DataFusion. // 这是一个与 Apache Arrow 内存查询引擎 DataFusion 绑定的 Python 库。 /// DataFusion's Python bindings can be used as a foundation for building new data systems in Python. Here are some examples: // DataFusion 的 Python 绑定可以作为构建新数据系统的基础。以下是一些示例： /// - Dask SQL uses DataFusion's Python bindings for SQL parsing, query planning, and logical plan optimizations, and then transpiles the logical plan to Dask operations for execution. // Dask SQL 使用 DataFusion 的 Python 绑定进行 SQL 解析、查询规划以及逻辑计划优化，然后将逻辑计划转换为 Dask 操作以执行。 /// - DataFusion Ballista is a distributed SQL query engine that extends DataFusion's Python bindings for distributed use cases. // DataFusion Ballista 是一个分布式 SQL 查询引擎，扩展了 DataFusion 的 Python 绑定以支持分布式用例。 /// - DataFusion Ray is another distributed query engine that uses DataFusion's Python bindings. // DataFusion Ray 是另一个使用 DataFusion 的 Python 绑定的分布式查询引擎。 /// The following example demonstrates running a SQL query against a Parquet file using DataFusion, storing the results in a Pandas DataFrame, and then plotting a chart. // 以下示例演示了使用 DataFusion 对 Parquet 文件运行 SQL 查询，将结果存储在 Pandas DataFrame 中，然后绘制图表。 ///:: (: from datafusion import SessionContext ;: ctx = SessionContext() # Create a DataFusion context ;: ctx.register_parquet('pokemon', 'pokemon.parquet') # Register table with context ;: ctx.sql('SELECT `Attack`+`Defense`, `Attack`-`Defense` FROM pokemon') # create a new statement via SQL ;: pandas_df = df.to_pandas() # collect and convert to pandas DataFrame ;: fig = pandas_df.plot(kind='bar', title='Pokemon Attack Defense').get_figure() # create a chart ;: fig.savefig('chart.png') # This produces a chart.png picture) (: data = {'a': [1, 2, 3, 4, 5], 'b': [10, 20, 30, 40, 50]} ;: df = ctx.from_pydict(data, 'dict_table_A') ;: from datafusion import col,literal ;: ctx.register_view('view_filtered_A', df.filter(col('a') > literal(2))) # Register the dataframe that keep rows where `a > 2` as a view with the context ;: df_fromview_A = ctx.sql('SELECT * FROM view_filtered_A') # run a SQL query against the registered view ;: results = df_view.collect() # Collect the results ;: result_dicts = [batch.to_pydict() for batch in results] # Convert results to a list of dictionaries for display ;: print(result_dicts) #> [{'a': [3, 4, 5], 'b': [30, 40, 50]}] # you can see only keep rows that `where a > 2`.) /// Features // 特性 /// - Execute queries using SQL or DataFrames against CSV, Parquet, and JSON data sources. // 使用 SQL 或 DataFrame 对 CSV、Parquet 和 JSON 数据源执行查询。 /// - Queries are optimized using DataFusion's query optimizer. // 使用 DataFusion 的查询优化器优化查询。 /// - Execute user-defined Python code from SQL. // 从 SQL 执行用户定义的 Python 代码。 /// - Exchange data with Pandas and other DataFrame libraries that support PyArrow. // 与支持 PyArrow 的 Pandas 和其他 DataFrame 库交换数据。 /// - Serialize and deserialize query plans in Substrait format. // 以 Substrait 格式序列化和反序列化查询计划。 /// - Experimental support for transpiling SQL queries to DataFrame calls with Polars, Pandas, and cuDF. // 实验性支持将 SQL 查询转换为使用 Polars、Pandas 和 cuDF 的 DataFrame 调用。 /// For tips on tuning parallelism, see Maximizing CPU Usage in the configuration guide. // 有关调整并行性的提示，请参阅配置指南中的“最大化 CPU 使用”。 ///:: (: from datafusion import SessionConfig ;: config = SessionConfig().with_target_partitions(16) # allow up to 16 concurrent partitions ;: SessionConfig().with_target_partitions(16).with_repartition_joins(True).with_repartition_aggregations(True).with_repartition_windows(True) # Automatic repartitioning for joins, aggregations, window functions and other operations can be enabled to increase parallelism // 可以启用对连接、聚合、窗口函数和其他操作的自动重新分区来增加并行性) (: ctx = SessionContext(config) ;: df = ctx.read_parquet('data.parquet') ;: df = df.repartition(16) # Evenly divide into 16 partitions ;: from datafusion import col ;: df = df.repartition_by_hash(col('a'), num=16) # Or partition by the hash of a column ;: result = df.collect() # --- Manual repartitioning is available on DataFrames when you need precise control // —— 手动重分区在需要精确控制时适用于 DataFrame)"
[datafusion-python<rust-impl>.pip/pypi]: https://pypi.org/project/datafusion/ "(: pip install -- datafusion) (Apache-2.0) (src: gh:apache/datafusion-python.git)"
[datafusion-python<rust-impl>.site/.site]: https://datafusion.apache.org/python/ "DataFusion in Python // Python 中的 DataFusion ¶ /// This is a Python library that binds to Apache Arrow in-memory query engine DataFusion. // 这是一个与 Apache Arrow 内存查询引擎 DataFusion 绑定的 Python 库。 /// Like pyspark, it allows you to build a plan through SQL or a DataFrame API against in-memory data, parquet or CSV files, run it in a multi-threaded environment, and obtain the result back in Python. // 类似于 pyspark，它允许你通过 SQL 或 DataFrame API 针对内存中的数据、Parquet 或 CSV 文件构建计划，在多线程环境中运行它，并将结果返回到 Python 中。 /// It also allows you to use UDFs and UDAFs for complex operations. // 它还允许你使用 UDFs 和 UDAFs 进行复杂操作。 /// The major advantage of this library over other execution engines is that this library achieves zero-copy between Python and its execution engine: there is no cost in using UDFs, UDAFs, and collecting the results to Python apart from having to lock the GIL when running those operations. // 该库相对于其他执行引擎的主要优势在于，它实现了 Python 与其执行引擎之间的零拷贝：在使用 UDFs、UDAFs 以及将结果收集到 Python 时，除了运行这些操作时需要锁定 GIL 外，没有额外成本。 /// Its query engine, DataFusion, is written in Rust, which makes strong assumptions about thread safety and lack of memory leaks. // 它的查询引擎 DataFusion 是用 Rust 编写的，这使得它在线程安全性和无内存泄漏方面做出了强假设。 /// Technically, zero-copy is achieved via the [c data interface](https://arrow.apache.org/docs/format/CDataInterface.html 'The Arrow C data interface // The Arrow C 数据接口'). // 在技术上，零拷贝是通过 c 数据接口实现的。 ///... # The Arrow C data interface // The Arrow C 数据接口 /// ## Rationale // 理由 /// Apache Arrow is designed to be a universal in-memory format for the representation of tabular (“columnar”) data. However, some projects may face a difficult choice between either depending on a fast-evolving project such as the Arrow C++ library, or having to reimplement adapters for data interchange, which may require significant, redundant development effort. // Apache Arrow 旨在成为一种通用的内存格式，用于表示表格（“列式”）数据。然而，一些项目可能需要在依赖快速发展的项目（如 Arrow C++库）或重新实现数据交换适配器之间做出艰难的选择，后者可能需要大量重复的开发工作。 /// The Arrow C data interface defines a very small, stable set of C definitions that can be easily copied in any project’s source code and used for columnar data interchange in the Arrow format. For non-C/C++ languages and runtimes, it should be almost as easy to translate the C definitions into the corresponding C FFI declarations. // Arrow C 数据接口定义了一组非常小且稳定的 C 定义，可以轻松地复制到任何项目的源代码中，并用于在 Arrow 格式中进行列式数据交换。对于非 C/C++语言和运行时，将 C 定义翻译成相应的 C FFI 声明应该几乎同样容易。 /// Applications and libraries can therefore work with Arrow memory without necessarily using Arrow libraries or reinventing the wheel. Developers can choose between tight integration with the Arrow software project (benefiting from the growing array of facilities exposed by e.g. the C++ or Java implementations of Apache Arrow, but with the cost of a dependency) or minimal integration with the Arrow format only. // 因此，应用程序和库可以在不使用 Arrow 库或不重新发明轮子的情况下与 Arrow 内存一起工作。开发者可以选择与 Arrow 软件项目紧密集成（从而受益于例如 Apache Arrow 的 C++或 Java 实现所暴露的日益增长的功能，但代价是依赖性），或仅与 Arrow 格式进行最小程度的集成。 /// ### Goals // 目标 /// - Expose an ABI-stable interface. // 暴露一个 ABI 稳定的接口。 /// - Make it easy for third-party projects to implement support for (including partial support where sufficient), with little initial investment. // 让第三方项目能够轻松实现对（包括部分支持，只要足够）的支持，且初始投入很小。 /// - Allow zero-copy sharing of Arrow data between independent runtimes and components running in the same process. // 允许在不同的独立运行时和同一进程中运行的组件之间进行零拷贝共享的 Arrow 数据。 /// - Match the Arrow array concepts closely to avoid the development of yet another marshalling layer. // 使 Arrow 数组概念与现有实现紧密匹配，以避免开发另一个序列化层。 /// - Avoid the need for one-to-one adaptation layers such as the limited JPype-based bridge between Java and Python. // 避免需要一对一的适配层，例如 Java 和 Python 之间基于 JPype 的有限桥接。 /// - Enable integration without an explicit dependency (either at compile-time or runtime) on the Arrow software project. // 实现无需显式依赖 Arrow 软件项目（无论是编译时还是运行时）的集成。 /// Ideally, the Arrow C data interface can become a low-level lingua franca for sharing columnar data at runtime and establish Arrow as the universal building block in the columnar processing ecosystem. // 理想情况下，Arrow C 数据接口可以成为运行时共享列式数据的低级通用语言，并确立 Arrow 作为列式处理生态系统中的通用构建模块。 /// ### Non-goals // 非目标 /// - Expose a C API mimicking operations available in higher-level runtimes (such as C++, Java…). // 暴露一个模仿高级运行时（如 C++、Java 等）中可用操作的 C API。 /// - Data sharing between distinct processes or storage persistence. // 不同进程之间的数据共享或存储持久化。 /// ### Comparison with the Arrow IPC format // 与 Arrow IPC 格式的比较 /// Pros of the C data interface vs. the IPC format: // C 数据接口相对于 IPC 格式的优点： /// - No dependency on Flatbuffers. // 不依赖 Flatbuffers。 /// - No buffer reassembly (data is already exposed in logical Arrow format). // 无需缓冲区重组（数据已以逻辑 Arrow 格式呈现）。 /// - Zero-copy by design. // 设计上支持零拷贝。 /// - Easy to reimplement from scratch. // 易于从头开始重新实现。 /// - Minimal C definition that can be easily copied into other codebases. // 极简的 C 语言定义，可轻松复制到其他代码库中。 /// - Resource lifetime management through a custom release callback. // 通过自定义释放回调进行资源生命周期管理。 /// Pros of the IPC format vs. the data interface: // IPC 格式相对于数据接口的优势： /// - Works across processes and machines. // 跨进程和跨机器工作。 /// - Allows data storage and persistence. // 支持数据存储和持久化。 /// - Being a streamable format, the IPC format has room for composing more features (such as integrity checks, compression…). // 作为可流式传输的格式，IPC 格式为添加更多功能（如完整性检查、压缩等）提供了空间。 /// - Does not require explicit C data access. // 不需要显式 C 数据访问。"
[datafusion.docs:sql-ref/.site]: https://datafusion.apache.org/user-guide/sql/ "SQL Reference"
[ballista<datafusion>.src/gh]: https://github.com/apache/datafusion-ballista.git "(Apache-2.0) (Languages: Rust 91.2%, Python 4.7%, Shell 2.4%, Other 1.7%) Apache DataFusion Ballista Distributed Query Engine // Apache DataFusion Ballista 分布式查询引擎 /// Ballista is a distributed query execution engine that enhances Apache DataFusion by enabling the parallelized execution of workloads across multiple nodes in a distributed environment. // Ballista 是一个分布式查询执行引擎，通过在分布式环境中的多个节点上并行执行工作负载来增强 Apache DataFusion。 /// A Ballista cluster consists of one or more scheduler processes and one or more executor processes. These processes can be run as native binaries and are also available as Docker Images, which can be easily deployed with Docker Compose or Kubernetes. // Ballista 集群由一个或多个调度进程和一个或多个执行进程组成。这些进程可以作为原生二进制文件运行，也以 Docker 镜像的形式提供，可以轻松使用 Docker Compose 或 Kubernetes 进行部署。 /// The following diagram shows the interaction between clients and the scheduler for submitting jobs, and the interaction between the executor(s) and the scheduler for fetching tasks and reporting task status. // 下图展示了客户端与调度器之间提交作业的交互，以及执行器（s）与调度器之间获取任务和报告任务状态的交互。 /// { {{svg}} - gh: apache/datafusion-ballista @main/docs/source/contributors-guide/ballista_architecture.excalidraw.svg }"
[ballista<datafusion>.site/.site]: https://datafusion.apache.org/ballista/ "Ballista is a distributed compute platform primarily implemented in Rust, and powered by Apache DataFusion. // Ballista 是一个主要用 Rust 实现的分布式计算平台，并由 Apache DataFusion 支持。 /// Ballista has a scheduler and an executor process that are standard Rust executables and can be executed directly, but Dockerfiles are provided to build images for use in containerized environments, such as Docker, Docker Compose, and Kubernetes. See the deployment guide for more information // Ballista 拥有标准 Rust 可执行文件形式的调度器和执行器进程，可以直接执行，但提供了 Dockerfile 以便在 Docker、Docker Compose 和 Kubernetes 等容器化环境中构建镜像。有关更多信息，请参阅部署指南。 /// SQL and DataFrame queries can be submitted from Python and Rust, and SQL queries can be submitted via the Arrow Flight SQL JDBC driver, supporting your favorite JDBC compliant tools such as DataGrip or tableau. For setup instructions, please see the FlightSQL guide. // SQL 和 DataFrame 查询可以从 Python 和 Rust 提交，SQL 查询可以通过 Arrow Flight SQL JDBC 驱动提交，支持 DataGrip 或 tableau 等您喜欢的 JDBC 兼容工具。有关设置说明，请参阅 FlightSQL 指南。 /// How does this compare to Apache Spark?¶ // 与 Apache Spark 相比如何？ ¶ /// Although Ballista is largely inspired by Apache Spark, there are some key differences. // 尽管 Ballista 大部分灵感来自 Apache Spark，但存在一些关键差异。 /// - The choice of Rust as the main execution language means that memory usage is deterministic and avoids the overhead of GC pauses. // 选择 Rust 作为主要执行语言意味着内存使用是确定的，避免了 GC 暂停的开销。 /// - Ballista is designed from the ground up to use columnar data, enabling a number of efficiencies such as vectorized processing (SIMD and GPU) and efficient compression. Although Spark does have some columnar support, it is still largely row-based today. // Ballista 从根本上设计为使用列式数据，从而实现多种效率，如矢量化处理（SIMD 和 GPU）以及高效的压缩。尽管 Spark 确实有一些列式支持，但目前仍然主要基于行。 /// - The combination of Rust and Arrow provides excellent memory efficiency and memory usage can be 5x - 10x lower than Apache Spark in some cases, which means that more processing can fit on a single node, reducing the overhead of distributed compute. // Rust 和 Arrow 的结合提供了出色的内存效率，在某些情况下，内存使用量可以比 Apache Spark 低 5 倍至 10 倍，这意味着更多的处理可以在单个节点上完成，从而减少了分布式计算的额外开销。 /// - The use of Apache Arrow as the memory model and network protocol means that data can be exchanged between executors in any programming language with minimal serialization overhead. // 使用 Apache Arrow 作为内存模型和网络协议，意味着数据可以在任何编程语言之间交换，且序列化开销极小。"
[ballista<datafusion>.guide:python<rust-impl>/.site]: https://datafusion.apache.org/ballista/user-guide/python.html "The Python bindings support executing SQL queries as well. /// (: df = ctx.sql('SELECT count(*) FROM trips') ;: df.show() #> <an ascii table> # Showing Query Results ;: df.collect() # The collect method executes the query and returns the results in PyArrow record batches. ;: df.explain() # The explain method can be used to show the logical and physical query plans for a query.)"
[ballista<datafusion>.deployment:docker<cmds-bin>/.site]: https://datafusion.apache.org/ballista/user-guide/deployment/docker.html "Download OCI images /// (: docker pull -- ghcr.io/apache/datafusion-ballista-standalone:latest) /// Start a Cluster /// (: docker run --network=host -d -- apache/datafusion-ballista-scheduler:latest --bind-port 50050 # Start a Scheduler ;: docker run --network=host -d -- apache/datafusion-ballista-executor:latest --external-host localhost --bind-port 50051 # Start one or more executor processes ;: docker run --network=host -it -- apache/datafusion-ballista-cli:latest --host localhost --port 50050 # Connect from the Client CLI)"
[ballista<datafusion>.deployment:kubernetes<oci-img>/.site]: https://datafusion.apache.org/ballista/user-guide/deployment/kubernetes.html "(ballista-scheduler -- (as~service: (port: 50050 (name: scheduler)) (port: 80 (name: scheduler-ui))) (replicas: 1) (oci-image: ghcr.io/apache/datafusion-ballista-scheduler) (args: --bind-port=50050) (container-port: 50050 (name: flight)) (mount: (persistent-volume-claim: data-pv-claim) (path: /mnt))) (ballista-executor -- (replicas: 2) (oci-image: ghcr.io/apache/datafusion-ballista-executor) (args: --bind-port=50051 --scheduler-host=ballista-scheduler (% a svc-name %) --scheduler-port=50050) (container-port: 50051 (name: flight)) (mount: (persistent-volume-claim: data-pv-claim) (path: /mnt))) (: kubectl port-forward service/ballista-scheduler 50050:50050 # Forward the scheduler port to localhost ;: ballista-cli --host localhost --port 50050 # Connect from the Client CLI)"
[ballista<datafusion>.pkg-lib:rust.cargo/crates]: https://crates.io/crates/ballista "(: cargo add -- ballista) (Apache-2.0) (147 SLoC) (48.5 KiB) Ballista Distributed Compute // Ballista 分布式计算 (src: gh:apache/datafusion-ballista.git)"
[ballista<datafusion>.pkg-cli:scheduler.cargo/crates]: https://crates.io/crates/ballista-scheduler "(: cargo install -- ballista-scheduler) (Apache-2.0) (12K SLoC) (136 KiB) Ballista Distributed Compute - Scheduler // Ballista 分布式计算 - 调度器 (src: gh:apache/datafusion-ballista.git)"
[ballista<datafusion>.pkg-cli:executor.cargo/crates]: https://crates.io/crates/ballista-executor "(: cargo install -- ballista-executor) (Apache-2.0) (3.0K SLoC) (60.9 KiB) Ballista Distributed Compute - Executor // Ballista 分布式计算 - 执行器 (src: gh:apache/datafusion-ballista.git)"
[ballista<datafusion>.pkg-cli:client.cargo/crates]: https://crates.io/crates/ballista-cli "(: cargo install -- ballista-cli) (Apache-2.0) (538 SLoC) (39.8 KiB) Command Line Client for Ballista distributed query engine. // Ballista 分布式查询引擎的命令行客户端。 (src: gh:apache/datafusion-ballista.git)"
[ballista<datafusion>.pkg-oci:arrow-ballista-standalone.oci/ghcr]: https://github.com/apache/datafusion-ballista/pkgs/container/arrow-ballista-standalone "(: docker pull -- ghcr.io/apache/arrow-ballista-standalone:latest)"
[ballista<datafusion>.pkg-oci:arrow-ballista-scheduler.oci/ghcr]: https://github.com/apache/datafusion-ballista/pkgs/container/arrow-ballista-scheduler "(: docker pull -- ghcr.io/apache/arrow-ballista-scheduler:latest)"
[ballista<datafusion>.pkg-oci:arrow-ballista-executor.oci/ghcr]: https://github.com/apache/datafusion-ballista/pkgs/container/arrow-ballista-executor "(: docker pull -- ghcr.io/apache/arrow-ballista-executor:latest)"
[ballista<datafusion>.pkg-oci:datafusion-ballista-standalone.oci/ghcr]: https://github.com/apache/datafusion-ballista/pkgs/container/datafusion-ballista-standalone "(: docker pull -- ghcr.io/apache/datafusion-ballista-standalone:latest)"
[ballista<datafusion>.pkg-oci:datafusion-ballista-scheduler.oci/ghcr]: https://github.com/apache/datafusion-ballista/pkgs/container/datafusion-ballista-scheduler "(: docker pull -- ghcr.io/apache/datafusion-ballista-scheduler:latest)"
[ballista<datafusion>.pkg-oci:datafusion-ballista-executor.oci/ghcr]: https://github.com/apache/datafusion-ballista/pkgs/container/datafusion-ballista-executor "(: docker pull -- ghcr.io/apache/datafusion-ballista-executor:latest)"
