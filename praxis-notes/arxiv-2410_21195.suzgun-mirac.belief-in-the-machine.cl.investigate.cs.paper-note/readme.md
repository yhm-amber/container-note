[paper/arxiv]: https://arxiv.org/abs/2410.21195 "(License: CC BY 4.0) [Submitted on 28 Oct 2024] (Cite as: 	arXiv:2410.21195 [cs.CL]) (arXiv-issued DOI via DataCite: 	doi.org/10.48550/arXiv.2410.21195) { Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computers and Society (cs.CY) } Belief in the Machine: Investigating Epistemological Blind Spots of Language Models // 机器的信念：探究语言模型的认识论盲点 /// As language models (LMs) become integral to fields like healthcare, law, and journalism, their ability to differentiate between fact, belief, and knowledge is essential for reliable decision-making. Failure to grasp these distinctions can lead to significant consequences in areas such as medical diagnosis, legal judgments, and dissemination of fake news. Despite this, current literature has largely focused on more complex issues such as theory of mind, overlooking more fundamental epistemic challenges. This study systematically evaluates the epistemic reasoning capabilities of modern LMs, including GPT-4, Claude-3, and Llama-3, using a new dataset, KaBLE, consisting of 13,000 questions across 13 tasks. Our results reveal key limitations. First, while LMs achieve 86% accuracy on factual scenarios, their performance drops significantly with false scenarios, particularly in belief-related tasks. Second, LMs struggle with recognizing and affirming personal beliefs, especially when those beliefs contradict factual data, which raises concerns for applications in healthcare and counseling, where engaging with a person's beliefs is critical. Third, we identify a salient bias in how LMs process first-person versus third-person beliefs, performing better on third-person tasks (80.7%) compared to first-person tasks (54.4%). Fourth, LMs lack a robust understanding of the factive nature of knowledge, namely, that knowledge inherently requires truth. Fifth, LMs rely on linguistic cues for fact-checking and sometimes bypass the deeper reasoning. These findings highlight significant concerns about current LMs' ability to reason about truth, belief, and knowledge while emphasizing the need for advancements in these areas before broad deployment in critical sectors. // 随着语言模型（LMs）在医疗保健、法律和新闻等领域的日益重要，它们区分事实、信念和知识的能力对于可靠决策至关重要。未能理解这些区别可能导致在医学诊断、法律判决和虚假新闻传播等领域产生严重后果。尽管如此，当前文献主要关注更复杂的问题，如心智理论，而忽略了更根本的认知挑战。本研究使用新的数据集 KaBLE 系统地评估了现代语言模型（包括 GPT-4、Claude-3 和 Llama-3）的认知推理能力，该数据集包含 13 项任务中的 13,000 个问题。我们的结果表明存在关键局限性。首先，虽然语言模型在事实场景中达到 86%的准确率，但在虚假场景中的表现显著下降，尤其是在与信念相关的任务中。其次，语言模型在识别和确认个人信念方面存在困难，尤其是在这些信念与事实数据相矛盾时，这引发了在医疗保健和咨询领域应用的担忧，因为这些领域需要与个人的信念进行互动。 第三，我们发现语言模型在处理第一人称与第三人称信念时存在显著偏差，在第三人称任务上的表现优于第一人称任务（分别为 80.7%和 54.4%）。第四，语言模型对知识的既定性缺乏稳固的理解，即知识本质上需要真理。第五，语言模型依赖语言线索进行事实核查，有时会绕过更深层次的推理。这些发现凸显了当前语言模型在推理真理、信念和知识方面的重大缺陷，同时也强调了在关键领域广泛部署前，必须在这些领域取得显著进步。 /// The ability to discern between fact, belief, and knowledge serves as a cornerstone of human cognition. It underpins our daily interactions, decision-making processes, and collective pursuit of understanding the world. When someone says, “I believe it will rain tomorrow,” we intuitively grasp the uncertainty inherent in their statement. Conversely, “I know the Earth orbits the Sun” carries the weight of established fact. This nuanced comprehension of epistemic language is crucial across various domains, from healthcare and law to journalism and politics [1, 2, 3]. // 辨别事实、信念和知识的能力是人类认知的基石。它支撑着我们的日常交往、决策过程以及集体追求理解世界。当有人说“我相信明天会下雨”时，我们直觉上就能把握其陈述中蕴含的不确定性。相反，“我知道地球绕太阳公转”则承载着既定事实的分量。对认识论语言这种细致入微的理解在各种领域都至关重要，从医疗保健和法律到新闻和政治[1, 2, 3]。 /// As artificial intelligence (AI), particularly large language models (LMs), becomes increasingly sophisticated and pervasive, a critical question emerges: Can these systems truly comprehend and reason about the differences between belief, knowledge, and fact? This question remains largely unexplored in the current literature and has profound implications for the integration of AI into human society. // 随着人工智能（AI），特别是大型语言模型（LMs）变得越来越复杂和普遍，一个关键问题出现了：这些系统能否真正理解和推理信念、知识与事实之间的差异？这个问题在当前文献中基本未被探讨，并且对人工智能融入人类社会具有深远影响。 /// Consider a patient saying to a doctor, “I believe I have cancer.” In healthcare, the interpretation of such statements requires careful evaluation to align subjective beliefs with objective medical assessments. Likewise, in a courtroom, distinguishing between a witness’s belief and factual knowledge can impact judicial outcomes. Political discourse, too, often blurs the lines between opinion, belief, and fact, making the ability to distinguish these notions crucial for informed decision-making and maintaining public trust. // 考虑一位患者对医生说：“我相信自己得了癌症。”在医疗领域，对这类陈述的解释需要仔细评估，以使主观信念与客观医学评估相一致。同样，在法庭上，区分证人的信念与事实知识可能会影响司法结果。政治话语中也常常模糊观点、信念与事实之间的界限，因此区分这些概念的能力对于做出明智决策和维护公众信任至关重要。"
[src/gh]: https://github.com/suzgunmirac/belief-in-the-machine.git "(MIT) Belief in the Machine: Investigating Epistemological Blind Spots of Language Models"
