[src/gh]: https://github.com/guoqingbao/vllm.rs.git "(Languages: Rust 95.5%, Python 4.1%, Shell 0.4%) Minimalist vLLM implementation in Rust // Rust ä¸­çš„æç®€ vLLM å®ç° /// ğŸš€ vLLM.rs â€“ A Minimalist vLLM in Rust // ğŸš€ vLLM.rs â€“ ä¸€ä¸ªç”¨ Rust ç¼–å†™çš„æç®€ vLLM /// A blazing-fast âš¡, lightweight Rust ğŸ¦€ implementation of vLLM. // ä¸€ä¸ªé€Ÿåº¦æå¿«âš¡ã€è½»é‡çº§çš„ Rust ğŸ¦€ vLLM å®ç°ã€‚ /// âœ¨ Key Features // âœ¨ ä¸»è¦ç‰¹ç‚¹ /// - ğŸ”§ Pure Rust Backend â€“ Absolutely no PyTorch required // ğŸ”§ çº¯ Rust åç«¯ â€“ å®Œå…¨ä¸éœ€è¦ PyTorch /// - ğŸš€ High Performance (with Context-cache and PD Disaggregation) // ğŸš€ é«˜æ€§èƒ½ ï¼ˆæ”¯æŒä¸Šä¸‹æ–‡ç¼“å­˜å’Œ PD è§£è€¦ ï¼‰ /// - ğŸ§  Minimalist Core â€“ Core logic written in <3000 lines of clean Rust // ğŸ§  æç®€æ ¸å¿ƒ â€“ æ ¸å¿ƒé€»è¾‘ç”¨ 3000 è¡Œç®€æ´çš„ Rust ä»£ç ç¼–å†™ã€‚ /// - ğŸ’» Cross-Platform â€“ Supports CUDA (Linux/Windows) and Metal (macOS) // ğŸ’» è·¨å¹³å° â€“ æ”¯æŒ CUDA ï¼ˆLinux/Windowsï¼‰å’Œ Metal ï¼ˆmacOSï¼‰ /// - ğŸ¤– Built-in API Server and ChatGPT-like Web UI â€“ Native Rust server for both CUDA and Metal // ğŸ¤– å†…ç½® API æœåŠ¡å™¨å’Œç±»ä¼¼ ChatGPT çš„ Web UI â€“ æ”¯æŒ CUDA å’Œ Metal çš„åŸç”Ÿ Rust æœåŠ¡å™¨ /// - ğŸ”Œ MCP Integration â€“ Model Context Protocol for tool calling support // ğŸ”Œ MCP é›†æˆ â€“ ç”¨äºå·¥å…·è°ƒç”¨æ”¯æŒçš„æ¨¡å‹ä¸Šä¸‹æ–‡åè®® /// - ğŸ“Š Embedding & Tokenizer APIs â€“ Full text processing support // ğŸ“Š åµŒå…¥å’Œåˆ†è¯å™¨ API â€“ å®Œæ•´çš„æ–‡æœ¬å¤„ç†æ”¯æŒ /// - ğŸ Lightweight Python Interface â€“ PyO3-powered bindings for chat completion // ğŸ è½»é‡çº§ Python æ¥å£ â€“ åŸºäº PyO3 çš„èŠå¤©è‡ªåŠ¨è¡¥å…¨ç»‘å®š"
[lib.pip/pypi]: https://pypi.org/project/vllm-rs/ "(: pip install -- vllm-rs) (: python -m vllm_rs.server --m zai-org/codegeex4-all-9b-GGUF --ui-server --context-cache)"
[knows_by]: https://github.com/huggingface/candle.git "... /// - vllm.rs: A minimalist vLLM implementation in Rust based on Candle. // vllm.rs ï¼šä¸€ä¸ªåŸºäº Candle çš„ Rust æç®€ vLLM å®ç°ã€‚"

