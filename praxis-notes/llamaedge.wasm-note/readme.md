[src/gh]: https://github.com/LlamaEdge/LlamaEdge.git "(Apache-2.0) (Languages: Rust 97.2%, Shell 2.7%, Dockerfile 0.1%) The easiest & fastest way to run customized and fine-tuned LLMs locally or on the edge // 用最简单最快的途径在本地或边缘运行、定制、微调大语言模型 (: 1. Install WasmEdge ;: 2. Download an LLM model file (e.g., : curl -LO https://huggingface.co/second-state/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q5_K_M.gguf) ;: 3. Download the LlamaEdge CLI chat (or api server) (e.g., : curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm) ;: 4. Deploy UI (e.g., : curl -LO https://github.com/LlamaEdge/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz ;: tar xzf chatbot-ui.tar.gz) ;: 5. Launch Server (e.g., : wasmedge --dir .:. --nn-preload default:GGML:AUTO:DeepSeek-R1-Distill-Llama-8B-Q5_K_M.gguf llama-api-server.wasm --prompt-template llama-3-chat --ctx-size 8096))"
[site]: https://llamaedge.com/ "The easiest, smallest and fastest local LLM runtime and API server. // 最简单、最小、最快的本地 LLM 运行时和 API 服务器。 /// Powered by Rust & WasmEdge (A CNCF hosted project) // 由 Rust 与 Wesmedge 提供支持（ CNCF 托管项目） /// Q: Why can't I use Python to run the LLM inference? // 问：为什么我不能使用 Python 运行 LLM 推理？ /// A: You can certainly use Python to run LLMs and even start an API server using Python. But keep mind that PyTorch has over 5GB of complex dependencies. These dependencies often conflict with Python toolchains such as LangChain. It is often a nightmare to set up Python dependencies across dev and deployment machines, especially with GPUs and containers. // 答：您当然可以使用 Python 进行运行 LLMs 甚至使用 Python 启动 API 服务器。但是请注意， Pytorch 具有超过 5GB 的复杂依赖性。这些依赖性经常与 python 工具链（例如 LangChain ）冲突。在开发机器和部署机器上建立 Python 依赖性通常是一场噩梦，尤其是使用 GPU 和容器。 /// In contrast, the entire LlamaEdge runtime is less than 30MB. It is has no external dependencies. Just install LlamaEdge and copy over your compiled application file! // 相比之下，整个 LlamaEdge 运行时都小于 30MB 。它没有外部依赖性。只需安装 LlamaEdge 并通过您的编译应用程序复制即可！ /// Q: Why can't I just use native (C/C++ compiled) inference engines? // 问：为什么我不能只使用原生（ C/C ++ 编译）推理引擎？ /// A: The biggest issue with native compiled apps is that they are not portable. You must rebuild and retest for each computer you deploy the application. It is a very tedious and error prone progress. LlamaEdge programs are written in Rust (soon JS) and compiled to Wasm. The Wasm app runs as fast as native apps, and is entirely portable. // 答：原生编译应用程序的最大问题是它们不可移植。您必须为部署应用程序的每台计算机进行重建和重新测试。这是一个非常乏味和犯错的进展。 LlamaEdge 程序用 Rust 编写（很快），并输出 WASM 编译结果。 WASM 应用程序的运行速度与原生应用程序一样快，并且完全可移植。"
[chatbotui.src/gh]: https://github.com/LlamaEdge/chatbot-ui.git "(MIT) (Languages: TypeScript 97.8%, JavaScript 1.4%, Other 0.8%) This project is an improved version of chatbot UI that allows customizing request URLs. The project is forked from mckaywrigley/chatbot-ui, which using MIT license. // 该项目是 Chatbot UI 的改进版本，允许自定义请求 URL 。该项目摘自使用 MIT 许可证的 McKaywrigley/Chatbot-UI 。"
