[src/gh]: https://github.com/huggingface/safetensors.git "(Apache-2.0) (Languages: Python 51.2%, Rust 48.1%, Other 0.7%) Simple, safe way to store and distribute tensors // å­˜å‚¨å’Œåˆ†å‘å¼ é‡çš„ç®€å•ã€å®‰å…¨çš„æ–¹æ³• /// This repository implements a new simple format for storing tensors safely (as opposed to pickle) and that is still fast (zero-copy). // è¯¥å­˜å‚¨åº“å®ç°äº†ä¸€ç§æ–°çš„ç®€å•æ ¼å¼ï¼Œç”¨äºå®‰å…¨åœ°å­˜å‚¨å¼ é‡ï¼ˆä¸ pickle ä¸åŒï¼‰ï¼Œè€Œä¸”é€Ÿåº¦ä»ç„¶å¾ˆå¿«ï¼ˆé›¶æ‹·è´ï¼‰ã€‚ /// Format // æ ¼å¼ /// - 8 bytes: N, an unsigned little-endian 64-bit integer, containing the size of the header // 8 å­—èŠ‚ï¼š N ï¼Œä¸€ä¸ªæ— ç¬¦å·å°ç«¯ 64 ä½æ•´æ•°ï¼ŒåŒ…å«å¤´éƒ¨çš„å¤§å° /// - N bytes: a JSON UTF-8 string representing the header. // N å­—èŠ‚ï¼šè¡¨ç¤ºæ ‡å¤´çš„ JSON UTF-8 å­—ç¬¦ä¸²ã€‚ /// - - The header data MUST begin with a { character (0x7B). // æ ‡å¤´æ•°æ®å¿…é¡»ä»¥ { å­—ç¬¦ (0x7B) å¼€å¤´ã€‚ /// - - The header data MAY be trailing padded with whitespace (0x20). // å¤´éƒ¨æ•°æ®æœ«å°¾å¯èƒ½å¡«å……ç©ºæ ¼ï¼ˆ0x20ï¼‰ã€‚ /// - - The header is a dict like {'TENSOR_NAME': {'dtype': 'F16', 'shape': [1, 16, 256], 'data_offsets': [BEGIN, END]}, 'NEXT_TENSOR_NAME': {...}, ...}, data_offsets point to the tensor data relative to the beginning of the byte buffer (i.e. not an absolute position in the file), with BEGIN as the starting offset and END as the one-past offset (so total tensor byte size = END - BEGIN). // å¤´éƒ¨æ˜¯ä¸€ä¸ªç±»ä¼¼ {'TENSOR_NAME': {'dtype': 'F16', 'shape': [1, 16, 256], 'data_offsets': [BEGIN, END]}, 'NEXT_TENSOR_NAME': {...}, ...} çš„å­—å…¸ï¼Œ data_offsets æŒ‡å‘ç›¸å¯¹äºå­—èŠ‚ç¼“å†²åŒºå¼€å¤´çš„å¼ é‡æ•°æ®ï¼ˆå³ä¸æ˜¯æ–‡ä»¶ä¸­çš„ç»å¯¹ä½ç½®ï¼‰ï¼Œå…¶ä¸­ BEGIN ä¸ºèµ·å§‹åç§»é‡ï¼Œ END ä¸ºåä¸€ä¸ªåç§»é‡ï¼ˆå› æ­¤å¼ é‡æ€»å­—èŠ‚å¤§å° = END - BEGIN ï¼‰ã€‚ /// - - A special key __metadata__ is allowed to contain free form string-to-string map. Arbitrary JSON is not allowed, all values must be strings. // å…è®¸ä½¿ç”¨ç‰¹æ®Šé”® __metadata__ æ¥å­˜å‚¨è‡ªç”±æ ¼å¼çš„å­—ç¬¦ä¸²åˆ°å­—ç¬¦ä¸²çš„æ˜ å°„ã€‚ä¸å…è®¸ä½¿ç”¨ä»»æ„ JSON æ ¼å¼ï¼Œæ‰€æœ‰å€¼éƒ½å¿…é¡»æ˜¯å­—ç¬¦ä¸²ã€‚ /// - Rest of the file: byte-buffer. // æ–‡ä»¶å…¶ä½™éƒ¨åˆ†ï¼šå­—èŠ‚ç¼“å†²åŒºã€‚ /// Notes: // ç¬”è®°ï¼š /// - Duplicate keys are disallowed. Not all parsers may respect this. // ä¸å…è®¸é‡å¤çš„é”®ã€‚å¹¶éæ‰€æœ‰è§£æå™¨éƒ½ä¼šéµå®ˆæ­¤è§„åˆ™ã€‚ /// - In general the subset of JSON is implicitly decided by serde_json for this library. Anything obscure might be modified at a later time, that odd ways to represent integer, newlines and escapes in utf-8 strings. This would only be done for safety concerns // é€šå¸¸æƒ…å†µä¸‹ï¼Œè¯¥åº“çš„ JSON å­é›†ç”± serde_json éšå¼å†³å®šã€‚ä¸€äº›ç‰¹æ®Šæƒ…å†µï¼Œä¾‹å¦‚ UTF-8 å­—ç¬¦ä¸²ä¸­è¡¨ç¤ºæ•´æ•°ã€æ¢è¡Œç¬¦å’Œè½¬ä¹‰ç¬¦çš„ç‰¹æ®Šæ–¹å¼ï¼Œå¯èƒ½ä¼šåœ¨ä»¥åè¿›è¡Œä¿®æ”¹ã€‚è¿™æ ·åšä»…å‡ºäºå®‰å…¨è€ƒè™‘ã€‚ /// - Tensor values are not checked against, in particular NaN and +/-Inf could be in the file // å¼ é‡å€¼æœªè¿›è¡Œæ£€æŸ¥ï¼Œç‰¹åˆ«æ˜¯æ–‡ä»¶ä¸­å¯èƒ½åŒ…å« NaN å’Œ +/-Inf å€¼ã€‚ /// - Empty tensors (tensors with 1 dimension being 0) are allowed. They are not storing any data in the databuffer, yet retaining size in the header. They don't really bring a lot of values but are accepted since they are valid tensors from traditional tensor libraries perspective (torch, tensorflow, numpy, ..). // å…è®¸ä½¿ç”¨ç©ºå¼ é‡ï¼ˆå³ä¸€ç»´å€¼ä¸º 0 çš„å¼ é‡ï¼‰ã€‚å®ƒä»¬ä¸ä¼šåœ¨æ•°æ®ç¼“å†²åŒºä¸­å­˜å‚¨ä»»ä½•æ•°æ®ï¼Œä½†ä»ä¼šåœ¨å¤´éƒ¨ä¿¡æ¯ä¸­ä¿ç•™å…¶å¤§å°ã€‚è™½ç„¶å®ƒä»¬å®é™…ä¸Šå¹¶ä¸åŒ…å«å¾ˆå¤šå€¼ï¼Œä½†ç”±äºä»ä¼ ç»Ÿå¼ é‡åº“ï¼ˆä¾‹å¦‚ torchã€tensorflowã€numpy ç­‰ï¼‰çš„è§’åº¦æ¥çœ‹ï¼Œå®ƒä»¬æ˜¯æœ‰æ•ˆçš„å¼ é‡ï¼Œå› æ­¤ä¹Ÿè¢«æ¥å—ã€‚ /// - 0-rank Tensors (tensors with shape []) are allowed, they are merely a scalar. // å…è®¸ä½¿ç”¨ç§©ä¸º 0 çš„å¼ é‡ï¼ˆå½¢çŠ¶ä¸º [] å¼ é‡ï¼‰ï¼Œå®ƒä»¬ä»…ä»…æ˜¯ä¸€ä¸ªæ ‡é‡ã€‚ /// - The byte buffer needs to be entirely indexed, and cannot contain holes. This prevents the creation of polyglot files. // å­—èŠ‚ç¼“å†²åŒºå¿…é¡»å®Œå…¨ç´¢å¼•ï¼Œä¸èƒ½å­˜åœ¨ç©ºä½ã€‚è¿™å¯ä»¥é˜²æ­¢åˆ›å»ºå¤šè¯­è¨€æ–‡ä»¶ã€‚ /// - Endianness: Little-endian. moment. // å­—èŠ‚åºï¼šå°ç«¯å­—èŠ‚åºã€‚æ—¶åˆ»ã€‚ /// - Order: 'C' or row-major. // é¡ºåºï¼š'C' æˆ–è¡Œä¸»åºã€‚ /// - Notes: Some smaller than 1 byte dtypes appeared, which make alignment tricky. Non traditional APIs might be required for those. // æ³¨ï¼šå‡ºç°äº†ä¸€äº›å°äº 1 å­—èŠ‚çš„æ•°æ®ç±»å‹ï¼Œè¿™ä½¿å¾—å¯¹é½å˜å¾—æ£˜æ‰‹ã€‚å¯¹äºè¿™äº›æ•°æ®ç±»å‹ï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨éä¼ ç»Ÿçš„ APIã€‚ /// Yet another format ? // åˆä¸€ç§æ ¼å¼ï¼Ÿ /// The main rationale for this crate is to remove the need to use pickle on PyTorch which is used by default. There are other formats out there used by machine learning and more general formats. (pickle (PyTorch), H5 (Tensorflow), SavedModel (Tensorflow), MsgPack (flax), Protobuf (ONNX), Cap'n'Proto, Arrow, Numpy (npy,npz), pdparams (Paddle), SafeTensors) // è¿™ä¸ªç®±å­çš„ä¸»è¦ç›®çš„æ˜¯ä¸ºäº†æ¶ˆé™¤ä½¿ç”¨çš„éœ€æ±‚ã€‚ PyTorch é»˜è®¤ä½¿ç”¨ pickle ã€‚æ­¤å¤–ï¼Œæœºå™¨å­¦ä¹ é¢†åŸŸè¿˜ä½¿ç”¨å…¶ä»–æ ¼å¼ï¼Œä»¥åŠæ›´é€šç”¨çš„æ ¼å¼ã€‚ (pickle (PyTorch), H5 (Tensorflow), SavedModel (Tensorflow), MsgPack (flax), Protobuf (ONNX), Cap'n'Proto, Arrow, Numpy (npy,npz), pdparams (Paddle), SafeTensors) // Capabilities & Qualities entry // èƒ½åŠ›ç´ è´¨é¡¹ /// - Safe: Can I use a file randomly downloaded and expect not to run arbitrary code ? // å®‰å…¨ï¼šæˆ‘å¯ä»¥ä½¿ç”¨éšæœºä¸‹è½½çš„æ–‡ä»¶ï¼Œå¹¶ä¸”ä¸å¸Œæœ›å®ƒè¿è¡Œä»»æ„ä»£ç å—ï¼Ÿ /// - Zero-copy: Does reading the file require more memory than the original file ? // é›¶æ‹·è´ï¼šè¯»å–è¯¥æ–‡ä»¶æ˜¯å¦æ¯”è¯»å–åŸå§‹æ–‡ä»¶éœ€è¦æ›´å¤šå†…å­˜ï¼Ÿ /// - Lazy loading: Can I inspect the file without loading everything ? And loading only some tensors in it without scanning the whole file (distributed setting) ? // å»¶è¿ŸåŠ è½½ï¼šæˆ‘èƒ½å¦åœ¨ä¸åŠ è½½æ‰€æœ‰å†…å®¹çš„æƒ…å†µä¸‹æ£€æŸ¥æ–‡ä»¶ï¼Ÿæˆ–è€…åªåŠ è½½å…¶ä¸­çš„ä¸€äº›å¼ é‡ï¼Œè€Œä¸æ‰«ææ•´ä¸ªæ–‡ä»¶ï¼ˆåˆ†å¸ƒå¼è®¾ç½®ï¼‰ï¼Ÿ /// - Layout control: Lazy loading, is not necessarily enough since if the information about tensors is spread out in your file, then even if the information is lazily accessible you might have to access most of your file to read the available tensors (incurring many DISK -> RAM copies). Controlling the layout to keep fast access to single tensors is important. // å¸ƒå±€æ§åˆ¶ï¼šå»¶è¿ŸåŠ è½½å¹¶ä¸ä¸€å®šè¶³å¤Ÿï¼Œå› ä¸ºå¦‚æœå¼ é‡ä¿¡æ¯åˆ†æ•£åœ¨æ–‡ä»¶ä¸­ï¼Œå³ä½¿ä¿¡æ¯å¯ä»¥å»¶è¿Ÿè®¿é—®ï¼Œä¹Ÿå¯èƒ½éœ€è¦è®¿é—®æ–‡ä»¶çš„å¤§éƒ¨åˆ†å†…å®¹æ‰èƒ½è¯»å–å¯ç”¨çš„å¼ é‡ï¼ˆå¯¼è‡´å¤§é‡çš„ç£ç›˜åˆ°å†…å­˜çš„å¤åˆ¶æ“ä½œï¼‰ã€‚å› æ­¤ï¼Œæ§åˆ¶å¸ƒå±€ä»¥ä¿æŒå¯¹å•ä¸ªå¼ é‡çš„å¿«é€Ÿè®¿é—®è‡³å…³é‡è¦ã€‚ /// - No file size limit: Is there a limit to the file size ? // æ–‡ä»¶å¤§å°æ— é™åˆ¶ï¼šæ–‡ä»¶å¤§å°æœ‰é™åˆ¶å—ï¼Ÿ /// - Flexibility: Can I save custom code in the format and be able to use it later with zero extra code ? (~ means we can store more than pure tensors, but no custom code) // çµæ´»æ€§ï¼šæˆ‘èƒ½å¦ä»¥è¿™ç§æ ¼å¼ä¿å­˜è‡ªå®šä¹‰ä»£ç ï¼Œå¹¶åœ¨ä»¥åæ— éœ€ç¼–å†™ä»»ä½•é¢å¤–ä»£ç å³å¯ä½¿ç”¨ï¼Ÿï¼ˆ~ è¡¨ç¤ºæˆ‘ä»¬å¯ä»¥å­˜å‚¨é™¤çº¯å¼ é‡ä¹‹å¤–çš„å…¶ä»–å†…å®¹ï¼Œä½†ä¸èƒ½å­˜å‚¨è‡ªå®šä¹‰ä»£ç ï¼‰ /// - Bfloat16/Fp8: Does the format support native bfloat16/fp8 (meaning no weird workarounds are necessary)? This is becoming increasingly important in the ML world. // Bfloat16/Fp8ï¼šè¯¥æ ¼å¼æ˜¯å¦åŸç”Ÿæ”¯æŒ bfloat16/fp8 ç±»å‹ï¼ˆå³æ— éœ€ä»»ä½•ç‰¹æ®Šå˜é€šæ–¹æ³•ï¼‰ï¼Ÿè¿™åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸå˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ /// Main oppositions // ä¸»è¦å¯¹ç«‹æ–¹æ¡ˆ /// - Pickle: Unsafe, runs arbitrary code // Pickleï¼šä¸å®‰å…¨ï¼Œä¼šè¿è¡Œä»»æ„ä»£ç  /// - H5: Apparently now discouraged for TF/Keras. Seems like a great fit otherwise actually. Some classic use after free issues (web: cvedetails.com/vulnerability-list/vendor_id-15991/product_id-35054/Hdfgroup-Hdf5.html). On a very different level than pickle security-wise. Also 210k lines of code vs ~400 lines for this lib currently. // H5ï¼šç›®å‰ä¼¼ä¹ä¸æ¨èåœ¨ TF/Keras ä¸­ä½¿ç”¨ã€‚ä½†å®é™…ä¸Šï¼Œå®ƒåœ¨å…¶ä»–æ–¹é¢ä¼¼ä¹éå¸¸åˆé€‚ã€‚ä¸€äº›ç»å…¸çš„é‡Šæ”¾åä½¿ç”¨é—®é¢˜ã€‚ä»å®‰å…¨æ€§è§’åº¦æ¥çœ‹ï¼Œå®ƒä¸ pickle å®Œå…¨ä¸åŒã€‚æ­¤å¤–ï¼ŒHdf5 çš„ä»£ç é‡é«˜è¾¾ 21 ä¸‡è¡Œï¼Œè€Œç›®å‰çš„è¿™ä¸ªåº“åªæœ‰å¤§çº¦ 400 è¡Œã€‚ /// - SavedModel: Tensorflow specific (it contains TF graph information). // SavedModelï¼šTensorflow ç‰¹æœ‰çš„ï¼ˆåŒ…å« TF å›¾ä¿¡æ¯ï¼‰ã€‚ /// - MsgPack: No layout control to enable lazy loading (important for loading specific parts in distributed setting) // MsgPackï¼šæ²¡æœ‰å¸ƒå±€æ§ä»¶æ¥å¯ç”¨å»¶è¿ŸåŠ è½½ï¼ˆè¿™å¯¹äºåœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­åŠ è½½ç‰¹å®šéƒ¨åˆ†éå¸¸é‡è¦ï¼‰ /// - Protobuf: Hard 2Go max file size limit // Protobufï¼š2Go æœ€å¤§æ–‡ä»¶å¤§å°é™åˆ¶ /// - Cap'n'proto: Float16 support is not present (. web: capnproto.org/language.html#built-in-types) so using a manual wrapper over a byte-buffer would be necessary. Layout control seems possible but not trivial as buffers have limitations (. stackoverflow: questions/48458839/capnproto-maximum-filesize). // Cap'n'protoï¼šä¸æ”¯æŒ Float16ï¼Œå› æ­¤éœ€è¦æ‰‹åŠ¨å°è£…å­—èŠ‚ç¼“å†²åŒº ã€‚å¸ƒå±€æ§åˆ¶çœ‹ä¼¼å¯è¡Œï¼Œä½†å¹¶éæ˜“äº‹ï¼Œå› ä¸ºç¼“å†²åŒºå­˜åœ¨å±€é™æ€§ ã€‚ /// - Numpy (npz): No bfloat16 support. Vulnerable to zip bombs (DOS). Not zero-copy. // Numpy (npz)ï¼šä¸æ”¯æŒ bfloat16 ç±»å‹ã€‚æ˜“å— zip ç‚¸å¼¹æ”»å‡»ï¼ˆæ‹’ç»æœåŠ¡æ”»å‡»ï¼‰ã€‚ä¸æ”¯æŒé›¶æ‹·è´ã€‚ /// - Arrow: No bfloat16 support. // ç®­å¤´ï¼šä¸æ”¯æŒ bfloat16 ã€‚ /// Notes // ç¬”è®° /// - Zero-copy: No format is really zero-copy in ML, it needs to go from disk to RAM/GPU RAM (that takes time). On CPU, if the file is already in cache, then it can truly be zero-copy, whereas on GPU there is not such disk cache, so a copy is always required but you can bypass allocating all the tensors on CPU at any given point. SafeTensors is not zero-copy for the header. The choice of JSON is pretty arbitrary, but since deserialization is <<< of the time required to load the actual tensor data and is readable I went that way, (also space is <<< to the tensor data). // é›¶æ‹·è´ï¼šåœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæ²¡æœ‰çœŸæ­£æ„ä¹‰ä¸Šçš„é›¶æ‹·è´æ ¼å¼ï¼Œæ•°æ®éœ€è¦ä»ç£ç›˜ä¼ è¾“åˆ°å†…å­˜/GPU å†…å­˜ï¼ˆè¿™éœ€è¦æ—¶é—´ï¼‰ã€‚åœ¨ CPU ä¸Šï¼Œå¦‚æœæ–‡ä»¶å·²å­˜åœ¨äºç¼“å­˜ä¸­ï¼Œåˆ™å¯ä»¥çœŸæ­£å®ç°é›¶æ‹·è´ï¼›è€Œåœ¨ GPU ä¸Šï¼Œç”±äºæ²¡æœ‰ç£ç›˜ç¼“å­˜ï¼Œå› æ­¤å§‹ç»ˆéœ€è¦è¿›è¡Œæ•°æ®å¤åˆ¶ï¼Œä½†æ‚¨å¯ä»¥é¿å…åœ¨ä»»ä½•ç»™å®šæ—¶é—´ç‚¹åœ¨ CPU ä¸Šåˆ†é…æ‰€æœ‰å¼ é‡ã€‚SafeTensors æ ¼å¼çš„å¤´éƒ¨æ•°æ®å¹¶éé›¶æ‹·è´ã€‚é€‰æ‹© JSON æ ¼å¼ç›¸å½“éšæ„ï¼Œä½†ç”±äºååºåˆ—åŒ–æ—¶é—´è¿œå°äºåŠ è½½å®é™…å¼ é‡æ•°æ®æ‰€éœ€çš„æ—¶é—´ï¼Œå¹¶ä¸” JSON æ ¼å¼å¯è¯»ï¼Œæ‰€ä»¥æˆ‘é€‰æ‹©äº†å®ƒï¼ˆç©ºé—´å ç”¨ä¹Ÿè¿œå°äºå¼ é‡æ•°æ®æœ¬èº«ï¼‰ã€‚ /// - Endianness: Little-endian. This can be modified later, but it feels really unnecessary at the moment. // å­—èŠ‚åºï¼šå°ç«¯å­—èŠ‚åºã€‚ä»¥åå¯ä»¥ä¿®æ”¹ï¼Œä½†ç›®å‰æ„Ÿè§‰æ²¡å¿…è¦ã€‚ /// - Order: 'C' or row-major. This seems to have won. We can add that information later if needed. // é¡ºåºï¼šC æˆ–è¡Œä¼˜å…ˆã€‚è¿™ä¸ªæ–¹æ¡ˆä¼¼ä¹æœ€ç»ˆè¢«é‡‡çº³ã€‚å¦‚æœ‰éœ€è¦ï¼Œæˆ‘ä»¬å¯ä»¥ç¨åæ·»åŠ ç›¸å…³ä¿¡æ¯ã€‚ /// - Stride: No striding, all tensors need to be packed before being serialized. I have yet to see a case where it seems useful to have a strided tensor stored in serialized format. // æ­¥é•¿ï¼šä¸æ”¯æŒæ­¥é•¿ï¼Œæ‰€æœ‰å¼ é‡åœ¨åºåˆ—åŒ–ä¹‹å‰éƒ½éœ€è¦æ‰“åŒ…ã€‚æˆ‘è¿˜æ²¡è§è¿‡ä»»ä½•éœ€è¦å°†æ­¥é•¿å¼ é‡ä»¥åºåˆ—åŒ–æ ¼å¼å­˜å‚¨çš„æƒ…å†µã€‚ /// - Sub 1 bytes dtypes: Dtypes can now have lower than 1 byte size, this makes alignment&adressing tricky. For now, the library will simply error out whenever an operation triggers an non aligned read. Trickier API may be created later for those non standard ops. // å°äº 1 å­—èŠ‚çš„æ•°æ®ç±»å‹ï¼šæ•°æ®ç±»å‹ç°åœ¨å¯ä»¥å°äº 1 å­—èŠ‚ï¼Œè¿™ä½¿å¾—å¯¹é½å’Œå¯»å€å˜å¾—å¤æ‚ã€‚ç›®å‰ï¼Œå½“æ“ä½œè§¦å‘æœªå¯¹é½çš„è¯»å–æ—¶ï¼Œåº“ä¼šç›´æ¥æŠ¥é”™ã€‚ä»¥åå¯èƒ½ä¼šä¸ºè¿™äº›éæ ‡å‡†æ“ä½œåˆ›å»ºæ›´å¤æ‚çš„ APIã€‚ /// Benefits // å¥½å¤„ /// Since we can invent a new format we can propose additional benefits: // æ—¢ç„¶æˆ‘ä»¬å¯ä»¥åˆ›é€ ä¸€ç§æ–°çš„å½¢å¼ï¼Œæˆ‘ä»¬å°±å¯ä»¥æå‡ºé¢å¤–çš„å¥½å¤„ï¼š /// - Prevent DOS attacks: We can craft the format in such a way that it's almost impossible to use malicious files to DOS attack a user. Currently, there's a limit on the size of the header of 100MB to prevent parsing extremely large JSON. Also when reading the file, there's a guarantee that addresses in the file do not overlap in any way, meaning when you're loading a file you should never exceed the size of the file in memory // é˜²æ­¢æ‹’ç»æœåŠ¡æ”»å‡»ï¼šæˆ‘ä»¬å¯ä»¥å¯¹æ–‡ä»¶æ ¼å¼è¿›è¡Œç‰¹æ®Šè®¾è®¡ï¼Œä½¿å…¶å‡ ä¹ä¸å¯èƒ½è¢«æ¶æ„æ–‡ä»¶ç”¨äºå‘èµ·æ‹’ç»æœåŠ¡æ”»å‡»ã€‚ç›®å‰ï¼Œæ–‡ä»¶å¤´çš„å¤§å°é™åˆ¶ä¸º 100MBï¼Œä»¥é˜²æ­¢è§£æè¿‡å¤§çš„ JSON æ–‡ä»¶ã€‚æ­¤å¤–ï¼Œåœ¨è¯»å–æ–‡ä»¶æ—¶ï¼Œæˆ‘ä»¬ä¿è¯æ–‡ä»¶ä¸­çš„åœ°å€ä¸ä¼šé‡å ï¼Œè¿™æ„å‘³ç€åŠ è½½æ–‡ä»¶æ—¶ï¼Œå†…å­˜ä¸­çš„å†…å­˜å¤§å°ç»ä¸ä¼šè¶…è¿‡æ–‡ä»¶æœ¬èº«çš„å¤§å°ã€‚ /// - Faster load: PyTorch seems to be the fastest file to load out in the major ML formats. However, it does seem to have an extra copy on CPU, which we can bypass in this lib by using torch.UntypedStorage.from_file. Currently, CPU loading times are extremely fast with this lib compared to pickle. GPU loading times are as fast or faster than PyTorch equivalent. Loading first on CPU with memmapping with torch, and then moving all tensors to GPU seems to be faster too somehow (similar behavior in torch pickle) // æ›´å¿«çš„åŠ è½½é€Ÿåº¦ï¼šåœ¨ä¸»æµæœºå™¨å­¦ä¹ æ ¼å¼ä¸­ï¼ŒPyTorch ä¼¼ä¹æ˜¯åŠ è½½é€Ÿåº¦æœ€å¿«çš„æ–‡ä»¶ã€‚ç„¶è€Œï¼Œå®ƒä¼¼ä¹ä¼šåœ¨ CPU ä¸Šè¿›è¡Œé¢å¤–çš„å‰¯æœ¬å¤åˆ¶ï¼Œè€Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ torch.UntypedStorage.from_file æ¥ç»•è¿‡è¿™ä¸€æ­¥éª¤ã€‚ç›®å‰ï¼Œä¸ pickle ç›¸æ¯”ï¼Œä½¿ç”¨æ­¤åº“çš„ CPU åŠ è½½é€Ÿåº¦éå¸¸å¿«ã€‚GPU åŠ è½½é€Ÿåº¦ä¸ PyTorch çš„ç­‰æ•ˆåŠ è½½é€Ÿåº¦ç›¸å½“ç”šè‡³æ›´å¿«ã€‚å…ˆä½¿ç”¨å†…å­˜æ˜ å°„åœ¨ CPU ä¸ŠåŠ è½½ PyTorchï¼Œç„¶åå†å°†æ‰€æœ‰å¼ é‡ç§»åŠ¨åˆ° GPUï¼Œä¼¼ä¹é€Ÿåº¦ä¹Ÿæ›´å¿«ï¼ˆPyTorch pickle ä¹Ÿå­˜åœ¨ç±»ä¼¼æƒ…å†µï¼‰ã€‚ /// - Lazy loading: in distributed (multi-node or multi-gpu) settings, it's nice to be able to load only part of the tensors on the various models. For BLOOM (. hf: bigscience/bloom) using this format enabled to load the model on 8 GPUs from 10mn with regular PyTorch weights down to 45s. This really speeds up feedbacks loops when developing on the model. For instance you don't have to have separate copies of the weights when changing the distribution strategy (for instance Pipeline Parallelism vs Tensor Parallelism). // å»¶è¿ŸåŠ è½½ï¼šåœ¨åˆ†å¸ƒå¼ï¼ˆå¤šèŠ‚ç‚¹æˆ–å¤š GPUï¼‰ç¯å¢ƒä¸­ï¼Œèƒ½å¤Ÿå»¶è¿ŸåŠ è½½æ˜¯éå¸¸æ–¹ä¾¿çš„ã€‚ ä»…å°†éƒ¨åˆ†å¼ é‡åŠ è½½åˆ°å„ä¸ªæ¨¡å‹ä¸­ã€‚ ä½¿ç”¨è¿™ç§æ ¼å¼çš„ BLOOM å¯ä»¥å°†æ¨¡å‹åœ¨ 8 ä¸ª GPU ä¸Šçš„åŠ è½½æ—¶é—´ä»ä½¿ç”¨å¸¸è§„ PyTorch æƒé‡æ—¶çš„ 10 åˆ†é’Ÿç¼©çŸ­åˆ° 45 ç§’ã€‚è¿™æå¤§åœ°åŠ å¿«äº†æ¨¡å‹å¼€å‘è¿‡ç¨‹ä¸­çš„åé¦ˆå¾ªç¯ã€‚ä¾‹å¦‚ï¼Œåœ¨æ›´æ”¹åˆ†å¸ƒç­–ç•¥ï¼ˆä¾‹å¦‚æµæ°´çº¿å¹¶è¡Œä¸å¼ é‡å¹¶è¡Œï¼‰æ—¶ï¼Œæ— éœ€ç»´æŠ¤å•ç‹¬çš„æƒé‡å‰¯æœ¬ã€‚"
[docs/rs]: https://docs.rs/safetensors/ "Crate safetensors"
[intro/hf-docs]: https://huggingface.co/docs/safetensors "Safetensors is a new simple format for storing tensors safely (as opposed to pickle) and that is still fast (zero-copy). Safetensors is really fast ğŸš€. // Safetensors æ˜¯ä¸€ç§æ–°çš„ç®€å•æ ¼å¼ï¼Œç”¨äºå®‰å…¨åœ°å­˜å‚¨å¼ é‡ï¼ˆä¸ pickle ä¸åŒï¼‰ï¼Œè€Œä¸”é€Ÿåº¦ä»ç„¶å¾ˆå¿«ï¼ˆé›¶æ‹·è´ï¼‰ã€‚Safetensors çœŸçš„å¾ˆå¿«ğŸš€ ã€‚ /// (: import torch ;: tensors = { 'weight1': torch.zeros((1024, 1024)), 'weight2': torch.zeros((1024, 1024)) } # tensors dict for demo ;: from safetensors.torch import save_file ;: save_file(tensors, 'model.safetensors') # save dict as .safetensors format) (: import numpy as np ;: from safetensors.numpy import save_file, load_file ;: tensors = { 'a': np.zeros((2, 2)), 'b': np.zeros((2, 3), dtype=np.uint8) } ;: save_file(tensors, './model.safetensors') # save numpy tensor) (: from safetensors import safe_open ;: tensors = {} # prepare for read) (: with safe_open('model.safetensors', framework='pt', device='cpu') as f: for key in f.keys(): tensors[key] = f.get_tensor(key) # load tensors back into a dict) (: with safe_open('model.safetensors', framework='pt', device=0) as f: ;:: tensor_slice = f.get_slice('embedding') ;:: vocab_size, hidden_dim = tensor_slice.get_shape() ;:: tensor = tensor_slice[:, :hidden_dim] # Loading only part of the tensors (interesting when running on multiple GPU) // (åœ¨å¤š GPU ä¸Šè¿è¡Œæ—¶å¾ˆæœ‰ç”¨) ä»…åŠ è½½éƒ¨åˆ†å¼ é‡)"
[lib-rs.cargo/crates]: https://crates.io/crates/safetensors "(: cargo add -- safetensors) (Apache-2.0) (1.7K SLoC) (30.3 KiB) Provides functions to read and write safetensors which aim to be safer than their PyTorch counterpart. The format is 8 bytes which is an unsized int, being the size of a JSON header, the JSON header refers the `dtype` the `shape` and `data_offsets` which are the offsets for the values in the rest of the file. // æä¾›ç”¨äºè¯»å†™å®‰å…¨å¼ é‡çš„å‡½æ•°ï¼Œæ—¨åœ¨æ¯” PyTorch çš„åŒç±»å¼ é‡æ›´å®‰å…¨ã€‚æ ¼å¼ä¸º 8 å­—èŠ‚ï¼Œæ˜¯ä¸€ä¸ªæ— å¤§å°çš„æ•´æ•°ï¼Œå¤§å°ä¸ JSON å¤´éƒ¨ç›¸åŒã€‚JSON å¤´éƒ¨åŒ…å« `dtype`ã€`shape` å’Œ `data_offsets`ï¼Œå…¶ä¸­ `data_offsets` æ˜¯æ–‡ä»¶ä¸­å…¶ä½™éƒ¨åˆ†å€¼çš„åç§»é‡ã€‚ (src: gh:huggingface/safetensors.git)"
[lib.py<rs>.pip/pypi]: https://pypi.org/project/safetensors/ "(: pip install -- safetensors) (License: Apache Software License) (Requires: Python >=3.9)  (src: gh:huggingface/safetensors.git)"

