[src/gh]: https://github.com/microsoft/BitNet.git "(MIT) (Languages: Python 50.5%, C++ 48.5%, Other 1.0%) Official inference framework for 1-bit LLMs // 1 位 LLMs 的官方推理框架 /// bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support fast and lossless inference of 1.58-bit models on CPU and GPU (NPU support will coming next). // bitnet.cpp 是 1-bit LLMs（例如 BitNet b1.58）的官方推理框架。它提供了一套优化的内核，支持在 CPU 和 GPU 上对 1.58-bit 模型进行快速无损推理（NPU 支持即将到来）。 /// The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of 1.37x to 5.07x on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by 55.4% to 70.0%, further boosting overall efficiency. On x86 CPUs, speedups range from 2.37x to 6.17x with energy reductions between 71.9% to 82.2%. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. Please refer to the technical report for more details. // bitnet.cpp 的首次发布是为了支持在 CPU 上的推理。bitnet.cpp 在 ARM CPU 上实现了 1.37x 到 5.07x 的加速，较大模型的性能提升更为显著。此外，它将能耗降低了 55.4% 到 70.0%，进一步提升了整体效率。在 x86 CPU 上，加速范围从 2.37x 到 6.17x，能耗降低在 71.9% 到 82.2% 之间。此外，bitnet.cpp 可以在单个 CPU 上运行 100B BitNet b1.58 模型，达到与人阅读相当的速度（每秒 5-7 个 token），显著提升了在本地设备上运行 LLMs 的潜力。请参考技术报告了解更多详情。"
[demo/azurewebsites.net]: https://bitnet-demo.azurewebsites.net/ "Welcome to BitNet. How can I help you today? /// By messaging BitNet, you agree to our Terms and Privacy Policy."
