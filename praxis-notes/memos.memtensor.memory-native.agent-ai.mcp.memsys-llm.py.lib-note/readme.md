[src/gh]: https://github.com/MemTensor/MemOS.git "(Apache-2.0) (Languages: Python 99.7%, Other 0.3%) Build memory-native AI agents with Memory OS â€” an open-source framework for long-term memory, retrieval, and adaptive learning in large language models. Agent Memory | Memory System | Memory Management | Memory MCP | MCP System | LLM Memory | Agents Memory System | // ä½¿ç”¨ Memory OS æ„å»ºå†…å­˜åŸç”Ÿ AI ä»£ç†â€”â€”ä¸€ä¸ªç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„é•¿æœŸè®°å¿†ã€æ£€ç´¢å’Œè‡ªé€‚åº”å­¦ä¹ çš„å¼€æºæ¡†æ¶ã€‚Agent Memory | Memory System | Memory Management | Memory MCP | MCP System | LLM Memory | Agents Memory System | /// MemOS: Memory Operating System for AI Agents // MemOSï¼šAI ä»£ç†çš„å†…å­˜æ“ä½œç³»ç»Ÿ /// MemOS is an open-source Agent Memory framework that empowers AI agents with long-term memory, personality consistency, and contextual recall. It enables agents to remember past interactions, learn over time, and build evolving identities across sessions. // MemOS æ˜¯ä¸€ä¸ªå¼€æºçš„ä»£ç†å†…å­˜æ¡†æ¶ï¼Œä¸º AI ä»£ç†èµ‹äºˆé•¿æœŸè®°å¿†ã€æ€§æ ¼ä¸€è‡´æ€§å’Œä¸Šä¸‹æ–‡å¬å›èƒ½åŠ›ã€‚å®ƒä½¿ä»£ç†èƒ½å¤Ÿè®°ä½è¿‡å»çš„äº¤äº’ï¼Œéšæ—¶é—´å­¦ä¹ ï¼Œå¹¶åœ¨ä¸åŒä¼šè¯ä¸­æ„å»ºä¸æ–­æ¼”å˜çš„èº«ä»½ã€‚ /// Designed for AI companions, role-playing NPCs, and multi-agent systems, MemOS provides a unified API for memory representation, retrieval, and update â€” making it the foundation for next-generation memory-augmented AI agents. // ä¸“ä¸º AI ä¼´ä¾£ã€è§’è‰²æ‰®æ¼” NPC å’Œå¤šä»£ç†ç³»ç»Ÿè®¾è®¡ï¼ŒMemOS æä¾›äº†ä¸€å¥—ç»Ÿä¸€çš„å†…å­˜è¡¨ç¤ºã€æ£€ç´¢å’Œæ›´æ–° APIï¼Œä½¿å…¶æˆä¸ºä¸‹ä¸€ä»£è®°å¿†å¢å¼ºå‹ AI ä»£ç†çš„åŸºç¡€ã€‚ /// Detailed Evaluation Results // è¯¦ç»†è¯„ä¼°ç»“æœ /// - We use gpt-4o-mini as the processing and judging LLM and bge-m3 as embedding model in MemOS evaluation. // æˆ‘ä»¬åœ¨ MemOS è¯„ä¼°ä¸­ä½¿ç”¨äº† gpt-4o-mini ä½œä¸ºå¤„ç†å’Œè¯„åˆ¤çš„ LLMï¼Œä»¥åŠ bge-m3 ä½œä¸ºåµŒå…¥æ¨¡å‹ã€‚ /// - The evaluation was conducted under conditions that align various settings as closely as possible. Reproduce the results with our scripts at evaluation. // è¯„ä¼°æ˜¯åœ¨å°½å¯èƒ½ä½¿å„ç§è®¾ç½®ä¿æŒä¸€è‡´çš„æ¡ä»¶ ä¸‹è¿›è¡Œçš„ã€‚è¯·ä½¿ç”¨æˆ‘ä»¬çš„è„šæœ¬åœ¨ evaluation å¤ç°ç»“æœã€‚ /// - Check the full search and response details at huggingface https://huggingface.co/datasets/MemTensor/MemOS_eval_result. // åœ¨ huggingface ä¸ŠæŸ¥çœ‹å®Œæ•´çš„æœç´¢å’Œå“åº”è¯¦æƒ… https://huggingface.co/datasets/MemTensor/MemOS_eval_result. /// > ğŸ’¡ MemOS outperforms all other methods (Mem0, Zep, Memobase, SuperMemory et al.) across all benchmarks! // ğŸ’¡ MemOS åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­éƒ½ä¼˜äºæ‰€æœ‰å…¶ä»–æ–¹æ³•ï¼ˆMem0ã€Zepã€Memobaseã€SuperMemory ç­‰ï¼‰ï¼ /// âœ¨ Key Features // âœ¨ ä¸»è¦åŠŸèƒ½ /// - ğŸ§  Memory-Augmented Generation (MAG): Provides a unified API for memory operations, integrating with LLMs to enhance chat and reasoning with contextual memory retrieval. // ğŸ§  å¢å¼ºè®°å¿†ç”Ÿæˆ (MAG)ï¼šæä¾›ç»Ÿä¸€çš„å†…å­˜æ“ä½œ APIï¼Œä¸ LLMs é›†æˆï¼Œé€šè¿‡ä¸Šä¸‹æ–‡è®°å¿†æ£€ç´¢å¢å¼ºèŠå¤©å’Œæ¨ç†èƒ½åŠ›ã€‚ /// - ğŸ“¦ Modular Memory Architecture (MemCube): A flexible and modular architecture that allows for easy integration and management of different memory types. // ğŸ“¦ æ¨¡å—åŒ–è®°å¿†æ¶æ„ (MemCube)ï¼šä¸€ç§çµæ´»ä¸”æ¨¡å—åŒ–çš„æ¶æ„ï¼Œå…è®¸è½»æ¾é›†æˆå’Œç®¡ç†ä¸åŒç±»å‹çš„å†…å­˜ã€‚ /// - ğŸ’¾ Multiple Memory Types: // ğŸ’¾ å¤šç§å†…å­˜ç±»å‹ï¼š /// - - Textual Memory: For storing and retrieving unstructured or structured text knowledge. // æ–‡æœ¬è®°å¿†ï¼šç”¨äºå­˜å‚¨å’Œæ£€ç´¢éç»“æ„åŒ–æˆ–ç»“æ„åŒ–æ–‡æœ¬çŸ¥è¯†ã€‚ /// - - Activation Memory: Caches key-value pairs (KVCacheMemory) to accelerate LLM inference and context reuse. // æ¿€æ´»å†…å­˜ï¼šç¼“å­˜é”®å€¼å¯¹ï¼ˆ KVCacheMemory ï¼‰ä»¥åŠ é€Ÿ LLM æ¨ç†å’Œä¸Šä¸‹æ–‡é‡ç”¨ã€‚ /// - - Parametric Memory: Stores model adaptation parameters (e.g., LoRA weights). // å‚æ•°åŒ–å†…å­˜ï¼šå­˜å‚¨æ¨¡å‹è‡ªé€‚åº”å‚æ•°ï¼ˆä¾‹å¦‚ï¼ŒLoRA æƒé‡ï¼‰ã€‚ /// - ğŸ”Œ Extensible: Easily extend and customize memory modules, data sources, and LLM integrations. // ğŸ”Œ å¯æ‰©å±•ï¼šè½»æ¾æ‰©å±•å’Œè‡ªå®šä¹‰å†…å­˜æ¨¡å—ã€æ•°æ®æºå’Œ LLM é›†æˆã€‚ /// ğŸ“œ Citation // ğŸ“œ å¼•è¯ /// Note // æ³¨æ„ /// We publicly released the Short Version on May 28, 2025, making it the earliest work to propose the concept of a Memory Operating System for LLMs. // æˆ‘ä»¬åœ¨ 2025 å¹´ 5 æœˆ 28 æ—¥å…¬å¼€å‘å¸ƒäº†ç®€ç‰ˆï¼Œä½¿å…¶æˆä¸ºæœ€æ—©æå‡º LLMs å†…å­˜æ“ä½œç³»ç»Ÿæ¦‚å¿µçš„å·¥ä½œã€‚ /// If you use MemOS in your research, we would appreciate citations to our papers. // å¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨ MemOSï¼Œæˆ‘ä»¬å°†ä¸èƒœæ„Ÿæ¿€ä½ èƒ½å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡ã€‚ /// (~ @article{li2025memos_long, title={MemOS: A Memory OS for AI System}, author={Li, Zhiyu and Song, Shichao and Xi, Chenyang and Wang, Hanyu and Tang, Chen and Niu, Simin and Chen, Ding and Yang, Jiawei and Li, Chunyu and Yu, Qingchen and Zhao, Jihao and Wang, Yezhaohui and Liu, Peng and Lin, Zehao and Wang, Pengyuan and Huo, Jiahao and Chen, Tianyi and Chen, Kai and Li, Kehang and Tao, Zhen and Ren, Junpeng and Lai, Huayi and Wu, Hao and Tang, Bo and Wang, Zhenren and Fan, Zhaoxin and Zhang, Ningyu and Zhang, Linfeng and Yan, Junchi and Yang, Mingchuan and Xu, Tong and Xu, Wei and Chen, Huajun and Wang, Haofeng and Yang, Hongkang and Zhang, Wentao and Xu, Zhi-Qin John and Chen, Siheng and Xiong, Feiyu}, journal={arXiv preprint arXiv:2507.03724}, year={2025}, url={https://arxiv.org/abs/2507.03724} }) (~ @article{li2025memos_short, title={MemOS: An Operating System for Memory-Augmented Generation (MAG) in Large Language Models}, author={Li, Zhiyu and Song, Shichao and Wang, Hanyu and Niu, Simin and Chen, Ding and Yang, Jiawei and Xi, Chenyang and Lai, Huayi and Zhao, Jihao and Wang, Yezhaohui and others}, journal={arXiv preprint arXiv:2505.22101}, year={2025}, url={https://arxiv.org/abs/2505.22101} }) (~ @article{yang2024memory3, author = {Yang, Hongkang and Zehao, Lin and Wenjin, Wang and Wu, Hao and Zhiyu, Li and Tang, Bo and Wenqiang, Wei and Wang, Jinbo and Zeyun, Tang and Song, Shichao and Xi, Chenyang and Yu, Yu and Kai, Chen and Xiong, Feiyu and Tang, Linpeng and Weinan, E}, title = {Memory$^3$: Language Modeling with Explicit Memory}, journal = {Journal of Machine Learning}, year = {2024}, volume = {3}, number = {3}, pages = {300--346}, issn = {2790-2048}, doi = {https://doi.org/10.4208/jml.240708}, url = {https://global-sci.com/article/91443/memory3-language-modeling-with-explicit-memory} })"
[lib.pip/pypi]: https://pypi.org/project/MemoryOS/ "(: pip install -- MemoryOS) (License: Apache Software License (Apache-2.0)) (Requires: Python >=3.10) (Provides-Extra: [all], [mem-reader], [mem-scheduler], [mem-user], [pref-mem], [tree-mem]) Intelligence Begins with Memory // æ™ºèƒ½å§‹äºè®°å¿† (src: gh:MemTensor/MemOS.git)"
[site/net]: https://memos.openmem.net/ "INTELLIGENCE BEGINS WITH MEMORY // æ™ºæ…§å§‹äºè®°å¿† /// MemOS Gives AI Continuous Memory & Growth // MemOS èµ‹äºˆ AI æŒç»­è®°å¿†ä¸æˆé•¿ /// AI Provides scalable memory for AI, ensuring consistent understanding and personalization across tasks and scenarios // AI æä¾›å¯æ‰©å±•çš„å†…å­˜ï¼Œç¡®ä¿åœ¨ä»»åŠ¡å’Œåœºæ™¯ä¸­å§‹ç»ˆå¦‚ä¸€çš„ç†è§£å’Œä¸ªæ€§åŒ– /// Not only can it remember, but itâ€™s fast // å®ƒä¸ä»…èƒ½å¤Ÿè®°å¿†ï¼Œè€Œä¸”é€Ÿåº¦å¿« /// Production-grade memory service with millisecond-level response // å·¥ä¸šçº§å†…å­˜æœåŠ¡ï¼Œæ¯«ç§’çº§å“åº” /// - MemOS provides businesses with highly available, scalable memory infrastructure // MemOS ä¸ºå•†ä¸šæä¾›é«˜å¯ç”¨ã€å¯æ‰©å±•çš„å†…å­˜åŸºç¡€è®¾æ–½ /// - Whether add or search, requests complete in milliseconds // æ— è®ºæ˜¯æ·»åŠ è¿˜æ˜¯æœç´¢ï¼Œè¯·æ±‚å‡åœ¨æ¯«ç§’å†…å®Œæˆ /// - Each call is stable and reliable, and every response is fast and predictable // æ¯æ¬¡è°ƒç”¨éƒ½ç¨³å®šå¯é ï¼Œæ¯æ¬¡å“åº”éƒ½å¿«é€Ÿå¯é¢„æµ‹ /// Unlock Custom Intelligence with MemOS // è§£é”å®šåˆ¶åŒ–æ™ºèƒ½ï¼Œä½¿ç”¨ MemOS /// A Memory-Native Framework for Building Intelligent Systems that Remember, Adapt, and Evolve // ä¸€ä¸ªç”¨äºæ„å»ºèƒ½å¤Ÿè®°å¿†ã€é€‚åº”å’Œè¿›åŒ–çš„æ™ºèƒ½ç³»ç»Ÿçš„è®°å¿†åŸç”Ÿæ¡†æ¶ /// - Structured Memory Architecture // ç»“æ„åŒ–è®°å¿†æ¶æ„ ///: MemOS unifies memory types into a layered system, enabling dynamic retrieval, updates, and smarter adaptive learning // MemOS å°†ä¸åŒç±»å‹çš„è®°å¿†ç»Ÿä¸€åˆ°ä¸€ä¸ªåˆ†å±‚ç³»ç»Ÿä¸­ï¼Œæ”¯æŒåŠ¨æ€æ£€ç´¢ã€æ›´æ–°å’Œæ›´æ™ºèƒ½çš„è‡ªé€‚åº”å­¦ä¹  /// - Predictive & Asynchronous Scheduling // é¢„æµ‹å¼ä¸å¼‚æ­¥è°ƒåº¦ ///: MemOS employs predictive, intent-aware scheduling to preload relevant memory before it is neededâ€”based on dialogue history, task semantics, or environmental cues. // MemOS é‡‡ç”¨é¢„æµ‹å¼ã€æ„å›¾æ„ŸçŸ¥çš„è°ƒåº¦æ–¹å¼ï¼Œåœ¨éœ€è¦ä¹‹å‰é¢„åŠ è½½ç›¸å…³è®°å¿†â€”â€”åŸºäºå¯¹è¯å†å²ã€ä»»åŠ¡è¯­ä¹‰æˆ–ç¯å¢ƒæç¤ºã€‚ /// - Predictability & Async Scheduling // å¯é¢„æµ‹æ€§ä¸å¼‚æ­¥è°ƒåº¦ ///: MemOS enables memory sharing across models, devices, and apps via MIP, making memory persistent and portable for collaboration and adaptability // MemOS é€šè¿‡ MIP å®ç°è·¨æ¨¡å‹ã€è®¾å¤‡å’Œåº”ç”¨çš„å†…å­˜å…±äº«ï¼Œä½¿å†…å­˜æŒä¹…åŒ–ä¸”å¯ç§»æ¤ï¼Œä»¥æ”¯æŒåä½œå’Œé€‚åº”æ€§"
[docs/.site]: https://memos-docs.openmem.net/overview/introduction "MemOS (Memory Operating System) is a memory management operating system designed for AI applications. // MemOSï¼ˆå†…å­˜æ“ä½œç³»ç»Ÿï¼‰æ˜¯ä¸º AI åº”ç”¨è®¾è®¡çš„å†…å­˜ç®¡ç†æ“ä½œç³»ç»Ÿã€‚ /// Its goal is: to enable your AI system to have long-term memory like a human, not only remembering what users have said but also actively invoking, updating, and scheduling these memories. // å®ƒçš„ç›®æ ‡æ˜¯ï¼šè®©ä½ çš„ AI ç³»ç»Ÿèƒ½å¤Ÿåƒäººç±»ä¸€æ ·æ‹¥æœ‰é•¿æœŸè®°å¿†ï¼Œä¸ä»…è®°ä½ç”¨æˆ·è¯´è¿‡çš„è¯ï¼Œè¿˜èƒ½ä¸»åŠ¨è°ƒç”¨ã€æ›´æ–°å’Œè°ƒåº¦è¿™äº›è®°å¿†ã€‚ /// For developers, MemOS is like a database for applications: you donâ€™t need to reinvent the wheel to solve the problem of â€œhow AI remembers.â€ By simply calling the services provided by MemOS, you can easily equip your Agent or application with â€œmemory capability.â€ // å¯¹äºå¼€å‘è€…æ¥è¯´ï¼ŒMemOS å°±åƒä¸€ä¸ªåº”ç”¨ç¨‹åºçš„æ•°æ®åº“ï¼šä½ ä¸éœ€è¦é‡æ–°å‘æ˜è½®å­æ¥è§£å†³â€œAI å¦‚ä½•è®°å¿†â€çš„é—®é¢˜ã€‚åªéœ€è°ƒç”¨ MemOS æä¾›çš„æœåŠ¡ï¼Œä½ å°±å¯ä»¥è½»æ¾åœ°ä¸ºä½ çš„ Agent æˆ–åº”ç”¨ç¨‹åºé…å¤‡â€œè®°å¿†èƒ½åŠ›â€ã€‚ /// Native memory in large models has limitations: // å¤§å‹æ¨¡å‹çš„æœ¬åœ°å†…å­˜å­˜åœ¨å±€é™æ€§ï¼š /// - Limited context: No matter how large the token window is, it cannot carry long-term knowledge. // ä¸Šä¸‹æ–‡æœ‰é™ï¼šæ— è®º token çª—å£æœ‰å¤šå¤§ï¼Œå®ƒéƒ½æ— æ³•æ‰¿è½½é•¿æœŸçŸ¥è¯†ã€‚ /// - Severe forgetting: Preferences mentioned by the user last week may disappear in the next conversation. // ä¸¥é‡é—å¿˜ï¼šç”¨æˆ·ä¸Šå‘¨æåˆ°çš„åå¥½å¯èƒ½åœ¨ä¸‹ä¸€æ¬¡å¯¹è¯ä¸­æ¶ˆå¤±ã€‚ /// - Difficult to manage: As interactions increase, memories become chaotic, requiring extra logic for developers to handle. // éš¾ä»¥ç®¡ç†ï¼šéšç€äº¤äº’çš„å¢åŠ ï¼Œè®°å¿†å˜å¾—æ··ä¹±ï¼Œéœ€è¦å¼€å‘è€…é¢å¤–é€»è¾‘æ¥å¤„ç†ã€‚ /// The value of MemOS lies in its ability to abstract the memory layer, allowing you to focus only on business logic: // MemOS çš„ä»·å€¼åœ¨äºå…¶æŠ½è±¡å†…å­˜å±‚çš„èƒ½åŠ›ï¼Œè®©ä½ åªéœ€ä¸“æ³¨äºä¸šåŠ¡é€»è¾‘ï¼š /// - No more writing complex â€œlong text concatenationsâ€ or â€œextra database queries.â€ // ä¸å†éœ€è¦ç¼–å†™å¤æ‚çš„â€œé•¿æ–‡æœ¬è¿æ¥â€æˆ–â€œé¢å¤–æ•°æ®åº“æŸ¥è¯¢â€ã€‚ /// - Memory can be reused and extended like a module, and even shared across different Agents and systems. // å†…å­˜å¯ä»¥åƒæ¨¡å—ä¸€æ ·è¢«é‡ç”¨å’Œæ‰©å±•ï¼Œç”šè‡³å¯ä»¥åœ¨ä¸åŒçš„ Agent å’Œç³»ç»Ÿä¹‹é—´å…±äº«ã€‚ /// - With proactive scheduling and multi-layer management, memory retrieval is faster and more accurate, significantly reducing hallucinations. // é€šè¿‡ä¸»åŠ¨è°ƒåº¦å’Œå¤šå±‚çº§ç®¡ç†ï¼Œè®°å¿†æ£€ç´¢æ›´å¿«æ›´å‡†ç¡®ï¼Œæ˜¾è‘—å‡å°‘äº†å¹»è§‰ã€‚ /// In short: MemOS transforms AI from a one-off conversation machine into a continuously growing partner. // ç®€è€Œè¨€ä¹‹ï¼šMemOS å°† AI ä»ä¸€æ¬¡æ€§å¯¹è¯æœºå™¨è½¬å˜ä¸ºæŒç»­æˆé•¿çš„ä¼™ä¼´ã€‚ /// What MemOS Can Do // MemOS çš„åŠŸèƒ½ /// - Personalized conversations: Remember the userâ€™s name, habits, interests, and instruction preferences, and automatically supplement them next time. // ä¸ªæ€§åŒ–å¯¹è¯ï¼šè®°ä½ç”¨æˆ·çš„åå­—ã€ä¹ æƒ¯ã€å…´è¶£å’ŒæŒ‡ä»¤åå¥½ï¼Œå¹¶åœ¨ä¸‹æ¬¡è‡ªåŠ¨è¡¥å……ã€‚ /// - Team knowledge base: Convert fragmented conversations into structured knowledge for multiple Agents to collaborate. // å›¢é˜ŸçŸ¥è¯†åº“ï¼šå°†é›¶æ•£çš„å¯¹è¯è½¬åŒ–ä¸ºç»“æ„åŒ–çŸ¥è¯†ï¼Œä¾›å¤šä¸ªæ™ºèƒ½ä½“åä½œä½¿ç”¨ã€‚ /// - Task continuity: Maintain memory across sessions and applications, enabling AI to handle long workflows with ease. // ä»»åŠ¡è¿ç»­æ€§ï¼šä¿æŒè·¨ä¼šè¯å’Œåº”ç”¨ç¨‹åºçš„è®°å¿†ï¼Œä½¿ AI èƒ½å¤Ÿè½»æ¾å¤„ç†é•¿æµç¨‹ã€‚ /// - Multi-layer memory scheduling: Invoke the most suitable memory for different needs, improving performance and accuracy. // å¤šå±‚å†…å­˜è°ƒåº¦ï¼šæ ¹æ®ä¸åŒéœ€æ±‚è°ƒç”¨æœ€åˆé€‚çš„å†…å­˜ï¼Œæå‡æ€§èƒ½å’Œå‡†ç¡®æ€§ã€‚ /// - Open extensibility: Can be used as a standalone API or integrated into existing frameworks (official integration guide coming soon). // å¼€æ”¾æ‰©å±•æ€§ï¼šå¯å•ç‹¬ä½œä¸º API ä½¿ç”¨ï¼Œæˆ–é›†æˆåˆ°ç°æœ‰æ¡†æ¶ä¸­ï¼ˆå®˜æ–¹é›†æˆæŒ‡å—å³å°†å‘å¸ƒï¼‰ã€‚"
[paper:detailed-principles/arxiv]: https://arxiv.org/abs/2507.03724 "(License: CC BY 4.0) (arXiv:2507.03724v4 [cs.CL]) [Submitted on 4 Jul 2025 (v1), last revised 3 Dec 2025 (this version, v4)] (Computation and Language (cs.CL)) MemOS: A Memory OS for AI System // MemOSï¼šä¸€ä¸ªç”¨äº AI ç³»ç»Ÿçš„å†…å­˜æ“ä½œç³»ç»Ÿ /// Large Language Models (LLMs) have become an essential infrastructure for Artificial General Intelligence (AGI), yet their lack of well-defined memory management systems hinders the development of long-context reasoning, continual personalization, and knowledge this http URL models mainly rely on static parameters and short-lived contextual states, limiting their ability to track user preferences or update knowledge over extended this http URL Retrieval-Augmented Generation (RAG) introduces external knowledge in plain text, it remains a stateless workaround without lifecycle control or integration with persistent this http URL work has modeled the training and inference cost of LLMs from a memory hierarchy perspective, showing that introducing an explicit memory layer between parameter memory and external retrieval can substantially reduce these costs by externalizing specific knowledge. Beyond computational efficiency, LLMs face broader challenges arising from how information is distributed over time and context, requiring systems capable of managing heterogeneous knowledge spanning different temporal scales and sources. To address this challenge, we propose MemOS, a memory operating system that treats memory as a manageable system resource. It unifies the representation, scheduling, and evolution of plaintext, activation-based, and parameter-level memories, enabling cost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates both memory content and metadata such as provenance and versioning. MemCubes can be composed, migrated, and fused over time, enabling flexible transitions between memory types and bridging retrieval with parameter-based learning. MemOS establishes a memory-centric system framework that brings controllability, plasticity, and evolvability to LLMs, laying the foundation for continual learning and personalized modeling. // å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²æˆä¸ºé€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„å…³é”®åŸºç¡€è®¾æ–½ï¼Œä½†å®ƒä»¬ç¼ºä¹æ˜ç¡®çš„å†…å­˜ç®¡ç†ç³»ç»Ÿï¼Œé˜»ç¢äº†é•¿ä¸Šä¸‹æ–‡æ¨ç†ã€æŒç»­ä¸ªæ€§åŒ–ä»¥åŠçŸ¥è¯†æ›´æ–°çš„å‘å±•ã€‚è¿™äº›æ¨¡å‹ä¸»è¦ä¾èµ–é™æ€å‚æ•°å’ŒçŸ­æ—¶ä¸Šä¸‹æ–‡çŠ¶æ€ï¼Œé™åˆ¶äº†å®ƒä»¬è·Ÿè¸ªç”¨æˆ·åå¥½æˆ–é•¿æ—¶é—´æ›´æ–°çŸ¥è¯†çš„èƒ½åŠ›ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å¼•å…¥äº†çº¯æ–‡æœ¬çš„å¤–éƒ¨çŸ¥è¯†ï¼Œä½†ä»ç„¶æ˜¯ä¸€ç§æ— çŠ¶æ€çš„å·¥ä½œæ–¹æ¡ˆï¼Œç¼ºä¹ç”Ÿå‘½å‘¨æœŸæ§åˆ¶æˆ–ä¸æŒä¹…åŒ–å­˜å‚¨çš„é›†æˆã€‚è¿™é¡¹å·¥ä½œä»å†…å­˜å±‚æ¬¡ç»“æ„çš„è§’åº¦å¯¹ LLMs çš„è®­ç»ƒå’Œæ¨ç†æˆæœ¬è¿›è¡Œäº†å»ºæ¨¡ï¼Œè¡¨æ˜åœ¨å‚æ•°å†…å­˜å’Œå¤–éƒ¨æ£€ç´¢ä¹‹é—´å¼•å…¥æ˜¾å¼å†…å­˜å±‚å¯ä»¥é€šè¿‡å°†ç‰¹å®šçŸ¥è¯†å¤–éƒ¨åŒ–æ¥æ˜¾è‘—é™ä½è¿™äº›æˆæœ¬ã€‚é™¤äº†è®¡ç®—æ•ˆç‡ä¹‹å¤–ï¼ŒLLMs è¿˜é¢ä¸´ç€ä¿¡æ¯åœ¨æ—¶é—´å’Œä¸Šä¸‹æ–‡ä¸­åˆ†å¸ƒæ‰€å¸¦æ¥çš„æ›´å¹¿æ³›æŒ‘æˆ˜ï¼Œéœ€è¦èƒ½å¤Ÿç®¡ç†è·¨è¶Šä¸åŒæ—¶é—´å°ºåº¦å’Œæ¥æºçš„å¼‚æ„çŸ¥è¯†ç³»ç»Ÿã€‚ ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº† MemOSï¼Œä¸€ä¸ªå°†å†…å­˜è§†ä¸ºå¯ç®¡ç†ç³»ç»Ÿèµ„æºçš„å†…å­˜æ“ä½œç³»ç»Ÿã€‚å®ƒç»Ÿä¸€äº†æ˜æ–‡ã€åŸºäºæ¿€æ´»å’Œå‚æ•°çº§å†…å­˜çš„è¡¨ç¤ºã€è°ƒåº¦å’Œæ¼”è¿›ï¼Œå®ç°äº†æˆæœ¬é«˜æ•ˆçš„å­˜å‚¨å’Œæ£€ç´¢ã€‚ä½œä¸ºåŸºæœ¬å•å…ƒï¼ŒMemCube å°è£…äº†å†…å­˜å†…å®¹å’Œå…ƒæ•°æ®ï¼Œå¦‚æ¥æºå’Œç‰ˆæœ¬ä¿¡æ¯ã€‚MemCube å¯ä»¥éšæ—¶é—´è¿›è¡Œç»„åˆã€è¿ç§»å’Œèåˆï¼Œå®ç°å†…å­˜ç±»å‹ä¹‹é—´çš„çµæ´»è½¬æ¢ï¼Œå¹¶è¿æ¥æ£€ç´¢ä¸åŸºäºå‚æ•°çš„å­¦ä¹ ã€‚ MemOS å»ºç«‹äº†ä¸€ä¸ªä»¥å†…å­˜ä¸ºä¸­å¿ƒçš„ç³»ç»Ÿæ¡†æ¶ï¼Œä¸º LLMs å¸¦æ¥äº†å¯æ§æ€§ã€å¯å¡‘æ€§å’Œå¯è¿›åŒ–æ€§ä¸ºæŒç»­å­¦ä¹ å’Œä¸ªæ€§åŒ–å»ºæ¨¡å¥ å®šåŸºç¡€ã€‚"
[paper:mag/arxiv]: https://arxiv.org/abs/2505.22101 "(License: CC BY 4.0) (arXiv:2505.22101v1 [cs.CL]) [Submitted on 28 May 2025] (Computation and Language (cs.CL)) MemOS: An Operating System for Memory-Augmented Generation (MAG) in Large Language Models // MemOSï¼šä¸€ç§ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­è®°å¿†å¢å¼ºç”Ÿæˆï¼ˆMAGï¼‰çš„æ“ä½œç³»ç»Ÿ /// Large Language Models (LLMs) have emerged as foundational infrastructure in the pursuit of Artificial General Intelligence (AGI). Despite their remarkable capabilities in language perception and generation, current LLMs fundamentally lack a unified and structured architecture for handling memory. They primarily rely on parametric memory (knowledge encoded in model weights) and ephemeral activation memory (context-limited runtime states). While emerging methods like Retrieval-Augmented Generation (RAG) incorporate plaintext memory, they lack lifecycle management and multi-modal integration, limiting their capacity for long-term knowledge evolution. To address this, we introduce MemOS, a memory operating system designed for LLMs that, for the first time, elevates memory to a first-class operational resource. It builds unified mechanisms for representation, organization, and governance across three core memory types: parametric, activation, and plaintext. At its core is the MemCube, a standardized memory abstraction that enables tracking, fusion, and migration of heterogeneous memory, while offering structured, traceable access across tasks and contexts. MemOS establishes a memory-centric execution framework with strong controllability, adaptability, and evolvability. It fills a critical gap in current LLM infrastructure and lays the groundwork for continual adaptation, personalized intelligence, and cross-platform coordination in next-generation intelligent systems. // å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²æˆä¸ºè¿½æ±‚é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„åŸºç¡€è®¾æ–½ã€‚å°½ç®¡å®ƒä»¬åœ¨è¯­è¨€æ„ŸçŸ¥å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å½“å‰çš„ LLMs åœ¨å¤„ç†è®°å¿†æ–¹é¢ç¼ºä¹ç»Ÿä¸€ä¸”ç»“æ„åŒ–çš„æ¶æ„ã€‚å®ƒä»¬ä¸»è¦ä¾èµ–å‚æ•°åŒ–è®°å¿†ï¼ˆç¼–ç åœ¨æ¨¡å‹æƒé‡ä¸­çš„çŸ¥è¯†ï¼‰å’ŒçŸ­æš‚æ¿€æ´»è®°å¿†ï¼ˆå—é™äºä¸Šä¸‹æ–‡çš„è¿è¡Œæ—¶çŠ¶æ€ï¼‰ã€‚è™½ç„¶åƒæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰æ–°å…´æ–¹æ³•ç»“åˆäº†æ˜æ–‡è®°å¿†ï¼Œä½†å®ƒä»¬ç¼ºä¹ç”Ÿå‘½å‘¨æœŸç®¡ç†å’Œå¤šæ¨¡æ€é›†æˆï¼Œé™åˆ¶äº†å®ƒä»¬é•¿æœŸçŸ¥è¯†æ¼”åŒ–çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº† MemOSï¼Œä¸€ä¸ªä¸º LLMs è®¾è®¡çš„è®°å¿†æ“ä½œç³»ç»Ÿï¼Œå®ƒé¦–æ¬¡å°†è®°å¿†æå‡ä¸ºä¸€çº§æ“ä½œèµ„æºã€‚å®ƒæ„å»ºäº†ä¸‰ä¸ªæ ¸å¿ƒè®°å¿†ç±»å‹ï¼ˆå‚æ•°åŒ–ã€æ¿€æ´»å’Œæ˜æ–‡ï¼‰çš„è¡¨ç¤ºã€ç»„ç»‡å’Œæ²»ç†çš„ç»Ÿä¸€æœºåˆ¶ã€‚å…¶æ ¸å¿ƒæ˜¯ MemCubeï¼Œä¸€ç§æ ‡å‡†åŒ–çš„è®°å¿†æŠ½è±¡ï¼Œå®ƒèƒ½å¤Ÿè·Ÿè¸ªã€èåˆå’Œè¿ç§»å¼‚æ„è®°å¿†ï¼ŒåŒæ—¶æä¾›è·¨ä»»åŠ¡å’Œä¸Šä¸‹æ–‡çš„ç»“æ„åŒ–ã€å¯è¿½æº¯è®¿é—®ã€‚ MemOS å»ºç«‹äº†ä¸€ä¸ªä»¥å†…å­˜ä¸ºä¸­å¿ƒçš„æ‰§è¡Œæ¡†æ¶ï¼Œå…·æœ‰å¼ºå¤§çš„å¯æ§æ€§ã€é€‚åº”æ€§å’Œå¯è¿›åŒ–æ€§ã€‚ å®ƒå¡«è¡¥äº†å½“å‰ LLM åŸºç¡€è®¾æ–½ä¸­çš„ä¸€ä¸ªå…³é”®ç©ºç™½ï¼Œå¹¶ä¸ºæŒç»­é€‚åº”ã€ä¸ªæ€§åŒ–æ™ºèƒ½ï¼Œä»¥åŠä¸‹ä¸€ä»£æ™ºèƒ½ç³»ç»Ÿä¸­çš„è·¨å¹³å°åè°ƒå¥ å®šäº†åŸºç¡€ã€‚"
[paper:mem3-explicit-memory/global-sci:jml]: https://global-sci.com/jml/article/view/13203 "(doi: doi.org/10.4208/jml.240708) Memory^3: Language Modeling with Explicit Memory // Memory^3: å¸¦æ˜¾å¼è®°å¿†çš„è¯­è¨€å»ºæ¨¡ ///  Keywords: Large language model, Explicit memory, Large-scale pretraining, Efficient inference, AI database. // å…³é”®è¯ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ã€æ˜¾å¼è®°å¿†ã€å¤§è§„æ¨¡é¢„è®­ç»ƒã€é«˜æ•ˆæ¨ç†ã€AI æ•°æ®åº“ã€‚ /// Abstract // æ‘˜è¦ /// The training and inference of large language models (LLMs) are together a costly process that transports knowledge from raw data to meaningful computation. Inspired by the memory hierarchy of the human brain, we reduce this cost by equipping LLMs with explicit memory, a memory format cheaper than model parameters and text retrieval-augmented generation (RAG). Conceptually, with most of its knowledge externalized to explicit memories, the LLM can enjoy a smaller parameter size, training cost, and inference cost, all proportional to the amount of remaining â€œabstract knowledgeâ€. As a preliminary proof of concept, we train from scratch a 2.4 B LLM, which achieves better performance than much larger LLMs as well as RAG models, and maintains higher decoding speed than RAG. The model is named Memory3, since explicit memory is the third form of memory in LLMs after implicit memory (model parameters) and working memory (context key-values). We introduce a memory circuitry theory to support the externalization of knowledge, and present novel techniques including a memory sparsification mechanism that makes storage tractable and a two-stage pretraining scheme that facilitates memory formation. // å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è®­ç»ƒå’Œæ¨ç†æ˜¯ä¸€ä¸ªæˆæœ¬é«˜æ˜‚çš„è¿‡ç¨‹ï¼Œå®ƒå°†çŸ¥è¯†ä»åŸå§‹æ•°æ®ä¼ è¾“åˆ°æœ‰æ„ä¹‰çš„è®¡ç®—ã€‚å—äººç±»å¤§è„‘è®°å¿†å±‚æ¬¡ç»“æ„çš„å¯å‘ï¼Œæˆ‘ä»¬é€šè¿‡ä¸º LLMs é…å¤‡æ˜¾å¼è®°å¿†æ¥é™ä½è¿™ä¸€æˆæœ¬ï¼Œæ˜¾å¼è®°å¿†æ¯”æ¨¡å‹å‚æ•°å’Œæ–‡æœ¬æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ›´ç»æµã€‚ä»æ¦‚å¿µä¸Šè®²ï¼Œå½“å¤§éƒ¨åˆ†çŸ¥è¯†è¢«å¤–éƒ¨åŒ–åˆ°æ˜¾å¼è®°å¿†ä¸­æ—¶ï¼ŒLLM å¯ä»¥äº«å—æ›´å°çš„å‚æ•°è§„æ¨¡ã€è®­ç»ƒæˆæœ¬å’Œæ¨ç†æˆæœ¬ï¼Œæ‰€æœ‰è¿™äº›éƒ½éšç€å‰©ä½™â€œæŠ½è±¡çŸ¥è¯†â€çš„é‡æˆæ¯”ä¾‹å˜åŒ–ã€‚ä½œä¸ºä¸€ä¸ªåˆæ­¥çš„æ¦‚å¿µéªŒè¯ï¼Œæˆ‘ä»¬ä»é›¶å¼€å§‹è®­ç»ƒäº†ä¸€ä¸ª 2.4B çš„ LLMï¼Œå®ƒæ¯”è®¸å¤šæ›´å¤§çš„ LLMs ä»¥åŠ RAG æ¨¡å‹å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ï¼Œå¹¶ä¿æŒäº†æ¯” RAG æ›´é«˜çš„è§£ç é€Ÿåº¦ã€‚è¯¥æ¨¡å‹å‘½åä¸º Memory3 ï¼Œå› ä¸ºæ˜¾å¼è®°å¿†æ˜¯ LLMs ä¸­ç»§éšå¼è®°å¿†ï¼ˆæ¨¡å‹å‚æ•°ï¼‰å’Œå·¥ä½œè®°å¿†ï¼ˆä¸Šä¸‹æ–‡é”®å€¼ï¼‰ä¹‹åçš„ç¬¬ä¸‰ç§è®°å¿†å½¢å¼ã€‚ æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è®°å¿†ç”µè·¯ç†è®ºæ¥æ”¯æŒçŸ¥è¯†çš„ å¤–éƒ¨åŒ–ï¼Œå¹¶æå‡ºäº†åŒ…æ‹¬ä¸€ç§ä½¿å­˜å‚¨æ˜“äºç®¡ç†çš„è®°å¿†ç¨€ç–åŒ–æœºåˆ¶ä»¥åŠä¸€ç§ä¿ƒè¿›è®°å¿†å½¢æˆçš„ä¸¤é˜¶æ®µé¢„è®­ç»ƒæ–¹æ¡ˆç­‰æ–° æŠ€æœ¯ã€‚"
[benchmark/hf-datasets]: https://huggingface.co/datasets/MemTensor/MemOS_eval_result "Evaluation result for MemOS // MemOS çš„è¯„ä¼°ç»“æœ /// - LOCOMO ///: All responses are generated using a short-answer prompt except MIRIX. // é™¤ MIRIX å¤–ï¼Œæ‰€æœ‰å›å¤éƒ½æ˜¯ä½¿ç”¨ç®€ç­”é¢˜æç¤ºç”Ÿæˆçš„ã€‚ /// - LongMemEval /// - PrefEval (0-round, 10-round, 300-round) /// - PersonaMem"

