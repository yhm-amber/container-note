[InstantID.src/gh]: https://github.com/InstantID/InstantID.git "(Apache-2.0) (Languages: Python 100.0%) InstantID : Zero-shot Identity-Preserving Generation in Seconds 🔥 // InstantID: 几秒内零次身份保存生成 🔥"
[InstantID.lfs/huggingface]: https://hugging/ghioface.co/InstantX/InstantID "(Text-to-Image) (Diffusers) (Safetensors) (English) (arxiv: 2401.07519) (license: apache-2.0)"


[InstantID.site/ghio]: https://instantid.github.io/ "Abstract: There has been significant progress in personalized image synthesis with methods such as Textual Inversion, DreamBooth, and LoRA. Yet, their real-world applicability is hindered by high storage demands, lengthy fine-tuning processes, and the need for multiple reference images. Conversely, existing ID embedding-based methods, while requiring only a single forward inference, face challenges: they either necessitate extensive fine-tuning across numerous model parameters, lack compatibility with community pre-trained models, or fail to maintain high face fidelity. Addressing these limitations, we introduce InstantID, a powerful diffusion model-based solution. Our plug-and-play module adeptly handles image personalization in various styles using just a single facial image, while ensuring high fidelity. To achieve this, we design a novel IdentityNet by imposing strong semantic and weak spatial conditions, integrating facial and landmark images with textual prompts to steer the image generation. InstantID demonstrates exceptional performance and efficiency, proving highly beneficial in real-world applications where identity preservation is paramount. Moreover, our work seamlessly integrates with popular pre-trained text-to-image diffusion models like SD1.5 and SDXL, serving as an adaptable plugin. Our codes and pre-trained checkpoints will be available at this URL. // 摘要: 通过 Textual Inversion、DreamBooth 和 LoRA 等方法，个性化图像合成取得了重大进展。然而，它们在现实世界中的适用性受到高存储需求、冗长的微调过程以及对多个参考图像的需求的阻碍。相反，现有的基于 ID 嵌入的方法虽然只需要一次前向推理，但也面临着挑战：它们要么需要对众多模型参数进行广泛的微调，要么缺乏与社区预训练模型的兼容性，要么无法保持高面部保真度。为了解决这些限制，我们引入了 InstantID ，这是一种强大的基于扩散模型的解决方案。 我们的即插即用模块仅使用单个面部图像就能熟练地处理各种风格的图像个性化，同时确保高保真度。为了实现这一目标，我们设计了一个新颖的 IdentityNet，通过强加语义和弱空间条件，将面部和地标图像与文本提示相结合来引导图像生成。 InstantID 展示了卓越的性能和效率，在身份保存至关重要的现实应用中非常有用。此外，我们的工作与流行的预训练文本到图像扩散模型（如 SD1.5 和 SDXL）无缝集成，作为一个适应性强的插件。 我们的代码和预先训练的检查点将在以下位置提供：（就是 Github 网址）。"
[InstantID.paper/arxiv]: https://arxiv.org/abs/2401.07519
[InstantID.demo/huggingface]: https://huggingface.co/spaces/InstantX/InstantID


[InstantStyle.src/gh]: https://github.com/InstantStyle/InstantStyle.git "(Apache-2.0) (Languages: Jupyter Notebook 93.5%, Python 6.5%) InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation 🔥 // 即时风格: 在文本到图像生成中保持风格的免费午餐 🔥 /// \"We release our recent work InstantStyle for style transfer, compatible with InstantID!\" // “我们最近发布了 InstantStyle ，用于风格转换，兼容InstantID ！”"

[InstantStyle.site/ghio]: https://instantstyle.github.io/ " InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation /// Abstract /// Tuning-free diffusion-based models have demonstrated sig- nificant potential in the realm of image personalization and customiza- tion. However, despite this notable progress, current models continue to grapple with several complex challenges in producing style-consistent image generation. Firstly, the concept of ’style’ is inherently underde- termined, encompassing a multitude of elements such as color, material, atmosphere, design, and structure, among others. Secondly, inversion- based methods are prone to style degradation, often resulting in the loss of fine-grained details. Lastly, adapter-based approaches frequently re- quire meticulous weight tuning for each reference image to achieve a bal- ance between style intensity and text controllability. In this paper, we commence by examining several compelling yet frequently overlooked observations. We then proceed to introduce InstantStyle, a framework designed to address these issues through the implementation of two key strategies: 1) A straightforward mechanism that decouples style and con- tent from reference images within the feature space, predicated on the assumption that features within the same space can be either added to or subtracted from one another. 2) The injection of reference image features exclusively into style-specific blocks, thereby preventing style leaks and eschewing the need for cumbersome weight tuning, which often charac- terizes more parameter-heavy designs.Our work demonstrates superior visual stylization outcomes, striking an optimal balance between the in- tensity of style and the controllability of textual elements. // InstantStyle: 文本到图像生成中风格保留的免费午餐 /// 摘要 /// 基于免调整扩散的模型已在图像个性化和定制领域展现出巨大的潜力。然而，尽管取得了显着的进步，当前的模型在生成风格一致的图像方面仍然面临着一些复杂的挑战。首先，“风格”的概念本质上是不确定的，它包含色彩、材质、氛围、设计和结构等多种元素。其次，基于反演的方法很容易出现风格退化，通常会导致细粒度细节的丢失。最后，基于适配器的方法经常需要对每个参考图像进行细致的权重调整，以实现风格强度和文本可控性之间的平衡。 在本文中，我们首先研究几个令人信服但经常被忽视的观察结果。然后，我们继续介绍 InstantStyle，这是一个旨在通过实施两个关键策略来解决这些问题的框架：1）一种简单的机制，可以将样式和内容与特征空间内的参考图像解耦，基于以下假设：特征空间内的特征相同的空间可以相互添加或减去。 2）将参考图像特征专门注入特定于风格的块中，从而防止风格泄漏并避免繁琐的权重调整，这通常是参数较多的设计的特征。我们的工作展示了卓越的视觉风格化结果，达到了最佳效果风格的强度和文本元素的可控性之间的平衡。"

[InstantStyle.paper/arxiv]: https://arxiv.org/abs/2404.02733

[InstantStyle.demo/huggingface]: https://huggingface.co/spaces/InstantX/InstantStyle
[InstantStyle.demo/modelscope]: https://modelscope.cn/studios/instantx/InstantStyle/summary








