[InstantID.src/gh]: https://github.com/InstantID/InstantID.git "(Apache-2.0) (Languages: Python 100.0%) InstantID : Zero-shot Identity-Preserving Generation in Seconds 🔥 // InstantID: 几秒内零次身份保存生成 🔥"
[InstantID.lfs/huggingface]: https://hugging/ghioface.co/InstantX/InstantID "(Text-to-Image) (Diffusers) (Safetensors) (English) (arxiv: 2401.07519) (license: apache-2.0)"


[InstantID.site/ghio]: https://instantid.github.io/
[InstantID.paper/arxiv]: https://arxiv.org/abs/2401.07519
[InstantID.demo/huggingface]: https://huggingface.co/spaces/InstantX/InstantID


[InstantStyle.src/gh]: https://github.com/InstantStyle/InstantStyle.git "(Apache-2.0) (Languages: Jupyter Notebook 93.5%, Python 6.5%) InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation 🔥 // InstantStyle: 在文本到图像生成中保持风格的免费午餐 🔥"

[InstantStyle.site/ghio]: https://instantstyle.github.io/ " InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation /// Abstract /// Tuning-free diffusion-based models have demonstrated sig- nificant potential in the realm of image personalization and customiza- tion. However, despite this notable progress, current models continue to grapple with several complex challenges in producing style-consistent image generation. Firstly, the concept of ’style’ is inherently underde- termined, encompassing a multitude of elements such as color, material, atmosphere, design, and structure, among others. Secondly, inversion- based methods are prone to style degradation, often resulting in the loss of fine-grained details. Lastly, adapter-based approaches frequently re- quire meticulous weight tuning for each reference image to achieve a bal- ance between style intensity and text controllability. In this paper, we commence by examining several compelling yet frequently overlooked observations. We then proceed to introduce InstantStyle, a framework designed to address these issues through the implementation of two key strategies: 1) A straightforward mechanism that decouples style and con- tent from reference images within the feature space, predicated on the assumption that features within the same space can be either added to or subtracted from one another. 2) The injection of reference image features exclusively into style-specific blocks, thereby preventing style leaks and eschewing the need for cumbersome weight tuning, which often charac- terizes more parameter-heavy designs.Our work demonstrates superior visual stylization outcomes, striking an optimal balance between the in- tensity of style and the controllability of textual elements. // InstantStyle: 文本到图像生成中风格保留的免费午餐 /// 摘要 /// 基于免调整扩散的模型已在图像个性化和定制领域展现出巨大的潜力。然而，尽管取得了显着的进步，当前的模型在生成风格一致的图像方面仍然面临着一些复杂的挑战。首先，“风格”的概念本质上是不确定的，它包含色彩、材质、氛围、设计和结构等多种元素。其次，基于反演的方法很容易出现风格退化，通常会导致细粒度细节的丢失。最后，基于适配器的方法经常需要对每个参考图像进行细致的权重调整，以实现风格强度和文本可控性之间的平衡。 在本文中，我们首先研究几个令人信服但经常被忽视的观察结果。然后，我们继续介绍 InstantStyle，这是一个旨在通过实施两个关键策略来解决这些问题的框架：1）一种简单的机制，可以将样式和内容与特征空间内的参考图像解耦，基于以下假设：特征空间内的特征相同的空间可以相互添加或减去。 2）将参考图像特征专门注入特定于风格的块中，从而防止风格泄漏并避免繁琐的权重调整，这通常是参数较多的设计的特征。我们的工作展示了卓越的视觉风格化结果，达到了最佳效果风格的强度和文本元素的可控性之间的平衡。"

[InstantStyle.paper/arxiv]: https://arxiv.org/abs/2404.02733

[InstantStyle.demo/huggingface]: https://huggingface.co/spaces/InstantX/InstantStyle
[InstantStyle.demo/modelscope]: https://modelscope.cn/studios/instantx/InstantStyle/summary








