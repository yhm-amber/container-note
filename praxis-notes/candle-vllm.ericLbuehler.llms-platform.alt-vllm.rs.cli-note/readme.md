[src/gh]: https://github.com/EricLBuehler/candle-vllm.git "(MIT) (Languages: Rust 72.9%, Cuda 19.4%, Metal 6.7%, Other 1.0%) Efficent platform for inference and serving local LLMs including an OpenAI compatible API server. // 高效的推理平台，并为本地 LLM 提供服务，包括一个兼容 OpenAI 的 API 服务器。 /// Features // 特征 /// - OpenAI compatible API server provided for serving LLMs. // 提供与 OpenAI 兼容的 API 服务器，用于服务 LLM。 /// - Highly extensible trait-based system to allow rapid implementation of new module pipelines, // 高度可扩展的基于特性的系统，可实现新模块流水线的快速部署。 /// - Streaming support in generation. // 支持流媒体生成。 /// - Efficient management of key-value cache with PagedAttention. // 利用 PagedAttention 高效管理键值缓存。 /// - Continuous batching (batched decoding for incoming requests over time). // 连续批处理（对一段时间内传入的请求进行批量解码）。 /// - In-situ quantization (and In-situ marlin format conversion) // In-situ 量化（以及 In-situ 马林格式转换） /// - GPTQ/Marlin format quantization (4-bit) // GPTQ/Marlin 格式量化（4 位） /// - Support Mac/Metal devices // 支持 Mac/Metal 设备 /// - Support Multi-GPU inference (both multi-process and multi-threaded mode) // 支持 Multi-GPU 推理（包括 multi-process 和 multi-threaded 模式） /// - Support Multi-node inference with MPI runner // 支持使用 MPI 运行器进行 Multi-node 推理 /// - Support Chunked Prefilling (default chunk size 8K) // 支持分块预填充（默认块大小为 8K） /// - Support CUDA Graph // 支持 CUDA 图"
[knows_by]: https://github.com/huggingface/candle.git "... /// - candle-vllm: Efficient platform for inference and serving local LLMs including an OpenAI compatible API server. // candle-vllm ：高效的推理平台，并提供本地 LLM 服务，包括一个兼容 OpenAI 的 API 服务器。"

