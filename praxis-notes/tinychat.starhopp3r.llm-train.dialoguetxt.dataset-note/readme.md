[src/hf:ds]: https://huggingface.co/datasets/starhopp3r/TinyChat "[Size of downloaded dataset files: 745 MB] [Size of the auto-converted Parquet files: 384 MB] [Number of rows: 1,000,000] (License: CC-BY-NC-4.0) (Languages: English) (ArXiv: 2305.07759)"
[paper/hf:pp]: https://huggingface.co/papers/2305.07759 "TinyStories, a synthetic dataset of simplified stories, enables small or simple language models to produce fluent and coherent text with reasoning capabilities using an evaluation framework based on human-like grading. // TinyStories 是一个由简化故事组成的合成数据集，它使小型或简单的语言模型能够借助基于类似人类评分的评价框架，产生具有推理能力的流畅且连贯的文本。 /// AI-generated summary // AI 生成的摘要"
[paper/arxiv]: https://huggingface.co/papers/2305.07759 "TinyStories: How Small Can Language Models Be and Still Speak Coherent English? // TinyStories：语言模型可以有多小，仍然能够说出连贯的英语？ /// Ronen Eldan, Yuanzhi Li /// Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention). // 语言模型（LMs）是自然语言处理领域的强大工具，但当它们规模较小时，往往难以生成连贯流畅的文本。参数量约为 125M 的模型，如 GPT-Neo（小型）或 GPT-2（小型），即使在经过大量训练后，也很少能生成超过几个字的连贯且一致的英文文本。这引发了这样一个问题：生成连贯英文文本的能力是否只有在更大规模（数千万参数或更多）和更复杂架构（包含多层全局注意力机制）的模型中才会出现。 /// In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities. // 在这项工作中，我们介绍了 TinyStories，这是一个由 GPT-3.5 和 GPT-4 生成的合成数据集，其中只包含典型 3 至 4 岁儿童通常理解的词汇。我们展示了 TinyStories 可以用于训练和评估远小于当前最先进模型（总参数量低于 1000 万）或具有更简单架构（仅包含一个 Transformer 模块）的语言模型，这些模型仍能生成包含数个段落的流畅且一致的、多样化的故事，且语法几乎完美，并展现出推理能力。 /// We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses GPT-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often requires the model's output to be very structures, and moreover provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and consistency. // 我们也介绍了一种新的语言模型评估范式：我们建议一个使用 GPT-4 来评估这些模型生成内容的框架，仿佛这些内容是由学生撰写的并由（人类）教师评分的故事。这种新的范式克服了标准基准的缺陷，这些基准通常要求模型的输出非常结构化，并且此外为模型提供多维度的评分，为不同的能力（如语法、创造性和一致性）提供评分。 /// We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in LMs. // 我们希望 TinyStories 能够促进语言模型（LMs）的开发、分析和研究，特别是对于低资源或专业领域，并为语言模型中语言能力的出现提供启示。"
[knows_by]: https://github.com/sammyuri/craftgpt.git "A small language model built to run in Minecraft, trained on the TinyChat dataset (. hf-ds: starhopp3r/TinyChat.git). // 一个为在 Minecraft 中运行而构建的小型语言模型，使用 TinyChat 数据集进行训练。"
