[src/gh]: https://github.com/huggingface/datasets.git "(Apache-2.0) (Python 100.0%) 🤗 The largest hub of ready-to-use datasets for AI models with fast, easy-to-use and efficient data manipulation tools // 🤗 面向 AI 模型的最大数据集中心，提供快速、易用且高效的数据操作工具 /// 🤗 Datasets is a lightweight library providing two main features: // 🤗 Datasets 是一个轻量级库，提供两个主要功能： /// - one-line dataloaders for many public datasets: one-liners to download and pre-process any of the number of datasets major public datasets (image datasets, audio datasets, text datasets in 467 languages and dialects, etc.) provided on the HuggingFace Datasets Hub. With a simple command like squad_dataset = load_dataset('rajpurkar/squad'), get any of these datasets ready to use in a dataloader for training/evaluating a ML model (Numpy/Pandas/PyTorch/TensorFlow/JAX), // 为许多公共数据集提供单行数据加载器：使用 number of datasets 代码行即可下载和预处理 HuggingFace Datasets Hub 上提供的任何主要公共数据集（图像数据集、音频数据集、467 种语言和方言的文本数据集等）。通过简单的命令 squad_dataset = load_dataset('rajpurkar/squad') ，即可将这些数据集准备好用于训练/评估机器学习模型（Numpy/Pandas/PyTorch/TensorFlow/JAX）, /// - efficient data pre-processing: simple, fast and reproducible data pre-processing for the public datasets as well as your own local datasets in CSV, JSON, text, PNG, JPEG, WAV, MP3, Parquet, HDF5, etc. With simple commands like processed_dataset = dataset.map(process_example), efficiently prepare the dataset for inspection and ML model evaluation and training. // 高效的数据预处理：为公共数据集以及您本地的 CSV、JSON、文本、PNG、JPEG、WAV、MP3、Parquet、HDF5 等格式的数据集提供简单、快速且可重复的数据预处理。使用简单的命令 processed_dataset = dataset.map(process_example) ，高效地准备数据集用于检查和机器学习模型的评估与训练。 /// 🤗 Datasets is designed to let the community easily add and share new datasets. // 🤗 Datasets 旨在让社区能够轻松地添加和共享新数据集。 /// 🤗 Datasets has many additional interesting features: // 🤗 Datasets 具有许多其他有趣的功能： /// - Thrive on large datasets: 🤗 Datasets naturally frees the user from RAM memory limitation, all datasets are memory-mapped using an efficient zero-serialization cost backend (Apache Arrow). // 在大型数据集上蓬勃发展：🤗 Datasets 自然地让用户摆脱 RAM 内存限制，所有数据集都使用高效的零序列化成本后端（Apache Arrow）进行内存映射。 /// - Smart caching: never wait for your data to process several times. // 智能缓存：无需多次等待您的数据处理。 /// - Lightweight and fast with a transparent and pythonic API (multi-processing/caching/memory-mapping). // 轻量级且快速，具有透明且符合 Python 风格的 API（多进程/缓存/内存映射）。 /// - Built-in interoperability with NumPy, PyTorch, TensorFlow 2, JAX, Pandas, Polars and more. // 与 NumPy、PyTorch、TensorFlow 2、JAX、Pandas、Polars 等内置互操作性。 /// - Native support for audio, image and video data. // 原生支持音频、图像和视频数据。 /// - Enable streaming mode to save disk space and start iterating over the dataset immediately. // 启用流式模式以节省磁盘空间并立即开始迭代数据集。 /// 🤗 Datasets originated from a fork of the awesome [TensorFlow Datasets] (. gh: tensorflow/datasets.git) and the HuggingFace team want to deeply thank the TensorFlow Datasets team for building this amazing library. // 🤗 Datasets 项目源自于 awesome TensorFlow Datasets 的一个分支，HuggingFace 团队想深度感谢 TensorFlow Datasets 团队构建了这个卓越的库。"
[docs/hf]: https://huggingface.co/docs/datasets/ "🤗 Datasets is a library for easily accessing and sharing AI datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks. // 🤗 Datasets 是一个用于轻松访问和共享音频、计算机视觉和自然语言处理（NLP）任务的 AI 数据集库。 /// Load a dataset in a single line of code, and use our powerful data processing and streaming methods to quickly get your dataset ready for training in a deep learning model. Backed by the Apache Arrow format, process large datasets with zero-copy reads without any memory constraints for optimal speed and efficiency. We also feature a deep integration with the [Hugging Face Hub](https://huggingface.co/datasets ''), allowing you to easily load and share a dataset with the wider machine learning community. // 在单行代码中加载数据集，并使用我们强大的数据处理和流式传输方法，快速将数据集准备好用于深度学习模型的训练。基于 Apache Arrow 格式，实现零拷贝读取大型数据集，无内存限制，以实现最佳速度和效率。我们还与 Hugging Face Hub 深度集成，让您能够轻松加载和分享数据集给更广泛的机器学习社区。"
[help-hfhub/hf]: https://huggingface.co/docs/hub/datasets-usage "load the separate splits if the dataset has train/validation/test splits: -(: {{train|validation|test}}_dataset = datasets.load_dataset('{username}/{dataset}', split='{{train|validation|test}}')) /// hf protocol format (for `datasets` type): (~ hf://datasets/{username}/{dataset}/{path_to_file}) /// auto-converted parquet files that Hugging Face provides: (~ hf://datasets/{username}/{dataset}@~parquet/{path_to_file})"
[hub.wui/hf]: https://huggingface.co/datasets ''
[lib.pip/pypi]: https://pypi.org/project/datasets/ "(: pip install -U -- datasets)"
[knows_by]: https://huggingface.co/datasets/starhopp3r/TinyChat "Use this dataset: (: from datasets import load_dataset ;: ds = load_dataset('starhopp3r/TinyChat'))"
