[src/gh]: https://github.com/huggingface/datasets.git "(Apache-2.0) (Python 100.0%) ğŸ¤— The largest hub of ready-to-use datasets for AI models with fast, easy-to-use and efficient data manipulation tools // ğŸ¤— é¢å‘ AI æ¨¡å‹çš„æœ€å¤§æ•°æ®é›†ä¸­å¿ƒï¼Œæä¾›å¿«é€Ÿã€æ˜“ç”¨ä¸”é«˜æ•ˆçš„æ•°æ®æ“ä½œå·¥å…· /// ğŸ¤— Datasets is a lightweight library providing two main features: // ğŸ¤— Datasets æ˜¯ä¸€ä¸ªè½»é‡çº§åº“ï¼Œæä¾›ä¸¤ä¸ªä¸»è¦åŠŸèƒ½ï¼š /// - one-line dataloaders for many public datasets: one-liners to download and pre-process any of the number of datasets major public datasets (image datasets, audio datasets, text datasets in 467 languages and dialects, etc.) provided on the HuggingFace Datasets Hub. With a simple command like squad_dataset = load_dataset('rajpurkar/squad'), get any of these datasets ready to use in a dataloader for training/evaluating a ML model (Numpy/Pandas/PyTorch/TensorFlow/JAX), // ä¸ºè®¸å¤šå…¬å…±æ•°æ®é›†æä¾›å•è¡Œæ•°æ®åŠ è½½å™¨ï¼šä½¿ç”¨ number of datasets ä»£ç è¡Œå³å¯ä¸‹è½½å’Œé¢„å¤„ç† HuggingFace Datasets Hub ä¸Šæä¾›çš„ä»»ä½•ä¸»è¦å…¬å…±æ•°æ®é›†ï¼ˆå›¾åƒæ•°æ®é›†ã€éŸ³é¢‘æ•°æ®é›†ã€467 ç§è¯­è¨€å’Œæ–¹è¨€çš„æ–‡æœ¬æ•°æ®é›†ç­‰ï¼‰ã€‚é€šè¿‡ç®€å•çš„å‘½ä»¤ squad_dataset = load_dataset('rajpurkar/squad') ï¼Œå³å¯å°†è¿™äº›æ•°æ®é›†å‡†å¤‡å¥½ç”¨äºè®­ç»ƒ/è¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆNumpy/Pandas/PyTorch/TensorFlow/JAXï¼‰, /// - efficient data pre-processing: simple, fast and reproducible data pre-processing for the public datasets as well as your own local datasets in CSV, JSON, text, PNG, JPEG, WAV, MP3, Parquet, HDF5, etc. With simple commands like processed_dataset = dataset.map(process_example), efficiently prepare the dataset for inspection and ML model evaluation and training. // é«˜æ•ˆçš„æ•°æ®é¢„å¤„ç†ï¼šä¸ºå…¬å…±æ•°æ®é›†ä»¥åŠæ‚¨æœ¬åœ°çš„ CSVã€JSONã€æ–‡æœ¬ã€PNGã€JPEGã€WAVã€MP3ã€Parquetã€HDF5 ç­‰æ ¼å¼çš„æ•°æ®é›†æä¾›ç®€å•ã€å¿«é€Ÿä¸”å¯é‡å¤çš„æ•°æ®é¢„å¤„ç†ã€‚ä½¿ç”¨ç®€å•çš„å‘½ä»¤ processed_dataset = dataset.map(process_example) ï¼Œé«˜æ•ˆåœ°å‡†å¤‡æ•°æ®é›†ç”¨äºæ£€æŸ¥å’Œæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¯„ä¼°ä¸è®­ç»ƒã€‚ /// ğŸ¤— Datasets is designed to let the community easily add and share new datasets. // ğŸ¤— Datasets æ—¨åœ¨è®©ç¤¾åŒºèƒ½å¤Ÿè½»æ¾åœ°æ·»åŠ å’Œå…±äº«æ–°æ•°æ®é›†ã€‚ /// ğŸ¤— Datasets has many additional interesting features: // ğŸ¤— Datasets å…·æœ‰è®¸å¤šå…¶ä»–æœ‰è¶£çš„åŠŸèƒ½ï¼š /// - Thrive on large datasets: ğŸ¤— Datasets naturally frees the user from RAM memory limitation, all datasets are memory-mapped using an efficient zero-serialization cost backend (Apache Arrow). // åœ¨å¤§å‹æ•°æ®é›†ä¸Šè“¬å‹ƒå‘å±•ï¼šğŸ¤— Datasets è‡ªç„¶åœ°è®©ç”¨æˆ·æ‘†è„± RAM å†…å­˜é™åˆ¶ï¼Œæ‰€æœ‰æ•°æ®é›†éƒ½ä½¿ç”¨é«˜æ•ˆçš„é›¶åºåˆ—åŒ–æˆæœ¬åç«¯ï¼ˆApache Arrowï¼‰è¿›è¡Œå†…å­˜æ˜ å°„ã€‚ /// - Smart caching: never wait for your data to process several times. // æ™ºèƒ½ç¼“å­˜ï¼šæ— éœ€å¤šæ¬¡ç­‰å¾…æ‚¨çš„æ•°æ®å¤„ç†ã€‚ /// - Lightweight and fast with a transparent and pythonic API (multi-processing/caching/memory-mapping). // è½»é‡çº§ä¸”å¿«é€Ÿï¼Œå…·æœ‰é€æ˜ä¸”ç¬¦åˆ Python é£æ ¼çš„ APIï¼ˆå¤šè¿›ç¨‹/ç¼“å­˜/å†…å­˜æ˜ å°„ï¼‰ã€‚ /// - Built-in interoperability with NumPy, PyTorch, TensorFlow 2, JAX, Pandas, Polars and more. // ä¸ NumPyã€PyTorchã€TensorFlow 2ã€JAXã€Pandasã€Polars ç­‰å†…ç½®äº’æ“ä½œæ€§ã€‚ /// - Native support for audio, image and video data. // åŸç”Ÿæ”¯æŒéŸ³é¢‘ã€å›¾åƒå’Œè§†é¢‘æ•°æ®ã€‚ /// - Enable streaming mode to save disk space and start iterating over the dataset immediately. // å¯ç”¨æµå¼æ¨¡å¼ä»¥èŠ‚çœç£ç›˜ç©ºé—´å¹¶ç«‹å³å¼€å§‹è¿­ä»£æ•°æ®é›†ã€‚ /// ğŸ¤— Datasets originated from a fork of the awesome [TensorFlow Datasets] (. gh: tensorflow/datasets.git) and the HuggingFace team want to deeply thank the TensorFlow Datasets team for building this amazing library. // ğŸ¤— Datasets é¡¹ç›®æºè‡ªäº awesome TensorFlow Datasets çš„ä¸€ä¸ªåˆ†æ”¯ï¼ŒHuggingFace å›¢é˜Ÿæƒ³æ·±åº¦æ„Ÿè°¢ TensorFlow Datasets å›¢é˜Ÿæ„å»ºäº†è¿™ä¸ªå“è¶Šçš„åº“ã€‚"
[docs/hf]: https://huggingface.co/docs/datasets/ "ğŸ¤— Datasets is a library for easily accessing and sharing AI datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks. // ğŸ¤— Datasets æ˜¯ä¸€ä¸ªç”¨äºè½»æ¾è®¿é—®å’Œå…±äº«éŸ³é¢‘ã€è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡çš„ AI æ•°æ®é›†åº“ã€‚ /// Load a dataset in a single line of code, and use our powerful data processing and streaming methods to quickly get your dataset ready for training in a deep learning model. Backed by the Apache Arrow format, process large datasets with zero-copy reads without any memory constraints for optimal speed and efficiency. We also feature a deep integration with the [Hugging Face Hub](https://huggingface.co/datasets ''), allowing you to easily load and share a dataset with the wider machine learning community. // åœ¨å•è¡Œä»£ç ä¸­åŠ è½½æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨æˆ‘ä»¬å¼ºå¤§çš„æ•°æ®å¤„ç†å’Œæµå¼ä¼ è¾“æ–¹æ³•ï¼Œå¿«é€Ÿå°†æ•°æ®é›†å‡†å¤‡å¥½ç”¨äºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒã€‚åŸºäº Apache Arrow æ ¼å¼ï¼Œå®ç°é›¶æ‹·è´è¯»å–å¤§å‹æ•°æ®é›†ï¼Œæ— å†…å­˜é™åˆ¶ï¼Œä»¥å®ç°æœ€ä½³é€Ÿåº¦å’Œæ•ˆç‡ã€‚æˆ‘ä»¬è¿˜ä¸ Hugging Face Hub æ·±åº¦é›†æˆï¼Œè®©æ‚¨èƒ½å¤Ÿè½»æ¾åŠ è½½å’Œåˆ†äº«æ•°æ®é›†ç»™æ›´å¹¿æ³›çš„æœºå™¨å­¦ä¹ ç¤¾åŒºã€‚"
[help-hfhub/hf]: https://huggingface.co/docs/hub/datasets-usage "load the separate splits if the dataset has train/validation/test splits: -(: {{train|validation|test}}_dataset = datasets.load_dataset('{username}/{dataset}', split='{{train|validation|test}}')) /// hf protocol format (for `datasets` type): (~ hf://datasets/{username}/{dataset}/{path_to_file}) /// auto-converted parquet files that Hugging Face provides: (~ hf://datasets/{username}/{dataset}@~parquet/{path_to_file})"
[hub.wui/hf]: https://huggingface.co/datasets ''
[lib.pip/pypi]: https://pypi.org/project/datasets/ "(: pip install -U -- datasets)"
[knows_by]: https://huggingface.co/datasets/starhopp3r/TinyChat "Use this dataset: (: from datasets import load_dataset ;: ds = load_dataset('starhopp3r/TinyChat'))"
