[src/gh]: https://github.com/locuslab/llm-idiosyncrasies.git "(MIT) (Languages: Python 100.0%) Code release for “Idiosyncrasies in Large Language Models” // “大型语言模型中的特质”代码发布 /// (~ @article{sun2025idiosyncrasies, title = {Idiosyncrasies in Large Language Models}, author = {Sun, Mingjie and Yin, Yida and Xu, Zhiqiu and Kolter, J. Zico and Liu, Zhuang}, year = {2025}, journal  = {arXiv preprint arXiv:2502.12150} })"
[site/ghio]: https://eric-mingjie.github.io/llm-idiosyncrasies "We unveil and study idiosyncrasies in Large Language Models (LLMs) – unique patterns in their outputs that can be used to distinguish among them. We consider a simple classification task: given a particular text output, the objective is to predict the source LLM that generates the text. We evaluate this synthetic task across various groups of LLMs, and find that simply fine-tuning existing text embedding models on LLM-generated texts yields excellent classification accuracy. Notably, we achieve 97.1% accuracy on held-out validation data in the fiveway classification problem involving ChatGPT, Claude, Grok, Gemini, and DeepSeek. Our further investigation reveals that these idiosyncrasies are rooted in word-level distributions. These patterns persist even when the texts are rewritten, translated, or summarized by an external LLM, suggesting that they are also encoded in the semantic content. Additionally, we leverage LLM judges to generate detailed, open-ended descriptions of each model’s idiosyncrasies. Finally, we discuss the broader implications of our findings, particularly for training on synthetic data and inferring model similarity. // 我们揭示并研究了大型语言模型（LLM）的特质——它们输出中的独特模式，这些模式可用于区分不同的 LLM。我们考虑一个简单的分类任务：给定一个特定的文本输出，目标是预测生成该文本的源 LLM。我们针对不同的 LLM 组评估了这项合成任务，发现只需在 LLM 生成的文本上微调现有的文本嵌入模型即可获得极佳的分类准确率。值得注意的是，在包含 ChatGPT、Claude、Grok、Gemini 和 DeepSeek 的五路分类问题中，我们在预留的验证数据集上实现了 97.1%的准确率。我们的进一步研究表明，这些特质源于词级分布。即使文本被外部 LLM 重写、翻译或概括，这些模式仍然存在，这表明它们也编码在语义内容中。此外，我们利用 LLM 评测员生成了每个模型特质的详细、开放式描述。最后，我们讨论了研究结果的更广泛意义，特别是对于合成数据训练和推断模型相似性的意义。"
[paper/arxiv]: https://arxiv.org/abs/2502.12150 "[Submitted on 17 Feb 2025 (v1), last revised 16 Jun 2025 (this version, v2)] { Computation and Language (cs.CL) } (doi: 10.48550/arXiv.2502.12150) Idiosyncrasies in Large Language Models // 大型语言模型的特殊性 /// In this work, we unveil and study idiosyncrasies in Large Language Models (LLMs) -- unique patterns in their outputs that can be used to distinguish the models. To do so, we consider a simple classification task: given a particular text output, the objective is to predict the source LLM that generates the text. We evaluate this synthetic task across various groups of LLMs and find that simply fine-tuning text embedding models on LLM-generated texts yields excellent classification accuracy. Notably, we achieve 97.1% accuracy on held-out validation data in the five-way classification problem involving ChatGPT, Claude, Grok, Gemini, and DeepSeek. Our further investigation reveals that these idiosyncrasies are rooted in word-level distributions. These patterns persist even when the texts are rewritten, translated, or summarized by an external LLM, suggesting that they are also encoded in the semantic content. Additionally, we leverage LLM as judges to generate detailed, open-ended descriptions of each model's idiosyncrasies. Finally, we discuss the broader implications of our findings, including training on synthetic data, inferring model similarity, and robust evaluation of LLMs. // 本文揭示并研究了大型语言模型（LLM）的特质——即其输出中可用于区分不同模型的独特模式。为此，我们考虑一个简单的分类任务：给定一段特定的文本输出，目标是预测生成该文本的源 LLM。我们针对不同的 LLM 组评估了这项合成任务，发现只需在 LLM 生成的文本上微调文本嵌入模型即可获得极佳的分类准确率。值得注意的是，在包含 ChatGPT、Claude、Grok、Gemini 和 DeepSeek 的五分类问题中，我们在预留的验证数据集上实现了 97.1%的准确率。进一步的研究表明，这些特质源于词级分布。即使文本被外部 LLM 重写、翻译或概括，这些模式仍然存在，表明它们也编码在语义内容中。此外，我们还利用 LLM 作为评判标准，生成了每个模型特质的详细、开放式描述。最后，我们讨论了研究结果的更广泛意义，包括使用合成数据进行训练、推断模型相似性以及对 LLM 进行稳健评估。"
[knowsby]: https://github.com/sunblaze-ucb/llm-api-audit.git "LLM API Audit // LLM API 审计 /// This project provides different methods for auditing Large Language Models (LLMs) to verify service integrity. // 该项目提供了不同的方法来审核大型语言模型（LLM），以验证服务的完整性。 /// ## Methods // 方法 /// ### Classifier // 分类器 /// Adapted from LLM Idiosyncrasies with added model support. // 改编自 LLM Idiosyncrasies ，并增加了模型支持。 /// 1. Generate responses from all models mentioned in paper // 生成论文中提到的所有模型的响应 /// 2. Train binary classifiers between original and quantized models. This example uses LLM2Vec, but other embedding models can be easily added // 训练二元分类器，区分原始模型和量化模型。本例使用 LLM2Vec，但也可以轻松添加其他嵌入模型/// 3. Analyze classification results in the ./classification. // 分析 ./classification 中的分类结果。 /// ### Identity Prompting // 身份提示 /// 1. Generate identity responses for multiple models // 为多个模型生成身份响应 /// 2. Analyze identity occurences using the example shown in the notebook // 使用笔记本中的示例分析身份出现情况"
