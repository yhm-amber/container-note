[home/hf]: https://huggingface.co/zai-org "Organization Card // ç»„ç»‡å¡ /// Zhipu AI (Z.ai) /// We build the ChatGLM (. arxiv: 2406.12793) family of LLMs, develop LLMs as Agents, and release related LLM training & inference techniques: // æˆ‘ä»¬æ„å»ºäº† ChatGLM ç³»åˆ— LLMï¼Œå°† LLM å¼€å‘ä¸ºæ™ºèƒ½ä½“ ï¼Œå¹¶å‘å¸ƒäº†ç›¸å…³çš„ LLM è®­ç»ƒå’Œæ¨ç†æŠ€æœ¯ï¼š /// - [GLM-4.6 & GLM-4.5] (. gh: zai-org/GLM-4.5.git), [GLM-4.5V & GLM-4.1V] (. gh: zai-org/GLM-V.git), [GLM-4] (. gh: zai-org/GLM-4.git), [CodeGeeX] (. gh: gh: zai-org/CodeGeeX4.git), [CogVLM (VisualGLM)] (. gh: zai-org/CogVLM2.git), [WebGLM] (. gh: zai-org/WebGLM.git), [GLM-130B] (. gh: zai-org/GLM-130B.git), [CogView] (. gh: zai-org/CogView.git), [CogVideo && CogVideoX] (. gh: zai-org/CogVideo.git). /// - [CogAgent] (. gh: zai-org/CogVLM.git), [AutoWebGLM] (. gh: zai-org/AutoWebGLM.git), [AgentTuning] (. gh: zai-org/AgentTuning.git), [APAR] (. arxiv: 2401.06761). /// We also work on LLM evaluations: // æˆ‘ä»¬ä¹Ÿä»äº‹ LLM è¯„ä¼°å·¥ä½œï¼š /// - LVBench (. gh: zai-org/LVBench.git) /// - MotionBench (. gh: zai-org/MotionBench.git) /// - ComplexFuncBench (. gh: zai-org/ComplexFuncBench.git) /// - AgentBench (. gh: THUDM/AgentBench.git) /// - AlignBench (. gh: THUDM/AlignBench.git) /// - LongBench (. gh: THUDM/LongBench.git) /// - NaturalCodeBench (. gh: THUDM/NaturalCodeBench.git) /// We started with social networks and graphs, and always love them: // æˆ‘ä»¬æœ€åˆæ¥è§¦çš„æ˜¯ç¤¾äº¤ç½‘ç»œå’Œå›¾è°± ï¼Œè€Œä¸”ä¸€ç›´å¾ˆå–œæ¬¢å®ƒä»¬ï¼š /// - GraphMAE (. gh: THUDM/GraphMAE.git) /// - GPT-GNN (. gh: acbull/GPT-GNN.git) /// - GCC (. gh: THUDM/CogDL.git) /// - SelfKG (. gh: THUDM/SelfKG.git) /// - CogDL (. gh: THUDM/CogDL.git) /// - AMiner (. web: aminer.cn)"
[site-sdk.python.src/gh]: https://github.com/zai-org/z-ai-sdk-python.git "(MIT) (Languages: Python 99.5%, Makefile 0.5%) The official Python SDK for Z.ai's large model open interface, making it easier for developers to call Z.ai's open APIs. // Z.ai å¤§å‹æ¨¡å‹å¼€æ”¾æ¥å£çš„å®˜æ–¹ Python SDKï¼Œä½¿å¼€å‘è€…æ›´å®¹æ˜“è°ƒç”¨ Z.ai çš„å¼€æ”¾ APIã€‚ /// ## âœ¨ Core Features // âœ¨ æ ¸å¿ƒåŠŸèƒ½ /// ### ğŸ¤– Chat Completions // ğŸ¤– èŠå¤©å®Œæˆ /// - Standard Chat: Create chat completions with various models including glm-4.7 // æ ‡å‡†èŠå¤© ï¼šä½¿ç”¨åŒ…æ‹¬ glm-4.7 åœ¨å†…çš„å„ç§æ¨¡å‹åˆ›å»ºèŠå¤©è¡¥å…¨åŠŸèƒ½ /// - Streaming Support: Real-time streaming responses for interactive applications // æµåª’ä½“æ”¯æŒ ï¼šäº¤äº’å¼åº”ç”¨ç¨‹åºçš„å®æ—¶æµåª’ä½“å“åº” /// - Tool Calling: Function calling capabilities for enhanced AI interactions // å·¥å…·è°ƒç”¨ ï¼šå¢å¼ºäººå·¥æ™ºèƒ½äº¤äº’çš„å‡½æ•°è°ƒç”¨åŠŸèƒ½ /// - Character Role-Playing: Support for character-based conversations with charglm-3 model // è§’è‰²æ‰®æ¼” ï¼šæ”¯æŒåŸºäºè§’è‰²çš„å¯¹è¯ï¼ˆä½¿ç”¨ charglm-3 æ¨¡å‹ï¼‰ /// - Multimodal Chat: Image understanding capabilities with vision models // å¤šæ¨¡æ€èŠå¤© ï¼šåˆ©ç”¨è§†è§‰æ¨¡å‹å®ç°å›¾åƒç†è§£åŠŸèƒ½ /// ### ğŸ§  Embeddings // ğŸ§  åµŒå…¥ /// - Text Embeddings: Generate high-quality vector embeddings for text // æ–‡æœ¬åµŒå…¥ ï¼šä¸ºæ–‡æœ¬ç”Ÿæˆé«˜è´¨é‡çš„çŸ¢é‡åµŒå…¥ /// - Configurable Dimensions: Customizable embedding dimensions // å¯é…ç½®å°ºå¯¸ ï¼šå¯è‡ªå®šä¹‰çš„åµŒå…¥å°ºå¯¸ /// - Batch Processing: Support for multiple inputs in a single request // æ‰¹é‡å¤„ç† ï¼šæ”¯æŒåœ¨å•ä¸ªè¯·æ±‚ä¸­è¾“å…¥å¤šä¸ªæ•°æ®ã€‚ /// ### ğŸ¥ Video Generation // ğŸ¥ è§†é¢‘ç”Ÿæˆ /// - Text-to-Video: Generate videos from text prompts // æ–‡æœ¬è½¬è§†é¢‘ ï¼šæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆè§†é¢‘ /// - Image-to-Video: Create videos from image inputs // å›¾åƒè½¬è§†é¢‘ ï¼šä»å›¾åƒè¾“å…¥åˆ›å»ºè§†é¢‘ /// - Customizable Parameters: Control quality, duration, FPS, and size // å¯è‡ªå®šä¹‰å‚æ•° ï¼šæ§åˆ¶ç”»è´¨ã€æŒç»­æ—¶é—´ã€å¸§ç‡å’Œå°ºå¯¸ /// - Audio Support: Optional audio generation for videos // éŸ³é¢‘æ”¯æŒ ï¼šè§†é¢‘å¯é€‰éŸ³é¢‘ç”ŸæˆåŠŸèƒ½ /// ### ğŸµ Audio Processing // ğŸµ éŸ³é¢‘å¤„ç† /// - Speech Transcription: Convert audio files to text // è¯­éŸ³è½¬å½• ï¼šå°†éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸ºæ–‡æœ¬ /// - Multiple Formats: Support for various audio file formats // æ”¯æŒå¤šç§æ ¼å¼ ï¼šæ”¯æŒå¤šç§éŸ³é¢‘æ–‡ä»¶æ ¼å¼ /// ### ğŸ¤ Assistant API // ğŸ¤ åŠ©æ‰‹ API /// - Conversation Management: Structured conversation handling // å¯¹è¯ç®¡ç† ï¼šç»“æ„åŒ–å¯¹è¯å¤„ç† /// - Streaming Conversations: Real-time assistant interactions // æµåª’ä½“å¯¹è¯ ï¼šå®æ—¶åŠ©æ‰‹äº’åŠ¨ /// - Metadata Support: Rich conversation context and user information // å…ƒæ•°æ®æ”¯æŒ ï¼šä¸°å¯Œçš„å¯¹è¯ä¸Šä¸‹æ–‡å’Œç”¨æˆ·ä¿¡æ¯ /// ### ğŸ”§ Advanced Tools // ğŸ”§ é«˜çº§å·¥å…· /// - Web Search: Integrated web search capabilities // ç½‘ç»œæœç´¢ ï¼šé›†æˆç½‘ç»œæœç´¢åŠŸèƒ½ /// - File Management: Upload, download, and manage files // æ–‡ä»¶ç®¡ç† ï¼šä¸Šä¼ ã€ä¸‹è½½å’Œç®¡ç†æ–‡ä»¶ /// - Batch Operations: Efficient batch processing for multiple requests // æ‰¹é‡æ“ä½œ ï¼šé«˜æ•ˆæ‰¹é‡å¤„ç†å¤šä¸ªè¯·æ±‚ /// - Content Moderation: Built-in content safety and moderation // å†…å®¹å®¡æ ¸ ï¼šå†…ç½®å†…å®¹å®‰å…¨å’Œå®¡æ ¸åŠŸèƒ½ /// - Image Generation: AI-powered image creation // å›¾åƒç”Ÿæˆ ï¼šäººå·¥æ™ºèƒ½é©±åŠ¨çš„å›¾åƒåˆ›å»º"
[site-sdk.python-lib.pip/pypi]: https://pypi.org/project/zai-sdk/ "(: pip install -- zai-sdk) (Requires: Python >=3.8) (Provides-Extra: [cli], [extended-testing])"
[site-sdk.java.src/gh]: https://github.com/zai-org/z-ai-sdk-java.git "(MIT) (Languages: Java 100.0%) Java SDK for Z.ai Open Platform /// The official Java SDK for Z.ai platforms, providing a unified interface to access powerful AI capabilities including chat completion, embeddings, image generation, audio processing, and more. // Z.ai å¹³å°çš„å®˜æ–¹ Java SDKï¼Œæä¾›ç»Ÿä¸€çš„æ¥å£æ¥è®¿é—®å¼ºå¤§çš„ AI åŠŸèƒ½ï¼ŒåŒ…æ‹¬èŠå¤©è‡ªåŠ¨å®Œæˆã€åµŒå…¥ã€å›¾åƒç”Ÿæˆã€éŸ³é¢‘å¤„ç†ç­‰ç­‰ã€‚ /// âœ¨ Features // âœ¨ ç‰¹ç‚¹ /// - ğŸš€ Type-safe API: All interfaces are fully type-encapsulated, no need to consult API documentation // ğŸš€ ç±»å‹å®‰å…¨çš„ API ï¼šæ‰€æœ‰æ¥å£éƒ½å®Œå…¨ç±»å‹å°è£…ï¼Œæ— éœ€æŸ¥é˜… API æ–‡æ¡£ã€‚ /// - ğŸ”§ Easy Integration: Simple and intuitive API design for quick integration // ğŸ”§ è½»æ¾é›†æˆ ï¼šç®€æ´ç›´è§‚çš„ API è®¾è®¡ï¼Œå®ç°å¿«é€Ÿé›†æˆ /// - âš¡ High Performance: Built with modern Java libraries for optimal performance // âš¡ é«˜æ€§èƒ½ ï¼šé‡‡ç”¨ç°ä»£ Java åº“æ„å»ºï¼Œä»¥å®ç°æœ€ä½³æ€§èƒ½ /// - ğŸ›¡ï¸ Secure: Built-in authentication and token management // ğŸ›¡ï¸ å®‰å…¨ ï¼šå†…ç½®èº«ä»½éªŒè¯å’Œä»¤ç‰Œç®¡ç† /// - ğŸ“¦ Lightweight: Minimal dependencies for easy project integration // ğŸ“¦ è½»é‡çº§ ï¼šä¾èµ–é¡¹æå°‘ï¼Œæ˜“äºé¡¹ç›®é›†æˆ"
[site-sdk.java-lib.mvn/sonatype]: https://central.sonatype.com/artifact/ai.z.openapi/zai-sdk "(~ pkg:maven/ai.z.openapi/zai-sdk@0.3.0) (Licenses: MIT License) Java SDK for Z.ai Open Platform API"
[ChatGLM.paper/arxiv]: https://arxiv.org/abs/2406.12793 "(License: CC BY 4.0) [Submitted on 18 Jun 2024 (v1), last revised 30 Jul 2024 (this version, v2)] { Computation and Language (cs.CL) } (doi: 10.48550/arXiv.2406.12793) ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools // ChatGLMï¼šä» GLM-130B åˆ° GLM-4 çš„ä¸€ç³»åˆ—å¤§å‹è¯­è¨€æ¨¡å‹ æ‰€æœ‰å·¥å…· /// We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL (. gh: @THUDM) and this https URL (. hf: @THUDM). // æˆ‘ä»¬æ¨å‡º ChatGLMï¼Œè¿™æ˜¯ä¸€ä¸ªä¸æ–­å‘å±•å£®å¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹å®¶æ—ï¼Œæ˜¯æˆ‘ä»¬æŒç»­ç ”å‘çš„æˆæœã€‚æœ¬æŠ¥å‘Šä¸»è¦èšç„¦äº GLM-4 è¯­è¨€ç³»åˆ—ï¼ŒåŒ…æ‹¬ GLM-4ã€GLM-4-Air å’Œ GLM-4-9Bã€‚å®ƒä»¬ä»£è¡¨äº†æˆ‘ä»¬ç›®å‰æœ€å¼ºå¤§çš„æ¨¡å‹ï¼Œèåˆäº†å‰ä¸‰ä»£ ChatGLM çš„æ‰€æœ‰ç»éªŒå’Œå¿ƒå¾—ã€‚è¿„ä»Šä¸ºæ­¢ï¼ŒGLM-4 æ¨¡å‹å·²åœ¨æ•°ä¸‡äº¿ä¸ªè¯å…ƒï¼ˆä¸»è¦ä¸ºä¸­æ–‡å’Œè‹±æ–‡ï¼‰ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åŒ…å«æ¥è‡ª 24 ç§è¯­è¨€çš„å°‘é‡è¯­æ–™åº“ï¼Œä¸”ä¸»è¦é’ˆå¯¹ä¸­æ–‡å’Œè‹±æ–‡ç”¨æ³•è¿›è¡Œäº†å¯¹é½ã€‚é«˜è´¨é‡çš„å¯¹é½æ˜¯é€šè¿‡å¤šé˜¶æ®µçš„åè®­ç»ƒè¿‡ç¨‹å®ç°çš„ï¼Œè¯¥è¿‡ç¨‹åŒ…æ‹¬ç›‘ç£å¼å¾®è°ƒå’Œä»äººå·¥åé¦ˆä¸­å­¦ä¹ ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒGLM-4 1) åœ¨ MMLUã€GSM8Kã€MATHã€BBHã€GPQA å’Œ HumanEval ç­‰é€šç”¨æŒ‡æ ‡æ–¹é¢ä¸ GPT-4 è¡¨ç°ç›¸å½“ç”šè‡³æ›´ä¼˜ï¼›2) åœ¨ IFEval æµ‹è¯•ä¸­ï¼Œå…¶æŒ‡ä»¤è·Ÿè¸ªèƒ½åŠ›æ¥è¿‘ GPT-4-Turboï¼›3) åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­ä¸ GPT-4 Turbo (128K) å’Œ Claude 3 è¡¨ç°ç›¸å½“ï¼›4) åœ¨ AlignBench æµ‹è¯•ä¸­ï¼Œå…¶ä¸­æ–‡å¯¹é½èƒ½åŠ›ä¼˜äº GPT-4ã€‚GLM-4 All Tools æ¨¡å‹è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œèƒ½å¤Ÿç†è§£ç”¨æˆ·æ„å›¾ï¼Œå¹¶è‡ªä¸»å†³å®šä½•æ—¶ä»¥åŠä½¿ç”¨å“ªäº›å·¥å…·ï¼ˆåŒ…æ‹¬ç½‘é¡µæµè§ˆå™¨ã€Python è§£é‡Šå™¨ã€æ–‡æœ¬è½¬å›¾åƒæ¨¡å‹å’Œç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°ï¼‰æ¥é«˜æ•ˆå®Œæˆå¤æ‚ä»»åŠ¡ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå®ƒåœ¨è¯¸å¦‚é€šè¿‡ç½‘é¡µæµè§ˆè·å–åœ¨çº¿ä¿¡æ¯å’Œä½¿ç”¨ Python è§£é‡Šå™¨è§£å†³æ•°å­¦é—®é¢˜ç­‰ä»»åŠ¡ä¸­ï¼Œè¡¨ç°ä¸ GPT-4 All Tools ç›¸å½“ç”šè‡³æ›´èƒœä¸€ç­¹ã€‚å¤šå¹´æ¥ï¼Œæˆ‘ä»¬å¼€æºäº†ä¸€ç³»åˆ—æ¨¡å‹ï¼ŒåŒ…æ‹¬ ChatGLM-6Bï¼ˆä¸‰ä»£ï¼‰ã€GLM-4-9Bï¼ˆ128Kã€1Mï¼‰ã€GLM-4V-9Bã€WebGLM å’Œ CodeGeeXï¼Œä»…åœ¨ 2023 å¹´å°±åœ¨ Hugging Face ä¸Šå¸å¼•äº†è¶…è¿‡ 1000 ä¸‡æ¬¡çš„ä¸‹è½½ã€‚ å¯ä»¥é€šè¿‡æ­¤ https URL å’Œæ­¤ https URL è®¿é—®å¼€æ”¾æ¨¡å‹ã€‚"
[codegeex-plg.vsix:vsc/ovsx]: https://open-vsx.org/extension/aminer/codegeex "(@AMiner - ZHIPU AI) CodeGeeXæ˜¯ä»€ä¹ˆï¼Ÿ // Whatâ€™s CodeGeeXï¼Ÿ  /// CodeGeeXæ˜¯æ™ºè°±AIæ——ä¸‹çš„ä¸€æ¬¾åŸºäºå¤§æ¨¡å‹çš„æ™ºèƒ½ç¼–ç¨‹åŠ©æ‰‹ï¼Œå®ƒå¯ä»¥å®ç°ä»£ç çš„ç”Ÿæˆä¸è¡¥å…¨ï¼Œè‡ªåŠ¨ä¸ºä»£ç æ·»åŠ æ³¨é‡Šï¼Œä¸åŒç¼–ç¨‹è¯­è¨€çš„ä»£ç é—´å®ç°äº’è¯‘ï¼Œé’ˆå¯¹æŠ€æœ¯å’Œä»£ç é—®é¢˜çš„æ™ºèƒ½é—®ç­”ï¼Œå½“ç„¶è¿˜åŒ…æ‹¬ä»£ç è§£é‡Šï¼Œç”Ÿæˆå•å…ƒæµ‹è¯•ï¼Œå®ç°ä»£ç å®¡æŸ¥ï¼Œä¿®å¤ä»£ç bugç­‰éå¸¸ä¸°å¯Œçš„åŠŸèƒ½ã€‚ // CodeGeeX is an AI-based coding assistant. It is powered by a large-scale multilingual code generation model, lots of features including generates and completes code, automatically adds comments, translates code between different programming languages, provides intelligent answers to technical and code-related questions, code explanation, unit test generation, code review, bug fixing and so on. (src: gh:zai-org/CodeGeeX4.git, src-old: gh:THUDM/CodeGeeX4.git)"
[codegeex-plg.vsix:vsc/msvs]: https://marketplace.visualstudio.com/items?itemName=aminer.codegeex "(@aminer - Zhipu AI) CodeGeeX is an AI-based coding assistant, which can suggest code in the current or following lines. It is powered by a large-scale multilingual code generation model with 13 billion parameters, pretrained on a large code corpus of more than 20 programming languages. // CodeGeeX æ˜¯ä¸€æ¬¾åŸºäºäººå·¥æ™ºèƒ½çš„ä»£ç åŠ©æ‰‹ï¼Œå¯ä»¥ä¸ºå½“å‰è¡Œæˆ–åç»­è¡Œæä¾›ä»£ç å»ºè®®ã€‚å®ƒé‡‡ç”¨å¤§è§„æ¨¡å¤šè¯­è¨€ä»£ç ç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ‹¥æœ‰ 130 äº¿ä¸ªå‚æ•°ï¼Œå¹¶åœ¨åŒ…å« 20 å¤šç§ç¼–ç¨‹è¯­è¨€çš„å¤§å‹ä»£ç è¯­æ–™åº“ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒã€‚ (src: gh:zai-org/CodeGeeX4.git, src-old: gh:THUDM/CodeGeeX4.git)"
[codegeex-plg.zip:jb/jetbrains]: https://plugins.jetbrains.com/plugin/20587-codegeex-ai-coding-assistant "(~ id: ai.codegeex.plugin) CodeGeeX: AI Coding Assistant"
[codegeex-plg.vsix:vs2022/msvs]: https://marketplace.visualstudio.com/items?itemName=aminer.CodeGeeXVisualStudio "(Works with: Visual Studio 2026 (Arm64), 2026 (amd64), 2022 (Arm64), 2022 (amd64))"
[codegeex-plg.vsix:vs2019/msvs]: https://marketplace.visualstudio.com/items?itemName=aminer.CodeGeeXVisualStudio2019 "(Works with: Visual Studio 2019)"
[codegeex-plg.zip:hbx/dcloud]: https://ext.dcloud.net.cn/plugin?id=15497 "(~ id: codegeex-zhipuai) (3.6MB) CodeGeeX: AI Code AutoComplete, Chat, Auto Comment /// CodeGeeX æ˜¯ä¸€æ¬¾åŸºäºå¤§æ¨¡å‹çš„æ™ºèƒ½ç¼–ç¨‹åŠ©æ‰‹ï¼Œå®ƒå¯ä»¥å®ç°ä»£ç çš„ç”Ÿæˆä¸è¡¥å…¨ï¼Œè‡ªåŠ¨ä¸ºä»£ç æ·»åŠ æ³¨é‡Šï¼Œä¸åŒç¼–ç¨‹è¯­è¨€çš„ä»£ç é—´å®ç°äº’è¯‘ä»¥åŠé’ˆå¯¹æŠ€æœ¯å’Œä»£ç é—®é¢˜çš„æ™ºèƒ½é—®ç­”åŠŸèƒ½ã€‚å¸®åŠ©å¼€å‘è€…æ˜¾è‘—æé«˜å·¥ä½œæ•ˆç‡ï¼Œ CodeGeeX æ”¯æŒ 100+ ç§ç¼–ç¨‹è¯­è¨€ï¼Œé€‚é…å¤šç§ä¸»æµ IDE å¹³å°ï¼ŒåŒ…æ‹¬ VSCodeã€JetBrains IDEsã€Vimã€HBuilderXã€Deepln-IDE ç­‰ã€‚"
[codegeex.site/cn]: https://codegeex.cn/ "Powerful AI Assistant for developers // é¢å‘å¼€å‘è€…çš„å¼ºå¤§ AI åŠ©æ‰‹ /// - Code Generation and Completion // ä»£ç ç”Ÿæˆä¸å®Œæˆ /// - Comment Generation // æ³¨é‡Šç”Ÿæˆ /// - Code Translation // ä»£ç ç¿»è¯‘ /// - Ask CodeGeeX // è¯¢é—® CodeGeeX"
[codegeex.src/gh]: https://github.com/zai-org/CodeGeeX4.git "(Apache-2.0) (Languages: Python 73.5%, Rust 26.5%) CodeGeeX4-ALL-9B, a versatile model for all AI software development scenarios, including code completion, code interpreter, web search, function calling, repository-level Q&A and much more. // CodeGeeX4-ALL-9B æ˜¯ä¸€æ¬¾é€‚ç”¨äºæ‰€æœ‰ AI è½¯ä»¶å¼€å‘åœºæ™¯çš„å¤šåŠŸèƒ½æ¨¡å‹ï¼ŒåŒ…æ‹¬ä»£ç è¡¥å…¨ã€ä»£ç è§£é‡Šå™¨ã€ç½‘ç»œæœç´¢ã€å‡½æ•°è°ƒç”¨ã€å­˜å‚¨åº“çº§é—®ç­”ç­‰ç­‰ã€‚ /// CodeGeeX4: Open Multilingual Code Generation Model // CodeGeeX4ï¼šå¼€æ”¾å¼å¤šè¯­è¨€ä»£ç ç”Ÿæˆæ¨¡å‹ /// We introduce CodeGeeX4-ALL-9B, the open-source version of the latest CodeGeeX4 model series. It is a multilingual code generation model continually trained on the GLM-4-9B (. gh: THUDM/GLM-4.git), significantly enhancing its code generation capabilities. Using a single CodeGeeX4-ALL-9B model, it can support comprehensive functions such as code completion and generation, code interpreter, web search, function call, repository-level code Q&A, covering various scenarios of software development. CodeGeeX4-ALL-9B has achieved highly competitive performance on public benchmarks, such as BigCodeBench (. hf-datasets: bigcode/bigcodebench) and NaturalCodeBench (. gh: THUDM/NaturalCodeBench.git). It is currently the most powerful code generation model with less than 10B parameters, even surpassing much larger general-purpose models, achieving the best balance in terms of inference speed and model performance. // æˆ‘ä»¬æ¨å‡º CodeGeeX4-ALL-9Bï¼Œå®ƒæ˜¯æœ€æ–° CodeGeeX4 æ¨¡å‹ç³»åˆ—çš„å¼€æºç‰ˆæœ¬ã€‚å®ƒæ˜¯ä¸€ä¸ªåŸºäº GLM-4-9B æŒç»­è®­ç»ƒçš„å¤šè¯­è¨€ä»£ç ç”Ÿæˆæ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚ä»…ä½¿ç”¨ä¸€ä¸ª CodeGeeX4-ALL-9B æ¨¡å‹ï¼Œå³å¯æ”¯æŒä»£ç è¡¥å…¨ä¸ç”Ÿæˆã€ä»£ç è§£é‡Šå™¨ã€ç½‘ç»œæœç´¢ã€å‡½æ•°è°ƒç”¨ã€ä»“åº“çº§ä»£ç é—®ç­”ç­‰å¤šç§åŠŸèƒ½ï¼Œè¦†ç›–è½¯ä»¶å¼€å‘çš„å„ç§åœºæ™¯ã€‚CodeGeeX4-ALL-9B åœ¨ BigCodeBench å’Œ NaturalCodeBench ç­‰å…¬å¼€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æå…·ç«äº‰åŠ›çš„æ€§èƒ½ã€‚å®ƒæ˜¯ç›®å‰å‚æ•°é‡å°äº 100 äº¿çš„æœ€å¼ºä»£ç ç”Ÿæˆæ¨¡å‹ï¼Œç”šè‡³è¶…è¶Šäº†è®¸å¤šå‚æ•°é‡æ›´å¤§çš„é€šç”¨æ¨¡å‹ï¼Œåœ¨æ¨ç†é€Ÿåº¦å’Œæ¨¡å‹æ€§èƒ½ä¹‹é—´å®ç°äº†æœ€ä½³å¹³è¡¡ã€‚"
[codegeex-model.demo/hf]: https://huggingface.co/spaces/zai-org/CodeGeeX "ğŸª§ HF DEMO // ğŸª§ HF æ¼”ç¤º"
[codegeex-model.tensor/hf]: https://huggingface.co/zai-org/codegeex4-all-9b "(CodeGeeX4 License) (Model size: 9B params; Tensor type: BF16) (Topics: Text Generation, Transformers, Safetensors, Chinese, English) (Tags: chatglm, glm, codegeex, thudm, custom_code) CodeGeeX4: Open Multilingual Code Generation Model"
[codegeex-models.coll/hf]: https://huggingface.co/collections/zai-org/codegeex4 "Open Multilingual Code Generation Model // å¼€æ”¾å¼å¤šè¯­è¨€ä»£ç ç”Ÿæˆæ¨¡å‹ (hf: zai-org/codegeex4-all-9b: Text Generation â€¢ 9B) (hf: zai-org/codegeex4-all-9b-GGUF: Text Generation â€¢ 9B)"
[codegeex-model.ollama/ollama]: https://ollama.com/library/codegeex4 "(: ollama run -- codegeex4) (9b) codegeex4 /// A versatile model for AI software development scenarios, including code completion. // é€‚ç”¨äºäººå·¥æ™ºèƒ½è½¯ä»¶å¼€å‘åœºæ™¯çš„å¤šåŠŸèƒ½æ¨¡å‹ï¼ŒåŒ…æ‹¬ä»£ç è¡¥å…¨åŠŸèƒ½ã€‚ (src: gh:THUDM/CodeGeeX4.git) (tensor: hf:THUDM/codegeex4-all-9b)"
[codegeex-build.candle/gh]: https://github.com/huggingface/candle/tree/main/candle-examples/examples/codegeex4-9b "candle-codegeex4_9b /// THUDM/CodeGeeX4 is a versatile model for all AI software development scenarios, including code completion, code interpreter, web search, function calling, repository-level Q&A and much more. // THUDM/CodeGeeX4 æ˜¯ä¸€ä¸ªé€‚ç”¨äºæ‰€æœ‰ AI è½¯ä»¶å¼€å‘åœºæ™¯çš„å¤šåŠŸèƒ½æ¨¡å‹ï¼ŒåŒ…æ‹¬ä»£ç è¡¥å…¨ã€ä»£ç è§£é‡Šå™¨ã€ç½‘ç»œæœç´¢ã€å‡½æ•°è°ƒç”¨ã€å­˜å‚¨åº“çº§é—®ç­”ç­‰ç­‰ã€‚ /// (: cargo run --example codegeex4-9b --release --features cuda -- --prompt 'please write a insertion sort in rust' --sample-len 300 # run with cuda) (: cargo run --example codegeex4-9b --release -- --cpu --prompt 'please write a insertion sort in rust' --sample-len 300) /// (~ cit: @inproceedings{zheng2023codegeex, title={CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X}, author={Qinkai Zheng and Xiao Xia and Xu Zou and Yuxiao Dong and Shan Wang and Yufei Xue and Zihan Wang and Lei Shen and Andi Wang and Yang Li and Teng Su and Zhilin Yang and Jie Tang}, booktitle={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages={5673--5684}, year={2023} })"
[efficient-method:APAR.paper/arxiv]: https://arxiv.org/abs/2401.06761 "(License: CC BY 4.0) [Submitted on 12 Jan 2024] { Computation and Language (cs.CL) } (doi: 10.48550/arXiv.2401.06761) APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding // APARï¼šLLM å¯ä»¥è¿›è¡Œè‡ªå¹¶è¡Œè‡ªå›å½’è§£ç  /// The massive adoption of large language models (LLMs) demands efficient deployment strategies. However, the auto-regressive decoding process, which is fundamental to how most LLMs generate text, poses challenges to achieve efficient serving. In this work, we introduce a parallel auto-regressive generation method. By instruct-tuning on general domain data that contains hierarchical structures, we enable LLMs to independently plan their generation process and perform auto-parallel auto-regressive (APAR) generation, significantly reducing the number of generation steps. APAR alone can achieve up to 2x speed-up, and when combined with speculative decoding, the speed-up can reach up to 4x. In addition, APAR reduces the key-value cache consumption and attention computation during generation. This leads to a throughput increase of 20-70% and a latency reduce of 20-35% in high-throughput scenarios, compared to state-of-the-art serving frameworks. // å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹¿æ³›åº”ç”¨å¯¹é«˜æ•ˆçš„éƒ¨ç½²ç­–ç•¥æå‡ºäº†æ›´é«˜çš„è¦æ±‚ã€‚ç„¶è€Œï¼Œè‡ªå›å½’è§£ç è¿‡ç¨‹æ˜¯å¤§å¤šæ•° LLM ç”Ÿæˆæ–‡æœ¬çš„åŸºç¡€ï¼Œä½†å…¶é«˜æ•ˆéƒ¨ç½²å´é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¹¶è¡Œè‡ªå›å½’ç”Ÿæˆæ–¹æ³•ã€‚é€šè¿‡å¯¹åŒ…å«å±‚çº§ç»“æ„çš„é€šç”¨é¢†åŸŸæ•°æ®è¿›è¡ŒæŒ‡ä»¤è°ƒä¼˜ï¼Œæˆ‘ä»¬ä½¿ LLM èƒ½å¤Ÿç‹¬ç«‹è§„åˆ’å…¶ç”Ÿæˆè¿‡ç¨‹å¹¶æ‰§è¡Œè‡ªåŠ¨å¹¶è¡Œè‡ªå›å½’ï¼ˆAPARï¼‰ç”Ÿæˆï¼Œä»è€Œæ˜¾è‘—å‡å°‘ç”Ÿæˆæ­¥éª¤ã€‚APAR æœ¬èº«å³å¯å®ç°é«˜è¾¾ 2 å€çš„åŠ é€Ÿï¼Œè€Œä¸æ¨æµ‹æ€§è§£ç ç›¸ç»“åˆï¼ŒåŠ é€Ÿæ¯”å¯è¾¾ 4 å€ã€‚æ­¤å¤–ï¼ŒAPAR è¿˜é™ä½äº†ç”Ÿæˆè¿‡ç¨‹ä¸­é”®å€¼ç¼“å­˜çš„æ¶ˆè€—å’Œæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—é‡ã€‚ä¸ç°æœ‰æœ€å…ˆè¿›çš„æœåŠ¡æ¡†æ¶ç›¸æ¯”ï¼Œåœ¨é«˜ååé‡åœºæ™¯ä¸‹ï¼Œè¯¥æ–¹æ³•å¯å®ç° 20%è‡³ 70%çš„ååé‡æå‡å’Œ 20%è‡³ 35%çš„å»¶è¿Ÿé™ä½ã€‚ /// Figure 1:APAR Decoding Overview. Different from the original auto-regressive decoding, APAR detects potential parts to be generated in parallel and issues multiple generation threads. // å›¾ 1ï¼š APAR è§£ç æ¦‚è¿°ã€‚ ä¸åŸå§‹çš„è‡ªå›å½’è§£ç ä¸åŒï¼ŒAPAR å¯ä»¥å¹¶è¡Œæ£€æµ‹å¾…ç”Ÿæˆçš„æ½œåœ¨éƒ¨åˆ†ï¼Œå¹¶å‘å‡ºå¤šä¸ªç”Ÿæˆçº¿ç¨‹ã€‚ /// Figure 2:APAR Training Inputs. Based on pre-defined rules, the original sequence is transformed into a paragraph tree, which is used to train APAR models. Any token attends only to tokens on its path to root. // å›¾ 2ï¼š APAR è®­ç»ƒè¾“å…¥ã€‚ åŸºäºé¢„å®šä¹‰çš„è§„åˆ™ï¼Œ åŸå§‹åºåˆ—è¢«è½¬æ¢ä¸ºæ®µè½æ ‘ ï¼Œç”¨äºè®­ç»ƒ APAR æ¨¡å‹ã€‚æ¯ä¸ªè¯å…ƒä»…å…³æ³¨å…¶åˆ°æ ¹èŠ‚ç‚¹è·¯å¾„ä¸Šçš„è¯å…ƒã€‚ /// Figure 3:APAR Decoding. Suppose we are generating a sequence p_0 âˆ’ p_1 âˆ’ p_2. The decoding of p_1 (root) and p_0 (detail) are issued in parallel at (t_1 + 1). Similarly, p_2 (root) and p_1 (detail) are issued at (t_2 + 1). Prefix tokens are shared and forked generation threads can be released once finished. // å›¾ 3ï¼š APAR è§£ç ã€‚ å‡è®¾æˆ‘ä»¬æ­£åœ¨ç”Ÿæˆåºåˆ— p_0 âˆ’ p_1 âˆ’ p_2 ã€‚ p_1 ï¼ˆæ ¹ï¼‰å’Œ p_0 ï¼ˆç»†èŠ‚ï¼‰çš„è§£ç åœ¨ t_1 + 1 å¤„å¹¶è¡Œå‘å‡ºã€‚ç±»ä¼¼åœ°ï¼Œ p_2 ï¼ˆæ ¹ï¼‰å’Œ p_1 ï¼ˆç»†èŠ‚ï¼‰çš„è§£ç åœ¨ t_2 + 1 å¤„å‘å‡ºã€‚å‰ç¼€æ ‡è®°æ˜¯å…±äº«çš„ï¼Œåˆ†å‰çš„ç”Ÿæˆçº¿ç¨‹å®Œæˆåå³å¯é‡Šæ”¾ã€‚ /// The key idea of APAR is to make LLMs explicitly aware of such parallelizable structures, and spawn auto-parallel auto-regressive decoding threads accordingly. Specifically, APAR involves two key components. First, we post-train the LLMs with hierarchical text structures, which we referred to as paragraph nodes (Section 2.2). Second, we design the decoding algorithm to support parallel decoding operations, including maintaining the hierarchical structure in generation and restoring it into a linear sequence (Section 2.3). // APAR çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ LLM èƒ½å¤Ÿæ˜¾å¼åœ°æ„ŸçŸ¥è¿™ç§å¯å¹¶è¡ŒåŒ–çš„ç»“æ„ï¼Œå¹¶æ®æ­¤ç”Ÿæˆè‡ªåŠ¨å¹¶è¡Œè‡ªå›å½’è§£ç çº¿ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼ŒAPAR åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨å±‚çº§æ–‡æœ¬ç»“æ„ï¼ˆç§°ä¸ºæ®µè½èŠ‚ç‚¹ ï¼Œè§ 2.2 èŠ‚ï¼‰å¯¹ LLM è¿›è¡Œåè®­ç»ƒã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è®¾è®¡è§£ç ç®—æ³•ä»¥æ”¯æŒå¹¶è¡Œè§£ç æ“ä½œï¼ŒåŒ…æ‹¬åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿æŒå±‚çº§ç»“æ„ï¼Œä»¥åŠå°†å…¶æ¢å¤ä¸ºçº¿æ€§åºåˆ—ï¼ˆè§ 2.3 èŠ‚ï¼‰ã€‚ /// Features // åŠŸèƒ½ ///: Based on the aforementioned decoding algorithm and the distribution of user queries, we identify three key features of APAR decoding that give rise to its superior performance in terms of inference latency, throughput, and memory consumption. // åŸºäºä¸Šè¿°è§£ç ç®—æ³•å’Œç”¨æˆ·æŸ¥è¯¢åˆ†å¸ƒï¼Œæˆ‘ä»¬ç¡®å®šäº† APAR è§£ç çš„ä¸‰ä¸ªå…³é”®ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾ä½¿å…¶åœ¨æ¨ç†å»¶è¿Ÿã€ååé‡å’Œå†…å­˜æ¶ˆè€—æ–¹é¢å…·æœ‰ä¼˜è¶Šçš„æ€§èƒ½ã€‚ /// 1. Paralleled decoding structure reduces latency. // å¹¶è¡Œè§£ç ç»“æ„é™ä½äº†å»¶è¿Ÿã€‚ ///: Through training on paragraph trees, the language model becomes an automatic online miner for parallel-able structure and concurrent generation threads are issued accordingly. The paralleled generation reduces generation steps. In memory-bound scenarios, the latency in each step remains roughly unchanged wrt. different level of decoding parallelism (i.e. dynamic batch size) and the latency can therefore be reduced proportionately (Fig 4). // é€šè¿‡å¯¹æ®µè½æ ‘è¿›è¡Œè®­ç»ƒï¼Œè¯­è¨€æ¨¡å‹å¯ä»¥è‡ªåŠ¨åœ¨çº¿æŒ–æ˜å¯å¹¶è¡ŒåŒ–çš„ç»“æ„ï¼Œå¹¶æ®æ­¤å‘å‡ºå¹¶å‘ç”Ÿæˆçº¿ç¨‹ã€‚å¹¶è¡Œç”Ÿæˆå‡å°‘äº†ç”Ÿæˆæ­¥éª¤ã€‚åœ¨å†…å­˜å—é™çš„åœºæ™¯ä¸‹ï¼Œæ¯ä¸ªæ­¥éª¤çš„å»¶è¿Ÿç›¸å¯¹äºä¸åŒçš„è§£ç å¹¶è¡Œåº¦ï¼ˆå³åŠ¨æ€æ‰¹å¤„ç†å¤§å°ï¼‰åŸºæœ¬ä¿æŒä¸å˜ï¼Œå› æ­¤å¯ä»¥æŒ‰æ¯”ä¾‹é™ä½å»¶è¿Ÿï¼ˆå›¾ 4 ï¼‰ã€‚ /// 2. Early release of child KV cache reduces memory consumption. // æå‰é‡Šæ”¾å­é”®å€¼ç¼“å­˜å¯å‡å°‘å†…å­˜æ¶ˆè€—ã€‚ ///: In the auto-regressive generation process, the KV cache of all tokens must be retained before the sequence is completely generated. In APAR, however, once a forked sequence (i.e. a generation thread) completes generation, the KV cache belonging only to the forked sequence can be released immediately, while the remaining part of the generation continues. Under the effect of early release strategy, as shown in later Fig 4(a), up to 50% of the generation cache can be saved while throughput remains the same. // åœ¨è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ‰€æœ‰ä»¤ç‰Œçš„é”®å€¼ç¼“å­˜å¿…é¡»åœ¨åºåˆ—å®Œå…¨ç”Ÿæˆä¹‹å‰ä¿ç•™ã€‚ç„¶è€Œï¼Œåœ¨ APAR ä¸­ï¼Œä¸€æ—¦ä¸€ä¸ªåˆ†å‰åºåˆ—ï¼ˆå³ä¸€ä¸ªç”Ÿæˆçº¿ç¨‹ï¼‰å®Œæˆç”Ÿæˆï¼Œå°±å¯ä»¥ç«‹å³é‡Šæ”¾ä»…å±äºè¯¥åˆ†å‰åºåˆ—çš„é”®å€¼ç¼“å­˜ï¼Œè€Œå‰©ä½™çš„ç”Ÿæˆå·¥ä½œåˆ™ç»§ç»­è¿›è¡Œã€‚åœ¨æå‰é‡Šæ”¾ç­–ç•¥çš„ä½œç”¨ä¸‹ï¼Œå¦‚å›¾ 4(a) æ‰€ç¤ºï¼Œåœ¨ååé‡ä¿æŒä¸å˜çš„æƒ…å†µä¸‹ï¼Œæœ€å¤šå¯ä»¥èŠ‚çœ 50% ä¸ªç”Ÿæˆç¼“å­˜ã€‚ /// 3. Reduced attention length saves computation. // ç¼©çŸ­æ³¨æ„åŠ›æŒç»­æ—¶é—´å¯ä»¥èŠ‚çœè®¡ç®—é‡ã€‚ ///: Auto-regressive generation requires a token to attend to all previously generated tokens. In APAR, on the other hand, a new token only attends to tokens along its path to the root of the paragraph tree, which reduces attention computation in generation. In a heavily-batched generation setting, latency incurred by memory access is amortized by the intense batched computation, make the generation process primarily computation-bound. Thus, the computation reduction in each token results in an improvement in throughput across different memory usages (Fig 4(a)), as well as reduction in latency with different extents of concurrency (Fig 4(b)). // è‡ªå›å½’ç”Ÿæˆè¦æ±‚æ¯ä¸ªè¯å…ƒå…³æ³¨æ‰€æœ‰å…ˆå‰ç”Ÿæˆçš„è¯å…ƒã€‚è€Œ APAR ç®—æ³•åˆ™ä¸åŒï¼Œæ–°ç”Ÿæˆçš„è¯å…ƒä»…å…³æ³¨å…¶é€šå¾€æ®µè½æ ‘æ ¹èŠ‚ç‚¹çš„è·¯å¾„ä¸Šçš„è¯å…ƒï¼Œä»è€Œå‡å°‘äº†ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ³¨æ„åŠ›è®¡ç®—ã€‚åœ¨é«˜åº¦æ‰¹é‡ç”Ÿæˆçš„åœºæ™¯ä¸‹ï¼Œå†…å­˜è®¿é—®å¸¦æ¥çš„å»¶è¿Ÿä¼šè¢«å¯†é›†çš„æ‰¹é‡è®¡ç®—æ‰€åˆ†æ‘Šï¼Œä½¿å¾—ç”Ÿæˆè¿‡ç¨‹ä¸»è¦å—è®¡ç®—é™åˆ¶ã€‚å› æ­¤ï¼Œæ¯ä¸ªè¯å…ƒè®¡ç®—é‡çš„å‡å°‘ï¼Œä½¿å¾—ä¸åŒå†…å­˜ä½¿ç”¨æƒ…å†µä¸‹çš„ååé‡å¾—åˆ°æå‡ï¼ˆå›¾ 4(a) ï¼‰ï¼Œå¹¶ä¸”ä¸åŒå¹¶å‘ç¨‹åº¦ä¸‹çš„å»¶è¿Ÿä¹Ÿå¾—åˆ°é™ä½ï¼ˆå›¾ 4(b) ï¼‰ã€‚ /// Results in Memory-Bound Scenarios // å†…å­˜å—é™åœºæ™¯ä¸‹çš„ç»“æœ /// Figure 4:Generation speed in memory-bound scenario. Models served on one H800-SXM5-80G GPU with batch size 1. // å›¾ 4ï¼š å†…å­˜å—é™åœºæ™¯ä¸‹çš„ç”Ÿæˆé€Ÿåº¦ã€‚ æ¨¡å‹åœ¨å•ä¸ª H800-SXM5-80G GPU ä¸Šè¿è¡Œï¼Œæ‰¹å¤„ç†å¤§å°ä¸º 1ã€‚ /// We inspect how APAR reduces generation latency in a memory-bound (i.e. small batch size) scenario, as well as its combined acceleration effect with speculative decoding. Considering that the the model is re-trained and the output length can be different on the same prompt, we normalize generation latency with generated tokens, adopting tokens per second as the metric for generation speed. The results are reported with batch size fixed as 1 and prefix sharing is not enabled. // æˆ‘ä»¬ç ”ç©¶äº†åœ¨å†…å­˜å—é™ï¼ˆå³å°æ‰¹é‡å¤§å°ï¼‰çš„åœºæ™¯ä¸‹ï¼ŒAPAR å¦‚ä½•é™ä½ç”Ÿæˆå»¶è¿Ÿï¼Œä»¥åŠå®ƒä¸æ¨æµ‹æ€§è§£ç ç›¸ç»“åˆçš„åŠ é€Ÿæ•ˆæœã€‚è€ƒè™‘åˆ°æ¨¡å‹ä¼šé‡æ–°è®­ç»ƒï¼Œå¹¶ä¸”åŒä¸€æç¤ºç¬¦çš„è¾“å‡ºé•¿åº¦å¯èƒ½ä¸åŒï¼Œæˆ‘ä»¬ä½¿ç”¨ç”Ÿæˆçš„æ ‡è®°æ•°é‡å¯¹ç”Ÿæˆå»¶è¿Ÿè¿›è¡Œå½’ä¸€åŒ–ï¼Œå¹¶é‡‡ç”¨æ¯ç§’æ ‡è®°æ•°ä½œä¸ºç”Ÿæˆé€Ÿåº¦çš„æŒ‡æ ‡ã€‚ç»“æœæŠ¥å‘ŠåŸºäºå›ºå®šæ‰¹é‡å¤§å°ä¸º 1 ä¸”æœªå¯ç”¨å‰ç¼€å…±äº«çš„æ¡ä»¶ã€‚ /// As shown in Fig 4, Vanilla-APAR achieves 2Ã— average speed up in Vicuna Bench and 1.4Ã— average speed up on MT Bench. APAR models learns to spawn parallel generation thread in and only in categories that exists a parallel-able structure. For instance, APAR-{7B,13B} seldom try to issue parallel a generation threads in coding and math related queries, which typically requires careful step by step reasoning or rigorous formats, resulting in no speed up. On the other hand, on categories like common-sense, generic and knowledge, the speed up is significant. When combined with speculative decoding, Medusa-APAR achieves an impressive 4Ã— average speed up in Vicuna Bench and 2.9Ã— average speed up in MT Bench, demonstrating strong reduction in generation latency. // å¦‚å›¾ 4 æ‰€ç¤ºï¼ŒVanilla-APAR åœ¨ Vicuna Bench æµ‹è¯•ä¸­å¹³å‡åŠ é€Ÿæ¯”ä¸º 2Ã— ï¼Œåœ¨ MT Bench æµ‹è¯•ä¸­å¹³å‡åŠ é€Ÿæ¯”ä¸º 1.4Ã— ã€‚APAR æ¨¡å‹å­¦ä¹ ä»…åœ¨å­˜åœ¨å¯å¹¶è¡Œç»“æ„çš„ç±»åˆ«ä¸­ç”Ÿæˆå¹¶è¡Œçº¿ç¨‹ã€‚ä¾‹å¦‚ï¼ŒAPAR-{7B,13B} å¾ˆå°‘å°è¯•åœ¨ç¼–ç å’Œæ•°å­¦ç›¸å…³çš„æŸ¥è¯¢ä¸­å‘å‡ºå¹¶è¡Œ A ç”Ÿæˆçº¿ç¨‹ï¼Œå› ä¸ºè¿™äº›æŸ¥è¯¢é€šå¸¸éœ€è¦ä»”ç»†çš„é€æ­¥æ¨ç†æˆ–ä¸¥æ ¼çš„æ ¼å¼ï¼Œå› æ­¤æ²¡æœ‰åŠ é€Ÿã€‚å¦ä¸€æ–¹é¢ï¼Œåœ¨å¸¸è¯†ã€é€šç”¨çŸ¥è¯†å’ŒçŸ¥è¯†ç­‰ç±»åˆ«ä¸­ï¼ŒåŠ é€Ÿæ¯”æ˜¾è‘—ã€‚å½“ä¸æ¨æµ‹æ€§è§£ç ç›¸ç»“åˆæ—¶ï¼ŒMedusa-APAR åœ¨ Vicuna Bench æµ‹è¯•ä¸­å¹³å‡åŠ é€Ÿæ¯”è¾¾åˆ°äº†ä»¤äººç©ç›®çš„ 4Ã— ï¼Œåœ¨ MT Bench æµ‹è¯•ä¸­å¹³å‡åŠ é€Ÿæ¯”è¾¾åˆ°äº† 2.9Ã— ï¼Œè¡¨æ˜ç”Ÿæˆå»¶è¿Ÿæ˜¾è‘—é™ä½ã€‚ /// Results in High-Throughput Scenarios // é«˜é€šé‡åœºæ™¯ä¸‹çš„ç»“æœ /// Figure 5:Serving statistics of Batched-APAR. Models served on one A100-SXM4-80G GPU. Error bars in (b) omitted for a clearer view. // å›¾ 5ï¼š æ‰¹é‡ APAR çš„æœåŠ¡ç»Ÿè®¡ä¿¡æ¯ã€‚ æ¨¡å‹ç”±å•ä¸ª A100-SXM4-80G GPU æä¾›æœåŠ¡ã€‚ä¸ºäº†æ›´æ¸…æ™°åœ°æ˜¾ç¤ºï¼Œ(b) ä¸­çš„è¯¯å·®çº¿å·²çœç•¥ã€‚ /// In high performance serving situations, increasing throughput and reducing serving memory are also important. We use Batched-APAR to serve the queries in APAR test set with different amount of GPU memory available. An overview of generation throughput and per-token latency is summarized in Fig 5. The dots in the plot show the mean value and error bars represent the 25% and 75% percentile in each setting. // åœ¨é«˜æ€§èƒ½æœåŠ¡åœºæ™¯ä¸­ï¼Œæé«˜ååé‡å’Œå‡å°‘æœåŠ¡å†…å­˜åŒæ ·é‡è¦ã€‚æˆ‘ä»¬ä½¿ç”¨æ‰¹é‡ APAR æ¥å¤„ç† APAR æµ‹è¯•é›†ä¸­çš„æŸ¥è¯¢ï¼Œå¹¶æµ‹è¯•äº†ä¸åŒå¯ç”¨ GPU å†…å­˜é‡çš„æƒ…å†µã€‚å›¾ 5 æ€»ç»“äº†ç”Ÿæˆååé‡å’Œå•ä»¤ç‰Œå»¶è¿Ÿçš„æ¦‚è§ˆã€‚å›¾ä¸­çš„ç‚¹è¡¨ç¤ºå¹³å‡å€¼ï¼Œè¯¯å·®çº¿è¡¨ç¤ºæ¯ç§è®¾ç½®ä¸‹çš„ 25% å’Œ 75% åˆ†ä½æ•°ã€‚ /// As shown in Fig 4(a), the throughput of Batched-APAR models surpass the maximum throughput of original models with only 20% of the KV Cache used, demonstrating memory efficiency. When using similar amount of memory, throughput is consistently increase by 20%âˆ¼70% across different cache usages. Batched-APAR models also demonstrate remarkable latency reduction in computation bound scenarios. As shown in Fig 4(b), Batched-APAR reduces 20%âˆ¼35% average latency when serving the same amount of concurrent requests. The latency of Batched-APAR-13B is even similar to the Original-7B model. // å¦‚å›¾ 4(a) æ‰€ç¤ºï¼Œä»…ä½¿ç”¨ 20% çš„ KV ç¼“å­˜ï¼Œæ‰¹é‡ APAR æ¨¡å‹çš„ååé‡å°±è¶…è¿‡äº†åŸå§‹æ¨¡å‹çš„æœ€å¤§ååé‡ï¼Œå±•ç°äº†å…¶å†…å­˜æ•ˆç‡ã€‚åœ¨å†…å­˜ä½¿ç”¨é‡ç›¸è¿‘çš„æƒ…å†µä¸‹ï¼Œä¸åŒç¼“å­˜ä½¿ç”¨ç‡ä¸‹çš„ååé‡å‡ç¨³å®šæå‡ 20% è‡³ 70%ã€‚æ‰¹é‡ APAR æ¨¡å‹åœ¨è®¡ç®—å¯†é›†å‹åœºæ™¯ä¸‹ä¹Ÿè¡¨ç°å‡ºæ˜¾è‘—çš„å»¶è¿Ÿé™ä½ã€‚å¦‚å›¾ 4(b) æ‰€ç¤ºï¼Œåœ¨å¤„ç†ç›¸åŒæ•°é‡çš„å¹¶å‘è¯·æ±‚æ—¶ï¼Œæ‰¹é‡ APAR çš„å¹³å‡å»¶è¿Ÿé™ä½äº† 20% è‡³ 35%ã€‚æ‰¹é‡ APAR-13B çš„å»¶è¿Ÿç”šè‡³ä¸åŸå§‹ 7B æ¨¡å‹ç›¸è¿‘ã€‚ /// The improvement in latency and throughput can be best explained by feature 2 and 3 as described in Section 2.4. We quantitatively measure how much computation and cache memory can be saved by using APAR decoding algorithm. We adopt the following metrics. // å»¶è¿Ÿå’Œååé‡çš„æå‡å¯ç”±ç¬¬ 2.4 èŠ‚æ‰€è¿°çš„ç‰¹æ€§ 2 å’Œç‰¹æ€§ 3 æ¥è§£é‡Šã€‚æˆ‘ä»¬å®šé‡åœ°è¡¡é‡äº†ä½¿ç”¨ APAR è§£ç ç®—æ³•å¯ä»¥èŠ‚çœå¤šå°‘è®¡ç®—èµ„æºå’Œç¼“å­˜å†…å­˜ã€‚æˆ‘ä»¬é‡‡ç”¨ä»¥ä¸‹æŒ‡æ ‡ã€‚ /// - â€¢ Max cached tokens is defined as the maximum number of KV cache slots required for generating a response. For auto-regressive generation, prompt tokens and all generated tokens need to be cached before generating [EOS] token. // â€¢ æœ€å¤§ç¼“å­˜ä»¤ç‰Œæ•°å®šä¹‰ä¸ºç”Ÿæˆå“åº”æ‰€éœ€çš„æœ€å¤§é”®å€¼ç¼“å­˜æ§½æ•°ã€‚å¯¹äºè‡ªå›å½’ç”Ÿæˆï¼Œåœ¨ç”Ÿæˆ [EOS] ä»¤ç‰Œä¹‹å‰ï¼Œéœ€è¦ç¼“å­˜æç¤ºä»¤ç‰Œå’Œæ‰€æœ‰å·²ç”Ÿæˆçš„ä»¤ç‰Œã€‚ /// - â€¢ Attended tokens is defined as the number of tokens attended to when predicting a specific token. For auto-regressive generation, all preceding tokens is needed when predicting a next token. // â€¢ å…³æ³¨è¯å…ƒæ•°æ˜¯æŒ‡åœ¨é¢„æµ‹ç‰¹å®šè¯å…ƒæ—¶æ‰€å…³æ³¨çš„è¯å…ƒæ•°é‡ã€‚å¯¹äºè‡ªå›å½’ç”Ÿæˆç®—æ³•ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯å…ƒæ—¶éœ€è¦ç”¨åˆ°æ‰€æœ‰å…ˆå‰çš„è¯å…ƒã€‚ /// Since the response length is difference between APAR models and original models, we flatten the paragraph tree generated by APAR models as the reference output. When calculating average, we exclude categories that are not accelerated by APAR, i.e. Coding, Extraction and Math. // ç”±äº APAR æ¨¡å‹ä¸åŸå§‹æ¨¡å‹çš„å“åº”é•¿åº¦å­˜åœ¨å·®å¼‚ï¼Œæˆ‘ä»¬å°† APAR æ¨¡å‹ç”Ÿæˆçš„æ®µè½æ ‘å±•å¹³ä½œä¸ºå‚è€ƒè¾“å‡ºã€‚è®¡ç®—å¹³å‡å€¼æ—¶ï¼Œæˆ‘ä»¬æ’é™¤äº†æœªè¢« APAR åŠ é€Ÿçš„ç±»åˆ«ï¼Œå³ç¼–ç ã€æå–å’Œæ•°å­¦è¿ç®—ã€‚"
[docs/.site]: https://docs.z.ai "Tired of limits? GLM Coding Plan â€” monthly access to world-class model GLM-4.7, compatible with top coding tools like Claude Code and Cline. All from just $3/month. Try it now â†’ // åŒå€¦äº†å„ç§é™åˆ¶ï¼ŸGLM ç¼–ç è®¡åˆ’â€”â€”æ¯æœˆåªéœ€ 3 ç¾å…ƒï¼Œå³å¯ä½¿ç”¨ä¸–ç•Œä¸€æµçš„ GLM-4.7 æ¨¡å‹ï¼Œå…¼å®¹ Claude Code å’Œ Cline ç­‰é¡¶çº§ç¼–ç å·¥å…·ã€‚ ç«‹å³ä½“éªŒ â†’"
[buy/.site]: https://z.ai/subscribe "GLM Coding Plan // GLM ç¼–ç è®¡åˆ’ /// - Powered by Top-Tier Coding Model // é‡‡ç”¨é¡¶çº§ç¼–ç æ¨¡å‹ ///: GLM-4.7 is the latest open-source SOTA model for advanced reasoning, coding, and agentic tasks. // GLM-4.7 æ˜¯æœ€æ–°çš„å¼€æº SOTA æ¨¡å‹ï¼Œé€‚ç”¨äºé«˜çº§æ¨ç†ã€ç¼–ç å’Œæ™ºèƒ½ä½“ä»»åŠ¡ã€‚ /// - Works Seamlessly Across Your Dev Stack // ä¸æ‚¨çš„å¼€å‘å †æ ˆæ— ç¼åä½œ ///: Compatible with 10+ popular AI coding tools such as Claude Code, Cursor, and Cline, offering greater flexibility. // å…¼å®¹ 10 å¤šç§æµè¡Œçš„ AI ç¼–ç å·¥å…·ï¼Œä¾‹å¦‚ Claude Codeã€Cursor å’Œ Clineï¼Œæä¾›æ›´å¤§çš„çµæ´»æ€§ã€‚ /// - High Quotas, Developer-Friendly Pricing // é«˜é…é¢ï¼Œå¼€å‘å•†å‹å¥½å®šä»· ///: Starting from only $3/month, enjoy up to 3Ã— the usage of a comparable Claude plan. // æ¯æœˆåªéœ€â€‹â€‹ 3 ç¾å…ƒèµ·ï¼Œå³å¯äº«å—ç›¸å½“äºåŒç­‰ Claude å¥—é¤ 3 å€çš„ä½¿ç”¨é‡ã€‚ /// - Free MCP Tools for Enhanced Capabilities // å…è´¹ MCP å·¥å…·ï¼Œå¢å¼ºåŠŸèƒ½ ///: All plans include Vision Analysis, Web Search, Web Reader and Zread MCP, with more capabilities being added. // æ‰€æœ‰å¥—é¤å‡åŒ…å«è§†è§‰åˆ†æã€ç½‘ç»œæœç´¢ã€ç½‘ç»œé˜…è¯»å™¨å’Œ Zread MCPï¼Œå¹¶ä¸”è¿˜ä¼šå¢åŠ æ›´å¤šåŠŸèƒ½ã€‚"
[capa:thinking-mode/.docs]: https://docs.z.ai/guides/capabilities/thinking-mode "Thinking Mode // æ€è€ƒæ¨¡å¼ /// GLM-4.7 offers multiple thinking modes for different scenarios. The sections below explain how to enable each mode, key considerations, and example usage. // GLM-4.7 æä¾›å¤šç§æ€ç»´æ¨¡å¼ï¼Œä»¥åº”å¯¹ä¸åŒçš„åº”ç”¨åœºæ™¯ã€‚ä»¥ä¸‹å„èŠ‚å°†ä»‹ç»å¦‚ä½•å¯ç”¨æ¯ç§æ¨¡å¼ã€å…³é”®æ³¨æ„äº‹é¡¹ä»¥åŠä½¿ç”¨ç¤ºä¾‹ã€‚ /// ## Default Thinking Behaviour // é»˜è®¤æ€è€ƒè¡Œä¸º /// Thinking is activated by default in GLM-4.7, different from the default hybrid thinking in GLM-4.6. // GLM-4.7 é»˜è®¤å¯ç”¨æ€ç»´æ¨¡å¼ï¼Œè¿™ä¸ GLM-4.6 é»˜è®¤å¯ç”¨çš„æ··åˆæ€ç»´æ¨¡å¼ä¸åŒã€‚ /// If you want to disable thinking, use: // å¦‚æœä½ æƒ³æŠ‘åˆ¶æ€è€ƒï¼Œè¯·ä½¿ç”¨ï¼š /// (: 'thinking': { 'type': 'disabled' }) /// ## Interleaved thinking // äº¤é”™å¼æ€è€ƒ /// We support interleaved thinking by default (supported since GLM-4.5), allowing GLM to think between tool calls and after receiving tool results. This enables more complex, step-by-step reasoning: interpreting each tool output before deciding what to do next, chaining multiple tool calls with reasoning steps, and making finer-grained decisions based on intermediate results. // æˆ‘ä»¬é»˜è®¤æ”¯æŒäº¤é”™å¼æ¨ç† ï¼ˆè‡ª GLM-4.5 ç‰ˆæœ¬èµ·æ”¯æŒï¼‰ï¼Œå…è®¸ GLM åœ¨å·¥å…·è°ƒç”¨ä¹‹é—´ä»¥åŠæ¥æ”¶åˆ°å·¥å…·ç»“æœåè¿›è¡Œæ¨ç†ã€‚è¿™ä½¿å¾— GLM èƒ½å¤Ÿè¿›è¡Œæ›´å¤æ‚ã€å¾ªåºæ¸è¿›çš„æ¨ç†ï¼šåœ¨å†³å®šä¸‹ä¸€æ­¥æ“ä½œä¹‹å‰è§£é‡Šæ¯ä¸ªå·¥å…·çš„è¾“å‡ºï¼Œå°†å¤šä¸ªå·¥å…·è°ƒç”¨ä¸æ¨ç†æ­¥éª¤ä¸²è”èµ·æ¥ï¼Œå¹¶åŸºäºä¸­é—´ç»“æœåšå‡ºæ›´ç²¾ç»†çš„å†³ç­–ã€‚ /// Tips: When using interleaved thinking with tools, thinking blocks should be explicitly preserved and returned together with the tool results. // è´´å£«: å½“ä½¿ç”¨å·¥å…·è¿›è¡Œäº¤é”™æ€è€ƒæ—¶ï¼Œ åº”è¯¥æ˜ç¡®åœ°ä¿ç•™æ€è€ƒå—ï¼Œå¹¶å°†å…¶ä¸å·¥å…·ç»“æœä¸€èµ·è¿”å›ã€‚ /// ## Preserved thinking // ä¿ç•™å¼æ€è€ƒ /// GLM-4.7 introduces a new capability in coding scenarios: the model can retain reasoning content from previous assistant turns in the context. This helps preserve reasoning continuity and conversation integrity, improves model performance, and increases cache hit ratesâ€”saving tokens in real tasks. // GLM-4.7 åœ¨ç¼–ç åœºæ™¯ä¸­å¼•å…¥äº†ä¸€é¡¹æ–°åŠŸèƒ½ ï¼šæ¨¡å‹å¯ä»¥ä¿ç•™ä¸Šä¸‹æ–‡ä¸­å…ˆå‰åŠ©æ‰‹å›åˆçš„æ¨ç†å†…å®¹ ã€‚è¿™æœ‰åŠ©äºä¿æŒæ¨ç†çš„è¿ç»­æ€§å’Œå¯¹è¯çš„å®Œæ•´æ€§ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ï¼Œå¹¶å¢åŠ ç¼“å­˜å‘½ä¸­ç‡â€”â€”ä»è€Œåœ¨å®é™…ä»»åŠ¡ä¸­èŠ‚çœä»¤ç‰Œã€‚ /// ## Turn-level Thinking // å›åˆçº§æ€è€ƒ /// â€œTurn-level Thinkingâ€ is a capability that lets you control reasoning computation on a per-turn basis: within the same session, each request can independently choose to enable or disable thinking. This is a new capability introduced in GLM-4.7, with the following advantages: // â€œå›åˆçº§æ€è€ƒâ€æ˜¯ä¸€é¡¹å…è®¸æ‚¨é€å›åˆæ§åˆ¶æ¨ç†è®¡ç®—çš„åŠŸèƒ½ï¼šåœ¨åŒä¸€ä¼šè¯ä¸­ï¼Œæ¯ä¸ªè¯·æ±‚éƒ½å¯ä»¥ç‹¬ç«‹é€‰æ‹©å¯ç”¨æˆ–ç¦ç”¨æ€è€ƒåŠŸèƒ½ã€‚è¿™æ˜¯ GLM-4.7 ä¸­å¼•å…¥çš„ä¸€é¡¹æ–°åŠŸèƒ½ï¼Œå…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š /// - More flexible cost/latency control: For lightweight turns like â€œasking a factâ€ or â€œtweaking wording,â€ you can disable thinking to get faster responses; for heavier tasks like â€œcomplex planning,â€ â€œmulti-constraint reasoning,â€ or â€œcode debugging,â€ you can enable thinking to improve accuracy and stability. // æ›´çµæ´»çš„æˆæœ¬/å»¶è¿Ÿæ§åˆ¶ï¼š å¯¹äºâ€œè¯¢é—®äº‹å®â€æˆ–â€œè°ƒæ•´æªè¾â€ç­‰è½»é‡çº§ä»»åŠ¡ï¼Œæ‚¨å¯ä»¥ç¦ç”¨æ€è€ƒåŠŸèƒ½ä»¥è·å¾—æ›´å¿«çš„å“åº”ï¼›å¯¹äºâ€œå¤æ‚è§„åˆ’â€ã€â€œå¤šçº¦æŸæ¨ç†â€æˆ–â€œä»£ç è°ƒè¯•â€ç­‰è¾ƒé‡çš„ä»»åŠ¡ï¼Œæ‚¨å¯ä»¥å¯ç”¨æ€è€ƒåŠŸèƒ½ä»¥æé«˜å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚ /// - Smoother multi-turn experience: The thinking switch can be toggled at any point within a session. The model stays coherent across turns and keeps a consistent output style, making it feel â€œsmarter when things are hard, faster when things are simple.â€ // æ›´æµç•…çš„å¤šè½®è¿ç®—ä½“éªŒï¼š æ€ç»´æ¨¡å¼å¯ä»¥åœ¨è¿ç®—è¿‡ç¨‹ä¸­çš„ä»»ä½•æ—¶åˆ»åˆ‡æ¢ã€‚æ¨¡å‹åœ¨å„è½®è¿ç®—ä¸­ä¿æŒè¿è´¯æ€§ï¼Œå¹¶ä¿æŒä¸€è‡´çš„è¾“å‡ºé£æ ¼ï¼Œä½¿å…¶åœ¨éš¾é¢˜ä¸Šè¡¨ç°å¾—æ›´æ™ºèƒ½ï¼Œåœ¨ç®€å•é—®é¢˜ä¸Šè¡¨ç°å¾—æ›´å¿«ã€‚ /// - Better for agent/tool-use scenarios: On turns that require quick tool execution, you can reduce reasoning overhead; on turns that require making decisions based on tool results, you can turn on deeper thinkingâ€”dynamically balancing efficiency and quality. // æ›´é€‚åˆæ™ºèƒ½ä½“ä»£ç†/å·¥å…·ä½¿ç”¨åœºæ™¯ï¼š åœ¨éœ€è¦å¿«é€Ÿæ‰§è¡Œå·¥å…·çš„å›åˆä¸­ï¼Œå¯ä»¥å‡å°‘æ¨ç†å¼€é”€ï¼›åœ¨éœ€è¦æ ¹æ®å·¥å…·ç»“æœåšå‡ºå†³ç­–çš„å›åˆä¸­ï¼Œå¯ä»¥å¼€å¯æ›´æ·±å±‚æ¬¡çš„æ€è€ƒâ€”â€”åŠ¨æ€åœ°å¹³è¡¡æ•ˆç‡å’Œè´¨é‡ã€‚"
[capa:thinking-mode/.docs-cn]: https://docs.bigmodel.cn/cn/guide/capabilities/thinking-mode "æ€è€ƒæ¨¡å¼ /// GLM-4.7 æä¾›å¤šç§æ€è€ƒæ¨¡å¼ï¼Œè¦†ç›–ä»å¸¸è§„å¯¹è¯åˆ°å·¥å…·è°ƒç”¨ä¸ç¼–ç æ™ºèƒ½ä½“çš„ä¸åŒéœ€æ±‚ã€‚ä¸‹æ–‡å°†åˆ†åˆ«è¯´æ˜å„æ¨¡å¼çš„å¯ç”¨æ–¹å¼ã€å…³é”®æ³¨æ„äº‹é¡¹ä¸ç¤ºä¾‹ç”¨æ³•ã€‚ /// ## é»˜è®¤æ€è€ƒè¡Œä¸º /// GLM-4.7 é»˜è®¤å¼€å¯ Thinkingï¼Œè¿™ä¸€ç‚¹ä¸åŒäº GLM-4.6 çš„é»˜è®¤â€œæ··åˆ thinkingï¼ˆè‡ªåŠ¨å¼€å¯ï¼‰â€ã€‚ /// å¦‚æœä½ æƒ³å…³é—­ thinkingï¼Œè¯·ä½¿ç”¨ï¼š /// (: 'thinking': { 'type': 'disabled' }) /// ## äº¤é”™å¼æ€è€ƒï¼ˆInterleaved thinkingï¼‰ /// æˆ‘ä»¬é»˜è®¤æ”¯æŒäº¤é”™å¼æ€è€ƒï¼ˆè¿™ä¸€ç‚¹ä» GLM 4.5 å¼€å§‹å°±æ”¯æŒï¼‰ï¼Œä½¿ GLM å¯ä»¥åœ¨å·¥å…·è°ƒç”¨ä¹‹é—´ã€ä»¥åŠæ”¶åˆ°å·¥å…·ç»“æœä¹‹åç»§ç»­æ€è€ƒã€‚è¿™è®©æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œæ›´å¤æ‚çš„åˆ†æ­¥æ¨ç†ï¼šåœ¨å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨å‰å…ˆè§£è¯»æ¯æ¬¡å·¥å…·è¾“å‡ºï¼ŒæŠŠå¤šæ¬¡å·¥å…·è°ƒç”¨ä¸æ¨ç†æ­¥éª¤ä¸²è”èµ·æ¥ï¼Œå¹¶æ ¹æ®ä¸­é—´ç»“æœåšæ›´ç»†ç²’åº¦çš„å†³ç­–ã€‚ /// æ³¨æ„ï¼šå½“ä½ åœ¨ä½¿ç”¨â€œäº¤é”™æ€è€ƒ + å·¥å…·â€æ—¶ï¼Œå¿…é¡»æ˜¾å¼ä¿ç•™ Reasoning contentï¼Œå¹¶åœ¨è¿”å›å·¥å…·ç»“æœæ—¶ä¸€å¹¶è¿”å› /// ## ä¿ç•™å¼æ€è€ƒï¼ˆPreserved thinkingï¼‰ /// GLM-4.7 åœ¨ç¼–ç åœºæ™¯ä¸­å¼•å…¥äº†ä¸€é¡¹æ–°èƒ½åŠ›ï¼šæ¨¡å‹å¯ä»¥åœ¨ä¸Šä¸‹æ–‡ä¸­ä¿ç•™æ¥è‡ªå…ˆå‰ assistant å›åˆçš„ reasoning contentã€‚è¿™æœ‰åŠ©äºä¿æŒæ¨ç†è¿ç»­æ€§ä¸å¯¹è¯å®Œæ•´æ€§ã€æå‡æ¨¡å‹è¡¨ç°ï¼Œå¹¶æé«˜ç¼“å­˜å‘½ä¸­ç‡ï¼Œåœ¨çœŸå®ä»»åŠ¡ä¸­èŠ‚çœæ›´å¤š tokensã€‚ /// è¯¥èƒ½åŠ›åœ¨ Coding Plan ç«¯ç‚¹é»˜è®¤å¼€å¯ã€æ ‡å‡† API ç«¯ç‚¹é»˜è®¤å…³é—­ã€‚å¦‚æœä½ æƒ³åœ¨ä½ çš„äº§å“ä¸­å¼€å¯ä¿ç•™å¼æ€è€ƒï¼ˆè¯¥èƒ½åŠ›ä¸»è¦æ¨è Coding / Agent åœºæ™¯ä½¿ç”¨ï¼‰ï¼Œä½ å¯ä»¥é€šè¿‡ã€Œâ€œclear_thinkingâ€: Falseã€åœ¨ API ç«¯ç‚¹ä¸­å¼€å¯ï¼Œå¹¶éœ€è¦å°†å®Œæ•´ã€æœªä¿®æ”¹çš„ reasoning content ä¼ å› APIã€‚æ‰€æœ‰è¿ç»­çš„ reasoning content å¿…é¡»ä¸æ¨¡å‹åœ¨åŸå§‹è¯·æ±‚æœŸé—´ç”Ÿæˆçš„åºåˆ—å®Œå…¨ä¸€è‡´ï¼Œä¸è¦é‡æ–°æ’åºæˆ–ä¿®æ”¹è¿™äº› contentï¼Œå¦åˆ™ä¼šé™ä½æ•ˆæœå¹¶å½±å“ç¼“å­˜å‘½ä¸­ã€‚ /// ## è½®çº§æ€è€ƒ /// ã€Œè½®çº§æ€è€ƒï¼ˆTurn-level Thinkingï¼‰ã€æ˜¯ä¸€ç§æŒ‰è½®æ§åˆ¶æ¨ç†è®¡ç®—çš„èƒ½åŠ›ï¼šåœ¨åŒä¸€è°ƒç”¨ä¼šè¯ä¸­ï¼Œæ¯ä¸€è½®è¯·æ±‚éƒ½å¯ä»¥ç‹¬ç«‹é€‰æ‹©å¼€å¯/å…³é—­æ€è€ƒã€‚è¿™æ˜¯ GLM-4.7 æ–°å¼•å…¥çš„èƒ½åŠ›ï¼Œå…·å¤‡ä»¥ä¸‹ä¼˜åŠ¿ï¼š /// - æ›´çµæ´»çš„æˆæœ¬/æ—¶å»¶æ§åˆ¶ï¼šå¯¹â€œé—®ä¸ªäº‹å®/æ”¹ä¸ªæªè¾â€ç­‰è½»é‡è½®æ¬¡å¯å…³é—­æ€è€ƒï¼Œè¿½æ±‚å¿«é€Ÿå“åº”ï¼›å¯¹â€œå¤æ‚è§„åˆ’/å¤šçº¦æŸæ¨ç†/ä»£ç è°ƒè¯•â€ç­‰é‡ä»»åŠ¡è½®æ¬¡å¯å¼€å¯æ€è€ƒï¼Œæå‡æ­£ç¡®ç‡ä¸ç¨³å®šæ€§ã€‚ /// - æ›´é¡ºæ»‘çš„å¤šè½®ä½“éªŒï¼šæ€è€ƒå¼€å…³åœ¨ä¼šè¯å†…å¯éšæ—¶åˆ‡æ¢ï¼Œæ¨¡å‹èƒ½åœ¨ä¸åŒè½®æ¬¡é—´ä¿æŒå¯¹è¯è¿è´¯ä¸è¾“å‡ºé£æ ¼ä¸€è‡´ï¼Œè®©ç”¨æˆ·æ„Ÿè§‰â€œèªæ˜æ—¶æ›´èªæ˜ã€ç®€å•æ—¶æ›´å¿«â€ã€‚ /// - æ›´é€‚åˆ Agent / å·¥å…·è°ƒç”¨åœºæ™¯ï¼šåœ¨éœ€è¦å¿«é€Ÿæ‰§è¡Œçš„å·¥å…·è½®æ¬¡å¯é™ä½æ¨ç†å¼€é”€ï¼Œåœ¨éœ€è¦ç»¼åˆå·¥å…·ç»“æœåšå†³ç­–çš„è½®æ¬¡å†å¼€å¯æ·±åº¦æ€è€ƒï¼Œå®ç°æ•ˆç‡ä¸è´¨é‡çš„åŠ¨æ€å¹³è¡¡ã€‚"
[GLM-4.5.src/gh]: https://github.com/zai-org/GLM-4.5.git "(Apache-2.0) (Languages: Python 100%) GLM-4.7 & GLM-4.6 & GLM-4.5 // GLM-4.7ã€GLM-4.6 å’Œ GLM-4.5 /// ## Model Introduction // æ¨¡å‹ä»‹ç» /// ### GLM-4.7 /// GLM-4.7, your new coding partner, is coming with the following features: // GLM-4.7 ï¼Œæ‚¨çš„å…¨æ–°ç¼–ç ä¼™ä¼´ï¼Œå…·å¤‡ä»¥ä¸‹åŠŸèƒ½ï¼š /// - Core Coding: GLM-4.7 brings clear gains, compared to its predecessor GLM-4.6, in multilingual agentic coding and terminal-based tasks, including (73.8%, +5.8%) on SWE-bench, (66.7%, +12.9%) on SWE-bench Multilingual, and (41%, +16.5%) on Terminal Bench 2.0. GLM-4.7 also supports thinking before acting, with significant improvements on complex tasks in mainstream agent frameworks such as Claude Code, Kilo Code, Cline, and Roo Code. // æ ¸å¿ƒç¼–ç  ï¼šä¸å‰ä»£ GLM-4.6 ç›¸æ¯”ï¼ŒGLM-4.7 åœ¨å¤šè¯­è¨€æ™ºèƒ½ä½“ç¼–ç å’ŒåŸºäºç»ˆç«¯çš„ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—æå‡ï¼ŒåŒ…æ‹¬åœ¨ SWE-bench æµ‹è¯•ä¸­æå‡ 73.8%ï¼ˆ+5.8%ï¼‰ï¼Œåœ¨ SWE-bench Multilingual æµ‹è¯•ä¸­æå‡ 66.7%ï¼ˆ+12.9%ï¼‰ï¼Œä»¥åŠåœ¨ Terminal Bench 2.0 æµ‹è¯•ä¸­æå‡ 41%ï¼ˆ+16.5%ï¼‰ã€‚GLM-4.7 è¿˜æ”¯æŒå…ˆæ€è€ƒåè¡ŒåŠ¨ï¼Œåœ¨ä¸»æµæ™ºèƒ½ä½“æ¡†æ¶ï¼ˆä¾‹å¦‚ Claude Codeã€Kilo Codeã€Cline å’Œ Roo Codeï¼‰çš„å¤æ‚ä»»åŠ¡ä¸Šå‡æœ‰æ˜¾è‘—æ”¹è¿›ã€‚ /// - Vibe Coding: GLM-4.7 takes a big step forward in improving UI quality. It produces cleaner, more modern webpages and generates better-looking slides with more accurate layout and sizing. // Vibe Coding ï¼šGLM-4.7 åœ¨æå‡ç”¨æˆ·ç•Œé¢è´¨é‡æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚å®ƒèƒ½å¤Ÿç”Ÿæˆæ›´ç®€æ´ã€æ›´ç°ä»£çš„ç½‘é¡µï¼Œå¹¶åˆ›å»ºå¸ƒå±€å’Œå°ºå¯¸æ›´ç²¾ç¡®ã€å¤–è§‚æ›´ç²¾ç¾çš„å¹»ç¯ç‰‡ã€‚ /// - Tool Using: GLM-4.7 achieves significantly improvements in Tool using. Significant better performances can be seen on benchmarks such as Ï„^2-Bench and on web browsing via BrowseComp. // å·¥å…·ä½¿ç”¨æ–¹é¢ ï¼šGLM-4.7 åœ¨å·¥å…·ä½¿ç”¨æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚åœ¨ Ï„^2-Bench ç­‰åŸºå‡†æµ‹è¯•ä»¥åŠé€šè¿‡ BrowseComp è¿›è¡Œçš„ç½‘é¡µæµè§ˆæµ‹è¯•ä¸­ï¼Œæ€§èƒ½å‡æœ‰æ˜¾è‘—æå‡ã€‚ /// - Complex Reasoning: GLM-4.7 delivers a substantial boost in mathematical and reasoning capabilities, achieving (42.8%, +12.4%) on the HLE (Humanityâ€™s Last Exam) benchmark compared to GLM-4.6. // å¤æ‚æ¨ç† ï¼šGLM-4.7 åœ¨æ•°å­¦å’Œæ¨ç†èƒ½åŠ›æ–¹é¢æœ‰äº†æ˜¾è‘—æå‡ï¼Œåœ¨ HLEï¼ˆäººç±»æœ€åçš„è€ƒè¯•ï¼‰åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ï¼ˆ42.8%ï¼Œ+12.4%ï¼‰çš„æˆç»©ï¼Œä¸ GLM-4.6 ç›¸æ¯”ã€‚ /// More general, one would also witness significant improvements in many other scenarios such as chat, creative writing, and role-play scenario. // æ›´æ™®éåœ°è¯´ï¼Œåœ¨èŠå¤©ã€åˆ›æ„å†™ä½œå’Œè§’è‰²æ‰®æ¼”ç­‰è®¸å¤šå…¶ä»–åœºæ™¯ä¸­ï¼Œäººä»¬ä¹Ÿä¼šçœ‹åˆ°æ˜¾è‘—çš„æ”¹è¿›ã€‚ /// Interleaved Thinking & Preserved Thinking // äº¤é”™å¼æ€ç»´ä¸ä¿ç•™å¼æ€ç»´ /// GLM-4.7 further enhances Interleaved Thinking (a feature introduced since GLM-4.5) and introduces Preserved Thinking and Turn-level Thinking. By thinking between actions and staying consistent across turns, it makes complex tasks more stable and more controllable: // GLM-4.7 è¿›ä¸€æ­¥å¢å¼ºäº†äº¤é”™æ€ç»´ ï¼ˆGLM-4.5 å¼•å…¥çš„åŠŸèƒ½ï¼‰ï¼Œå¹¶å¼•å…¥äº†ä¿ç•™å¼æ€ç»´å’Œå›åˆçº§æ€ç»´ ã€‚é€šè¿‡åœ¨è¡ŒåŠ¨ä¹‹é—´è¿›è¡Œæ€è€ƒå¹¶åœ¨å›åˆé—´ä¿æŒä¸€è‡´æ€§ï¼Œå®ƒä½¿å¤æ‚ä»»åŠ¡æ›´åŠ ç¨³å®šå’Œå¯æ§ï¼š /// - Interleaved Thinking: The model thinks before every response and tool calling, improving instruction following and the quality of generation. // äº¤é”™å¼æ€ç»´ ï¼šè¯¥æ¨¡å‹åœ¨æ¯æ¬¡å“åº”å’Œå·¥å…·è°ƒç”¨ä¹‹å‰éƒ½ä¼šè¿›è¡Œæ€è€ƒï¼Œä»è€Œæé«˜æŒ‡ä»¤éµå¾ªæ€§å’Œç”Ÿæˆè´¨é‡ã€‚ /// - Preserved Thinking: In coding agent scenarios, the model automatically retains all thinking blocks across multi-turn conversations, reusing the existing reasoning instead of re-deriving from scratch. This reduces information loss and inconsistencies, and is well-suited for long-horizon, complex tasks. // ä¿ç•™å¼æ€ç»´ ï¼šåœ¨ç¼–ç æ™ºèƒ½ä½“ä»£ç†åœºæ™¯ä¸­ï¼Œè¯¥æ¨¡å‹ä¼šè‡ªåŠ¨ä¿ç•™å¤šè½®å¯¹è¯ä¸­çš„æ‰€æœ‰æ€ç»´æ¨¡å—ï¼Œé‡ç”¨ç°æœ‰æ¨ç†è¿‡ç¨‹ï¼Œè€Œä¸æ˜¯ä»å¤´å¼€å§‹é‡æ–°æ¨å¯¼ã€‚è¿™å‡å°‘äº†ä¿¡æ¯ä¸¢å¤±å’Œä¸ä¸€è‡´ï¼Œéå¸¸é€‚åˆé•¿æœŸã€å¤æ‚çš„ä»»åŠ¡ã€‚ /// - Turn-level Thinking: The model supports per-turn control over reasoning within a sessionâ€”disable thinking for lightweight requests to reduce latency/cost, enable it for complex tasks to improve accuracy and stability. // å›åˆçº§æ€è€ƒ ï¼šè¯¥æ¨¡å‹æ”¯æŒå¯¹ä¼šè¯ä¸­çš„æ¨ç†è¿›è¡Œé€å›åˆæ§åˆ¶â€”â€”å¯¹äºè½»é‡çº§è¯·æ±‚ç¦ç”¨æ€è€ƒä»¥å‡å°‘å»¶è¿Ÿ/æˆæœ¬ï¼Œå¯¹äºå¤æ‚ä»»åŠ¡å¯ç”¨æ€è€ƒä»¥æé«˜å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚ /// ### GLM-4.6 /// Compared with GLM-4.5, GLM-4.6 brings several key improvements: // ä¸ GLM-4.5 ç›¸æ¯”ï¼Œ GLM-4.6 å¸¦æ¥äº†å‡ é¡¹å…³é”®æ”¹è¿›ï¼š /// - Longer context window: The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks. // æ›´é•¿çš„ä¸Šä¸‹æ–‡çª—å£ï¼š ä¸Šä¸‹æ–‡çª—å£å·²ä» 128K ä¸ªä»¤ç‰Œæ‰©å±•åˆ° 200K ä¸ªä»¤ç‰Œï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†æ›´å¤æ‚çš„æ™ºèƒ½ä½“ä»£ç†ä»»åŠ¡ã€‚ /// - Superior coding performance: The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Codeã€Clineã€Roo Code and Kilo Code, including improvements in generating visually polished front-end pages. // å“è¶Šçš„ç¼–ç æ€§èƒ½ï¼š è¯¥æ¨¡å‹åœ¨ä»£ç åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ›´é«˜çš„åˆ†æ•°ï¼Œå¹¶åœ¨ Claude Codeã€Clineã€Roo Code å’Œ Kilo Code ç­‰åº”ç”¨ç¨‹åºä¸­è¡¨ç°å‡ºæ›´å¥½çš„å®é™…æ€§èƒ½ï¼ŒåŒ…æ‹¬æ”¹è¿›ç”Ÿæˆè§†è§‰æ•ˆæœç²¾ç¾çš„å‰ç«¯é¡µé¢ã€‚ /// - Advanced reasoning: GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability. // é«˜çº§æ¨ç†ï¼š GLM-4.6 åœ¨æ¨ç†æ€§èƒ½æ–¹é¢è¡¨ç°å‡ºæ˜æ˜¾çš„æå‡ï¼Œå¹¶æ”¯æŒåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨å·¥å…·ï¼Œä»è€Œå¸¦æ¥æ›´å¼ºçš„æ•´ä½“èƒ½åŠ›ã€‚ /// - More capable agents: GLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks. // æ›´å¼ºå¤§çš„æ™ºèƒ½ä½“ä»£ç†ï¼š GLM-4.6 åœ¨å·¥å…·ä½¿ç”¨å’ŒåŸºäºæœç´¢çš„æ™ºèƒ½ä½“ä»£ç†æ–¹é¢è¡¨ç°å‡ºæ›´å¼ºçš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨æ™ºèƒ½ä½“ä»£ç†æ¡†æ¶ä¸­é›†æˆå¾—æ›´æœ‰æ•ˆã€‚ /// - Refined writing: Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios. // æ›´ç²¾ç‚¼çš„å†™ä½œï¼š åœ¨é£æ ¼å’Œå¯è¯»æ€§æ–¹é¢æ›´ç¬¦åˆäººç±»çš„å–œå¥½ï¼Œå¹¶ä¸”åœ¨è§’è‰²æ‰®æ¼”åœºæ™¯ä¸­è¡¨ç°å¾—æ›´è‡ªç„¶ã€‚ /// We evaluated GLM-4.6 across eight public benchmarks covering agents, reasoning, and coding. Results show clear gains over GLM-4.5, with GLM-4.6 also holding competitive advantages over leading domestic and international models such as DeepSeek-V3.1-Terminus and Claude Sonnet 4. // æˆ‘ä»¬ä½¿ç”¨æ¶µç›–æ™ºèƒ½ä½“ã€æ¨ç†å’Œç¼–ç çš„å…«ä¸ªå…¬å¼€åŸºå‡†æµ‹è¯•å¯¹ GLM-4.6 è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒGLM-4.6 ç›¸è¾ƒäº GLM-4.5 æœ‰æ˜¾è‘—æå‡ï¼Œå¹¶ä¸”åœ¨ä¸å›½å†…å¤–é¢†å…ˆæ¨¡å‹ï¼ˆä¾‹å¦‚ DeepSeek-V3.1-Terminus å’Œ Claude Sonnet 4ï¼‰ çš„ç«äº‰ä¸­ä¹Ÿå…·æœ‰ä¼˜åŠ¿ã€‚ /// ### GLM-4.5 /// The GLM-4.5 series models are foundation models designed for intelligent agents. GLM-4.5 has 355 billion total parameters with 32 billion active parameters, while GLM-4.5-Air adopts a more compact design with 106 billion total parameters and 12 billion active parameters. GLM-4.5 models unify reasoning, coding, and intelligent agent capabilities to meet the complex demands of intelligent agent applications. // GLM-4.5 ç³»åˆ—æ¨¡å‹æ˜¯ä¸“ä¸ºæ™ºèƒ½ä½“è®¾è®¡çš„åŸºç¡€æ¨¡å‹ã€‚GLM-4.5 æ‹¥æœ‰ 3550 äº¿ä¸ªå‚æ•°ï¼Œå…¶ä¸­ 320 äº¿ä¸ªä¸ºæœ‰æ•ˆå‚æ•°ï¼›è€Œ GLM-4.5-Air é‡‡ç”¨æ›´ä¸ºç´§å‡‘çš„è®¾è®¡ï¼Œæ‹¥æœ‰ 1060 äº¿ä¸ªå‚æ•°å’Œ 120 äº¿ä¸ªæœ‰æ•ˆå‚æ•°ã€‚GLM-4.5 æ¨¡å‹é›†æ¨ç†ã€ç¼–ç å’Œæ™ºèƒ½ä½“åŠŸèƒ½äºä¸€ä½“ï¼Œèƒ½å¤Ÿæ»¡è¶³æ™ºèƒ½ä½“åº”ç”¨çš„å¤æ‚éœ€æ±‚ã€‚ /// Both GLM-4.5 and GLM-4.5-Air are hybrid reasoning models that provide two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. // GLM-4.5 å’Œ GLM-4.5-Air éƒ½æ˜¯æ··åˆæ¨ç†æ¨¡å‹ï¼Œæä¾›ä¸¤ç§æ¨¡å¼ï¼šç”¨äºå¤æ‚æ¨ç†å’Œå·¥å…·ä½¿ç”¨çš„æ€è€ƒæ¨¡å¼ï¼Œä»¥åŠç”¨äºå³æ—¶ååº”çš„éæ€è€ƒæ¨¡å¼ã€‚ /// We have open-sourced the base models, hybrid reasoning models, and FP8 versions of the hybrid reasoning models for both GLM-4.5 and GLM-4.5-Air. They are released under the MIT open-source license and can be used commercially and for secondary development. // æˆ‘ä»¬å·²å°† GLM-4.5 å’Œ GLM-4.5-Air çš„åŸºç¡€æ¨¡å‹ã€æ··åˆæ¨ç†æ¨¡å‹ä»¥åŠ FP8 ç‰ˆæœ¬çš„æ··åˆæ¨ç†æ¨¡å‹å¼€æºã€‚å®ƒä»¬ä»¥ MIT å¼€æºè®¸å¯è¯å‘å¸ƒï¼Œå¯ç”¨äºå•†ä¸šç”¨é€”å’ŒäºŒæ¬¡å¼€å‘ã€‚ /// As demonstrated in our comprehensive evaluation across 12 industry-standard benchmarks, GLM-4.5 achieves exceptional performance with a score of 63.2, in the 3rd place among all the proprietary and open-source models. Notably, GLM-4.5-Air delivers competitive results at 59.8 while maintaining superior efficiency. // æ­£å¦‚æˆ‘ä»¬åœ¨ 12 é¡¹è¡Œä¸šæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å¾—å‡ºçš„å…¨é¢è¯„ä¼°ç»“æœæ‰€ç¤ºï¼ŒGLM-4.5 è¡¨ç°å“è¶Šï¼Œå¾—åˆ†é«˜è¾¾ 63.2 åˆ†ï¼Œåœ¨æ‰€æœ‰ä¸“æœ‰å’Œå¼€æºæ¨¡å‹ä¸­æ’åç¬¬ä¸‰ ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒGLM-4.5-Air åœ¨ä¿æŒå“è¶Šæ•ˆç‡çš„åŒæ—¶ï¼Œä¹Ÿå–å¾—äº† 59.8 åˆ†çš„ä¼˜å¼‚æˆç»©ã€‚"
[GLM-4.7-model.tensor/hf]: https://huggingface.co/zai-org/GLM-4.7 "(MIT) (Model size: 358B params; Tensor type: BF16Â·F32) (Topics: Text Generation, Transformers, Safetensors, English, Chinese) (Tags: glm4_moe, conversational) (arxiv: 2508.06471)"
[GLM-4.7-models.coll/hf]: https://huggingface.co/collections/zai-org/glm-47 "GLM-4.7 (hf: zai-org/GLM-4.7: Text Generation â€¢ 358B) (hf: zai-org/GLM-4.7-FP8: Text Generation â€¢ 358B)"
[GLM-4.7-model.intro/.site:blog]: https://z.ai/blog/glm-4.7 "GLM-4.7: Advancing the Coding Capability // GLM-4.7ï¼šæå‡ç¼–ç èƒ½åŠ›"
[GLM-4.6-model.intro/.site:blog]: https://z.ai/blog/glm-4.6 "GLM-4.6: Advanced Agentic, Reasoning and Coding Capabilities // GLM-4.6ï¼šé«˜çº§æ™ºèƒ½ä½“ã€æ¨ç†å’Œç¼–ç èƒ½åŠ›"
[GLM-4.5-model.intro/.site:blog]: https://z.ai/blog/glm-4.5 "GLM-4.5: Reasoning, Coding, and Agentic Abililties // GLM-4.5ï¼šæ¨ç†ã€ç¼–ç å’Œèƒ½åŠ¨èƒ½åŠ›"
[GLM-4.5.paper/arxiv]: https://arxiv.org/abs/2508.06471 "[Submitted on 8 Aug 2025] { Computation and Language (cs.CL) } (doi: 10.48550/arXiv.2508.06471) GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models // GLM-4.5ï¼šæ™ºèƒ½ä½“ã€æ¨ç†å’Œç¼–ç ï¼ˆARCï¼‰åŸºç¡€æ¨¡å‹ /// We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at this https URL (. gh: zai-org/GLM-4.5.git). // æˆ‘ä»¬æå‡ºäº† GLM-4.5ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ€»å‚æ•°é‡ä¸º 3550 äº¿ï¼Œæ¿€æ´»å‚æ•°é‡ä¸º 320 äº¿ã€‚è¯¥æ¨¡å‹é‡‡ç”¨æ··åˆæ¨ç†æ–¹æ³•ï¼ŒåŒæ—¶æ”¯æŒæ€è€ƒæ¨¡å¼å’Œç›´æ¥å“åº”æ¨¡å¼ã€‚é€šè¿‡åœ¨ 23T ä¸ªè¯å…ƒä¸Šè¿›è¡Œå¤šé˜¶æ®µè®­ç»ƒï¼Œå¹¶ç»“åˆä¸“å®¶æ¨¡å‹è¿­ä»£å’Œå¼ºåŒ–å­¦ä¹ è¿›è¡Œå…¨é¢çš„åè®­ç»ƒï¼ŒGLM-4.5 åœ¨æ™ºèƒ½ä½“ã€æ¨ç†å’Œç¼–ç ï¼ˆARCï¼‰ä»»åŠ¡ä¸­å‡å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œåœ¨ TAU-Benchã€AIME 24 å’Œ SWE-bench Verified æµ‹è¯•ä¸­åˆ†åˆ«è·å¾—äº† 70.1%ã€91.0% å’Œ 64.2% çš„åˆ†æ•°ã€‚ä¸ä¸€äº›ç«äº‰å¯¹æ‰‹ç›¸æ¯”ï¼ŒGLM-4.5 çš„å‚æ•°é‡è¦å°‘å¾—å¤šï¼Œåœ¨æ‰€æœ‰è¯„ä¼°æ¨¡å‹ä¸­æ’åç¬¬ä¸‰ï¼Œåœ¨æ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•ä¸­æ’åç¬¬äºŒã€‚æˆ‘ä»¬å‘å¸ƒäº† GLM-4.5ï¼ˆ3550 äº¿å‚æ•°ï¼‰åŠå…¶ç²¾ç®€ç‰ˆæœ¬ GLM-4.5-Airï¼ˆ1060 äº¿å‚æ•°ï¼‰ï¼Œä»¥æ¨åŠ¨æ¨ç†å’Œæ™ºèƒ½ä½“äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ç ”ç©¶ã€‚ä»£ç ã€æ¨¡å‹å’Œæ›´å¤šä¿¡æ¯å¯åœ¨æ­¤ https URL è·å–ã€‚"
[GLM-4.7-model.intro/.docs]: https://docs.z.ai/guides/llm/glm-4.7 "GLM-4.7 is Z.AIâ€™s latest flagship model, featuring upgrades in two key areas: enhanced programming capabilities and more stable multi-step reasoning/execution. It demonstrates significant improvements in executing complex agent tasks while delivering more natural conversational experiences and superior front-end aesthetics. // GLM-4.7 æ˜¯ Z.AI çš„æœ€æ–°æ——èˆ°å‹å·ï¼Œåœ¨ä¸¤å¤§å…³é”®é¢†åŸŸè¿›è¡Œäº†å‡çº§ï¼šå¢å¼ºçš„ç¼–ç¨‹èƒ½åŠ›å’Œæ›´ç¨³å®šçš„å¤šæ­¥éª¤æ¨ç†/æ‰§è¡Œã€‚å®ƒåœ¨æ‰§è¡Œå¤æ‚æ™ºèƒ½ä½“ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶æä¾›äº†æ›´è‡ªç„¶çš„å¯¹è¯ä½“éªŒå’Œæ›´ç¾è§‚çš„å‰ç«¯ç•Œé¢ã€‚ /// Properties /// - Input Modalities // è¾“å…¥æ–¹å¼ ///: Text // æ–‡æœ¬ /// - Output Modalitie // è¾“å‡ºæ¨¡å¼ ///: Text // æ–‡æœ¬ /// - Context Length // ä¸Šä¸‹æ–‡é•¿åº¦ ///: 200K // 20ä¸‡ /// - Maximum Output Tokens // æœ€å¤§è¾“å‡ºé•¿åº¦ ///: 128K /// Capability // èƒ½åŠ› /// - Thinking Mode // æ€è€ƒæ¨¡å¼ ///: Offering multiple thinking modes for different scenarios // é’ˆå¯¹ä¸åŒåœºæ™¯æä¾›å¤šç§æ€ç»´æ¨¡å¼ /// - Streaming Output // æµè¾“å‡º ///: Support real-time streaming responses to enhance user interaction experience // æ”¯æŒå®æ—¶æµå¼å“åº”ï¼Œä»¥å¢å¼ºç”¨æˆ·äº¤äº’ä½“éªŒ /// - Function Call // å‡½æ•°è°ƒç”¨ ///: Powerful tool invocation capabilities, enabling integration with various external toolsets // å¼ºå¤§çš„å·¥å…·è°ƒç”¨åŠŸèƒ½ï¼Œå¯ä¸å„ç§å¤–éƒ¨å·¥å…·é›†é›†æˆ /// - Context Caching // ä¸Šä¸‹æ–‡ç¼“å­˜ ///: Intelligent caching mechanism to optimize performance in long conversations // æ™ºèƒ½ç¼“å­˜æœºåˆ¶ï¼Œä¼˜åŒ–é•¿æ—¶é—´å¯¹è¯ä¸­çš„æ€§èƒ½ /// - Structured Output // ç»“æ„åŒ–è¾“å‡º ///: Support for structured output formats like JSON, facilitating system integration // æ”¯æŒ JSON ç­‰ç»“æ„åŒ–è¾“å‡ºæ ¼å¼ï¼Œä¾¿äºç³»ç»Ÿé›†æˆ /// ### Comprehensive Coding Capability Enhancement // å…¨é¢æå‡ç¼–ç èƒ½åŠ› /// GLM-4.7 achieves significant breakthroughs across three dimensions: programming, reasoning, and agent capabilities: // GLM-4.7 åœ¨ä¸‰ä¸ªæ–¹é¢å–å¾—äº†é‡å¤§çªç ´ï¼šç¼–ç¨‹ã€æ¨ç†å’Œæ™ºèƒ½ä»£ç†äººèƒ½åŠ›ï¼š /// - Enhanced Programming Capabilities: Substantially improves model performance in multi-language coding and terminal agent applications; GLM-4.7 now implements a â€œthink before actingâ€ mechanism within programming frameworks like Claude Code, Kilo Code, TRAE, Cline, and Roo Code, delivering more stable performance on complex tasks. // å¢å¼ºçš„ç¼–ç¨‹èƒ½åŠ› ï¼šå¤§å¹…æå‡äº†å¤šè¯­è¨€ç¼–ç å’Œç»ˆç«¯æ™ºèƒ½ä½“åº”ç”¨ç¨‹åºä¸­çš„æ¨¡å‹æ€§èƒ½ï¼›GLM-4.7 ç°åœ¨åœ¨ Claude Codeã€Kilo Codeã€TRAEã€Cline å’Œ Roo Code ç­‰ç¼–ç¨‹æ¡†æ¶ä¸­å®ç°äº†â€œå…ˆæ€è€ƒåè¡ŒåŠ¨â€æœºåˆ¶ï¼Œä»è€Œåœ¨å¤æ‚ä»»åŠ¡ä¸Šæä¾›äº†æ›´ç¨³å®šçš„æ€§èƒ½ã€‚ /// - Enhanced Frontend Aesthetics: GLM-4.7 shows marked progress in frontend generation quality, producing visually superior webpages, PPTs, and posters. // å¢å¼ºå‰ç«¯ç¾å­¦ ï¼šGLM-4.7 åœ¨å‰ç«¯ç”Ÿæˆè´¨é‡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œç”Ÿæˆäº†è§†è§‰æ•ˆæœæ›´ä½³çš„ç½‘é¡µã€PPT å’Œæµ·æŠ¥ã€‚ /// - Enhanced Tool Invocation Capabilities: GLM-4.7 demonstrates improved tool invocation skills, scoring 67 points on the BrowseComp web task evaluation and achieving an open-source SOTA of 84.7 points on the Ï„Â²-Bench interactive tool invocation benchmark, surpassing Claude Sonnet 4.5 // å¢å¼ºçš„å·¥å…·è°ƒç”¨èƒ½åŠ› ï¼šGLMâ€‹â€‹-4.7 å±•ç°å‡ºæ›´å¼ºå¤§çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œåœ¨ BrowseComp ç½‘ç»œä»»åŠ¡è¯„ä¼°ä¸­è·å¾— 67 åˆ†ï¼Œå¹¶åœ¨ Ï„Â²-Bench äº¤äº’å¼å·¥å…·è°ƒç”¨åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº† 84.7 åˆ†çš„å¼€æº SOTA æˆç»©ï¼Œè¶…è¶Šäº† Claude Sonnet 4.5ã€‚ /// - Enhanced reasoning capabilities: Significantly improved mathematical and reasoning skills, achieving 42.8% on the HLE (â€œHuman Last Examâ€) benchmarkâ€”a 41% increase over GLM-4.6 and surpassing GPT-5.1 // æ¨ç†èƒ½åŠ›æ˜¾è‘—æå‡ ï¼šæ•°å­¦å’Œæ¨ç†æŠ€èƒ½å¤§å¹…æé«˜ï¼Œåœ¨ HLEï¼ˆâ€œäººç±»æœ€ç»ˆè€ƒè¯•â€ï¼‰åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ° 42.8%ï¼Œæ¯” GLM-4.6 æé«˜äº† 41%ï¼Œå¹¶è¶…è¶Šäº† GPT-5.1ã€‚ /// - Enhanced General Capabilities: GLM-4.7 delivers more concise, intelligent, and empathetic conversations, with more eloquent and immersive writing and role-playing // å¢å¼ºçš„é€šç”¨èƒ½åŠ› ï¼šGLMâ€‹â€‹-4.7 æä¾›æ›´ç®€æ´ã€æ›´æ™ºèƒ½ã€æ›´å…·åŒç†å¿ƒçš„å¯¹è¯ï¼Œä»¥åŠæ›´ä¼˜ç¾ã€æ›´æ²‰æµ¸å¼çš„å†™ä½œå’Œè§’è‰²æ‰®æ¼”ä½“éªŒã€‚ /// In mainstream benchmark performance, GLM-4.7â€™s coding capabilities align with Claude Sonnet 4.5: Achieved top open-source ranking on SWE-bench-Verified; Reached an open-source SOTA score of 84.9 on LiveCodeBench V6, surpassing Claude Sonnet 4.5; Achieved 73.8% on SWE-bench Verified (a 5.8% improvement over GLM-4.6), 66.7% on SWE-bench Multilingual (a 12.9% improvement), and 41% on Terminal Bench 2.0 (a 16.5% improvement). // åœ¨ä¸»æµåŸºå‡†æµ‹è¯•æ€§èƒ½æ–¹é¢ï¼ŒGLM-4.7 çš„ç¼–ç èƒ½åŠ›ä¸ Claude Sonnet 4.5 ç›¸å½“ï¼šåœ¨ SWE-bench-Verified ä¸Šè·å¾—äº†å¼€æºè½¯ä»¶çš„æœ€é«˜æ’åï¼›åœ¨ LiveCodeBench V6 ä¸Šè¾¾åˆ°äº† 84.9 çš„å¼€æº SOTA åˆ†æ•°ï¼Œè¶…è¿‡äº† Claude Sonnet 4.5ï¼›åœ¨ SWE-bench Verified ä¸Šè·å¾—äº† 73.8% çš„åˆ†æ•°ï¼ˆæ¯” GLM-4.6 æé«˜äº† 5.8%ï¼‰ï¼Œåœ¨ SWE-bench Multilingual ä¸Šè·å¾—äº† 66.7% çš„åˆ†æ•°ï¼ˆæé«˜äº† 12.9%ï¼‰ï¼Œåœ¨ Terminal Bench 2.0 ä¸Šè·å¾—äº† 41% çš„åˆ†æ•°ï¼ˆæé«˜äº† 16.5%ï¼‰ã€‚ /// ### Perceived Improvement in Real Programming Scenarios // åœ¨å®é™…ç¼–ç¨‹åœºæ™¯ä¸­æ„ŸçŸ¥åˆ°çš„æ”¹è¿› /// #### Performance on Real Programming Tasks // å®é™…ç¼–ç¨‹ä»»åŠ¡ä¸Šçš„è¡¨ç° /// In the Claude Code environment, we tested 100 real programming tasks covering core capabilities like frontend, backend, and instruction following. Results show GLM-4.7 demonstrates significant improvements over GLM-4.6 in both stability and deliverability. // åœ¨ Claude Code ç¯å¢ƒä¸‹ï¼Œæˆ‘ä»¬æµ‹è¯•äº† 100 ä¸ªæ¶µç›–å‰ç«¯ã€åç«¯å’ŒæŒ‡ä»¤æ‰§è¡Œç­‰æ ¸å¿ƒåŠŸèƒ½çš„çœŸå®ç¼–ç¨‹ä»»åŠ¡ã€‚ç»“æœè¡¨æ˜ï¼ŒGLM-4.7 åœ¨ç¨³å®šæ€§å’Œå¯äº¤ä»˜æ€§æ–¹é¢å‡æ¯” GLM-4.6 æœ‰æ˜¾è‘—æå‡ã€‚ /// With enhanced programming capabilities, developers can more naturally organize their development workflow around â€œtask delivery,â€ forming an end-to-end closed loop from requirement understanding to implementation. // å‡­å€Ÿå¢å¼ºçš„ç¼–ç¨‹èƒ½åŠ›ï¼Œå¼€å‘äººå‘˜å¯ä»¥æ›´è‡ªç„¶åœ°å›´ç»•â€œä»»åŠ¡äº¤ä»˜â€ç»„ç»‡å¼€å‘å·¥ä½œæµç¨‹ï¼Œå½¢æˆä»éœ€æ±‚ç†è§£åˆ°å®ç°çš„ç«¯åˆ°ç«¯é—­ç¯ã€‚ /// #### Controlled Evolution of Reasoning Capabilities // æ¨ç†èƒ½åŠ›çš„å—æ§æ¼”åŒ– /// GLM-4.7 further enhances the interleaved reasoning capabilities introduced in GLM-4.5 by introducing retained reasoning and round-based reasoning, making complex task execution more stable and controllable. // GLM-4.7 é€šè¿‡å¼•å…¥ä¿ç•™æ¨ç†å’ŒåŸºäºè½®æ¬¡çš„æ¨ç†ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº† GLM-4.5 ä¸­å¼•å…¥çš„äº¤é”™æ¨ç†èƒ½åŠ›ï¼Œä½¿å¤æ‚ä»»åŠ¡çš„æ‰§è¡Œæ›´åŠ ç¨³å®šå’Œå¯æ§ã€‚ /// - Interleaved Reasoning: Performs reasoning before each response/tool invocation, improving compliance with complex instructions and code generation quality. // äº¤é”™æ¨ç†ï¼šåœ¨æ¯æ¬¡å“åº”/å·¥å…·è°ƒç”¨ä¹‹å‰æ‰§è¡Œæ¨ç†ï¼Œä»è€Œæé«˜å¯¹å¤æ‚æŒ‡ä»¤çš„éµå®ˆç¨‹åº¦å’Œä»£ç ç”Ÿæˆè´¨é‡ã€‚ /// - Retention-Based Reasoning: Automatically preserves reasoning blocks across multi-turn dialogues, improving cache hit rates and reducing computational costsâ€”ideal for long-term complex tasks. // åŸºäºä¿ç•™çš„æ¨ç†ï¼šè‡ªåŠ¨åœ¨å¤šè½®å¯¹è¯ä¸­ä¿ç•™æ¨ç†å—ï¼Œæé«˜ç¼“å­˜å‘½ä¸­ç‡å¹¶é™ä½è®¡ç®—æˆæœ¬â€”â€”éå¸¸é€‚åˆé•¿æœŸå¤æ‚ä»»åŠ¡ã€‚ /// - Round-Level Reasoning: Enables round-based control of reasoning overhead within a single sessionâ€”disable reasoning for simple tasks to reduce latency, or enable it for complex tasks to boost accuracy and stability. // å›åˆçº§æ¨ç†ï¼šå…è®¸åœ¨å•ä¸ªä¼šè¯ä¸­æŒ‰å›åˆæ§åˆ¶æ¨ç†å¼€é”€â€”â€”ç¦ç”¨ç®€å•ä»»åŠ¡çš„æ¨ç†ä»¥å‡å°‘å»¶è¿Ÿï¼Œæˆ–å¯ç”¨å¤æ‚ä»»åŠ¡çš„æ¨ç†ä»¥æé«˜å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚ /// #### Comprehensive Task Execution Capabilities // å…¨é¢çš„ä»»åŠ¡æ‰§è¡Œèƒ½åŠ› /// GLM-4.7 demonstrates superior task decomposition and technology stack integration in complex tasks, delivering complete, executable code in a single step while clearly identifying critical dependencies and execution steps, significantly reducing manual debugging costs. // GLM-4.7 åœ¨å¤æ‚ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„ä»»åŠ¡åˆ†è§£å’ŒæŠ€æœ¯æ ˆé›†æˆèƒ½åŠ›ï¼Œä¸€æ­¥å³å¯ç”Ÿæˆå®Œæ•´çš„å¯æ‰§è¡Œä»£ç ï¼Œ åŒæ—¶æ¸…æ™°åœ°è¯†åˆ«å…³é”®ä¾èµ–å…³ç³»å’Œæ‰§è¡Œæ­¥éª¤ï¼Œæ˜¾è‘—é™ä½äº†æ‰‹åŠ¨è°ƒè¯•æˆæœ¬ã€‚ /// Case studies showcase highly interactive mini-games independently developed by GLM-4.7, such as Plants vs. Zombies and Fruit Ninja. // æ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº† GLM-4.7 ç‹¬ç«‹å¼€å‘çš„é«˜åº¦äº’åŠ¨çš„å°æ¸¸æˆï¼Œä¾‹å¦‚ã€Šæ¤ç‰©å¤§æˆ˜åƒµå°¸ã€‹å’Œã€Šæ°´æœå¿è€…ã€‹ã€‚ /// #### Frontend Aesthetic Enhancement // å‰ç«¯ç¾å­¦å¢å¼º /// GLM-4.7 enhances its comprehension of visual code. In frontend design, it better interprets UI design specifications, offering more aesthetically pleasing default solutions for layout structures, color harmony, and component styling. This reduces the time developers spend on style â€œfine-tuning.â€ // GLM-4.7 å¢å¼ºäº†å¯¹è§†è§‰ä»£ç çš„ç†è§£èƒ½åŠ›ã€‚åœ¨å‰ç«¯è®¾è®¡ä¸­ï¼Œå®ƒèƒ½æ›´å¥½åœ°è§£è¯» UI è®¾è®¡è§„èŒƒï¼Œä¸ºå¸ƒå±€ç»“æ„ã€è‰²å½©æ­é…å’Œç»„ä»¶æ ·å¼æä¾›æ›´ç¾è§‚çš„é»˜è®¤è§£å†³æ–¹æ¡ˆã€‚è¿™å‡å°‘äº†å¼€å‘äººå‘˜åœ¨æ ·å¼â€œå¾®è°ƒâ€ä¸ŠèŠ±è´¹çš„æ—¶é—´ã€‚ /// GLM-4.7 delivers significant upgrades in layout and aesthetics for office creation. PPT 16:9 compatibility soars from 52% to 91%, with generated results being essentially â€œready to use.â€ Poster design now features more flexible typography and color schemes, exuding a stronger sense of design. // GLM-4.7 åœ¨åŠå…¬æ–‡æ¡£åˆ›å»ºçš„å¸ƒå±€å’Œç¾è§‚æ€§æ–¹é¢è¿›è¡Œäº†æ˜¾è‘—å‡çº§ã€‚PPT 16:9 å…¼å®¹æ€§ä» 52% å¤§å¹…æå‡è‡³ 91%ï¼Œç”Ÿæˆçš„ç»“æœå‡ ä¹å¯ä»¥â€œå³åˆ»ä½¿ç”¨â€ã€‚æµ·æŠ¥è®¾è®¡ç°åœ¨æ‹¥æœ‰æ›´çµæ´»çš„å­—ä½“å’Œé…è‰²æ–¹æ¡ˆï¼Œå±•ç°å‡ºæ›´å¼ºçš„è®¾è®¡æ„Ÿã€‚"
[GLM-4.6-model.intro/.docs]: https://docs.z.ai/guides/llm/glm-4.6 "GLM-4.6 achieves comprehensive enhancements across multiple domains, including real-world coding, long-context processing, reasoning, searching, writing, and agentic applications. Details are as follows: // GLM-4.6 åœ¨å¤šä¸ªé¢†åŸŸå®ç°äº†å…¨é¢å¢å¼ºï¼ŒåŒ…æ‹¬å®é™…ç¼–ç ã€é•¿ä¸Šä¸‹æ–‡å¤„ç†ã€æ¨ç†ã€æœç´¢ã€å†™ä½œå’Œæ™ºèƒ½ä½“åº”ç”¨ã€‚è¯¦æƒ…å¦‚ä¸‹ï¼š /// - Longer context window: The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks. // æ›´é•¿çš„ä¸Šä¸‹æ–‡çª—å£ ï¼šä¸Šä¸‹æ–‡çª—å£å·²ä» 128K ä¸ªä»¤ç‰Œæ‰©å±•åˆ° 200K ä¸ªä»¤ç‰Œï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†æ›´å¤æ‚çš„æ™ºèƒ½ä½“ä»»åŠ¡ã€‚ /// - Superior coding performance: The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Codeã€Clineã€Roo Code and Kilo Code, including improvements in generating visually polished front-end pages. // å“è¶Šçš„ç¼–ç æ€§èƒ½ ï¼šè¯¥æ¨¡å‹åœ¨ä»£ç åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ›´é«˜çš„åˆ†æ•°ï¼Œå¹¶åœ¨ Claude Codeã€Clineã€Roo Code å’Œ Kilo Code ç­‰åº”ç”¨ç¨‹åºä¸­è¡¨ç°å‡ºæ›´å¥½çš„å®é™…æ€§èƒ½ï¼ŒåŒ…æ‹¬æ”¹è¿›ç”Ÿæˆè§†è§‰æ•ˆæœç²¾ç¾çš„å‰ç«¯é¡µé¢ã€‚ /// - Advanced reasoning: GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability. // é«˜çº§æ¨ç† ï¼šGLM-4.6 åœ¨æ¨ç†æ€§èƒ½æ–¹é¢è¡¨ç°å‡ºæ˜æ˜¾çš„æå‡ï¼Œå¹¶æ”¯æŒåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨å·¥å…·ï¼Œä»è€Œæé«˜äº†æ•´ä½“èƒ½åŠ›ã€‚ /// - More capable agents: GLM-4.6 exhibits stronger performance in tool use and search-based agents, and integrates more effectively within agent frameworks. // æ›´å¼ºå¤§çš„æ™ºèƒ½ä½“ ï¼šGLM-4.6 åœ¨å·¥å…·ä½¿ç”¨å’ŒåŸºäºæœç´¢çš„æ™ºèƒ½ä½“ä»£ç†æ–¹é¢è¡¨ç°å‡ºæ›´å¼ºçš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨æ™ºèƒ½ä½“æ¡†æ¶ä¸­é›†æˆå¾—æ›´æœ‰æ•ˆã€‚ /// - Refined writing: Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios. // æ›´ç²¾ç‚¼çš„å†™ä½œ ï¼šåœ¨é£æ ¼å’Œå¯è¯»æ€§æ–¹é¢æ›´ç¬¦åˆäººç±»çš„å–œå¥½ï¼Œå¹¶ä¸”åœ¨è§’è‰²æ‰®æ¼”åœºæ™¯ä¸­è¡¨ç°å¾—æ›´è‡ªç„¶ã€‚ /// Properties /// - Input Modalities // è¾“å…¥æ–¹å¼ ///: Text // æ–‡æœ¬ /// - Output Modalitie // è¾“å‡ºæ¨¡å¼ ///: Text // æ–‡æœ¬ /// - Context Length // ä¸Šä¸‹æ–‡é•¿åº¦ ///: 200K // 20ä¸‡ /// - Maximum Output Tokens // æœ€å¤§è¾“å‡º Tokens ///: 128K /// ### 1. Comprehensive Evaluation // 1. ç»¼åˆè¯„ä»· /// In evaluations across 8 authoritative benchmarks for general model capabilitiesâ€”including AIME 25, GPQA, LCB v6, HLE, and SWE-Bench Verifiedâ€”GLM-4.6 achieves performance on par with Claude Sonnet 4/Claude Sonnet 4.6 on several leaderboards, solidifying its position as the top model developed in China. // åœ¨ 8 é¡¹æƒå¨é€šç”¨æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆåŒ…æ‹¬ AIME 25ã€GPQAã€LCB v6ã€HLE å’Œ SWE-Bench Verifiedï¼‰çš„è¯„ä¼°ä¸­ï¼ŒGLM-4.6 åœ¨å¤šä¸ªæ’è¡Œæ¦œä¸Šå–å¾—äº†ä¸ Claude Sonnet 4/Claude Sonnet 4.6 ç›¸å½“çš„æ€§èƒ½ï¼Œå·©å›ºäº†å…¶ä½œä¸ºä¸­å›½é¡¶çº§æ¨¡å‹çš„åœ°ä½ã€‚ /// ### 2. Real-World Coding Evaluation // 2. å®é™…ç¼–ç è¯„ä¼° /// To better test the modelâ€™s capabilities in practical coding tasks, we conducted 74 real-world coding tests within the Claude Code environment. The results show that GLM-4.6 surpasses Claude Sonnet 4 and other domestic models in these real-world tests. // ä¸ºäº†æ›´å¥½åœ°æµ‹è¯•æ¨¡å‹åœ¨å®é™…ç¼–ç ä»»åŠ¡ä¸­çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨ Claude Code ç¯å¢ƒä¸‹è¿›è¡Œäº† 74 æ¬¡çœŸå®ä¸–ç•Œçš„ç¼–ç æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼ŒGLM-4.6 åœ¨è¿™äº›çœŸå®ä¸–ç•Œæµ‹è¯•ä¸­ä¼˜äº Claude Sonnet 4 å’Œå…¶ä»–å›½äº§æ¨¡å‹ã€‚ /// In terms of average token consumption, GLM-4.6 is over 30% more efficient than GLM-4.5, achieving the lowest consumption rate among comparable models. // å°±å¹³å‡ Token æ¶ˆè€—è€Œè¨€ï¼ŒGLM-4.6 æ¯” GLM-4.5 æ•ˆç‡é«˜å‡º 30% ä»¥ä¸Šï¼Œåœ¨åŒç±»æ¨¡å‹ä¸­å®ç°äº†æœ€ä½çš„æ¶ˆè€—ç‡ã€‚ /// To ensure transparency and credibility, Z.ai has publicly released all test questions and agent trajectories for verification and reproduction. (Link: https://huggingface.co/datasets/zai-org/CC-Bench-trajectories). // ä¸ºç¡®ä¿é€æ˜åº¦å’Œå¯ä¿¡åº¦ï¼ŒZ.ai å·²å…¬å¼€æ‰€æœ‰æµ‹è¯•é¢˜å’Œæ™ºèƒ½ä½“è½¨è¿¹ï¼Œä»¥ä¾›éªŒè¯å’Œå¤ç°ã€‚"
[GLM-4.5-model.intro/.docs]: https://docs.z.ai/guides/llm/glm-4.5 "GLM-4.5 and GLM-4.5-Air are our latest flagship models, purpose-built as foundational models for agent-oriented applications. Both leverage a Mixture-of-Experts (MoE) architecture. GLM-4.5 has a total parameter count of 355B with 32B active parameters per forward pass, while GLM-4.5-Air adopts a more streamlined design with 106B total parameters and 12B active parameters. // GLM-4.5 å’Œ GLM-4.5-Air æ˜¯æˆ‘ä»¬æœ€æ–°çš„æ——èˆ°æ¨¡å‹ï¼Œä¸“ä¸ºé¢å‘æ™ºèƒ½ä½“ä»£ç†çš„åº”ç”¨è€Œæ„å»ºï¼Œæ˜¯å…¶åŸºç¡€æ¨¡å‹ã€‚ä¸¤è€…éƒ½é‡‡ç”¨äº†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ã€‚GLM-4.5 çš„æ€»å‚æ•°æ•°ä¸º 355 äº¿ï¼Œæ¯æ¬¡å‰å‘ä¼ æ’­æœ‰ 32 äº¿ä¸ªæœ‰æ•ˆå‚æ•°ï¼›è€Œ GLM-4.5-Air åˆ™é‡‡ç”¨äº†æ›´ä¸ºç²¾ç®€çš„è®¾è®¡ï¼Œæ€»å‚æ•°æ•°ä¸º 106 äº¿ï¼Œæ¯æ¬¡å‰å‘ä¼ æ’­æœ‰ 12 äº¿ä¸ªæœ‰æ•ˆå‚æ•°ã€‚ /// Both models share a similar training pipeline: an initial pretraining phase on 15 trillion tokens of general-domain data, followed by targeted fine-tuning on datasets covering code, reasoning, and agent-specific tasks. The context length has been extended to 128k tokens, and reinforcement learning was applied to further enhance reasoning, coding, and agent performance. // è¿™ä¸¤ä¸ªæ¨¡å‹é‡‡ç”¨ç±»ä¼¼çš„è®­ç»ƒæµç¨‹ï¼šé¦–å…ˆåœ¨åŒ…å« 15 ä¸‡äº¿ä¸ªè¯å…ƒçš„é€šç”¨é¢†åŸŸæ•°æ®é›†ä¸Šè¿›è¡Œåˆå§‹é¢„è®­ç»ƒï¼Œç„¶ååœ¨æ¶µç›–ä»£ç ã€æ¨ç†å’Œæ™ºèƒ½ä½“ç‰¹å®šä»»åŠ¡çš„æ•°æ®é›†ä¸Šè¿›è¡Œé’ˆå¯¹æ€§å¾®è°ƒã€‚ä¸Šä¸‹æ–‡é•¿åº¦å·²æ‰©å±•åˆ° 12.8 ä¸‡ä¸ªè¯å…ƒï¼Œå¹¶åº”ç”¨å¼ºåŒ–å­¦ä¹ æ¥è¿›ä¸€æ­¥æå‡æ¨ç†ã€ç¼–ç å’Œæ™ºèƒ½ä½“æ€§èƒ½ã€‚ /// GLM-4.5 and GLM-4.5-Air are optimized for tool invocation, web browsing, software engineering, and front-end development. They can be integrated into code-centric agents such as Claude Code and Roo Code, and also support arbitrary agent applications through tool invocation APIs. // GLM-4.5 å’Œ GLM-4.5-Air é’ˆå¯¹å·¥å…·è°ƒç”¨ã€ç½‘é¡µæµè§ˆã€è½¯ä»¶å·¥ç¨‹å’Œå‰ç«¯å¼€å‘è¿›è¡Œäº†ä¼˜åŒ–ã€‚å®ƒä»¬å¯ä»¥é›†æˆåˆ° Claude Code å’Œ Roo Code ç­‰ä»¥ä»£ç ä¸ºä¸­å¿ƒçš„æ™ºèƒ½ä½“ä¸­ï¼Œå¹¶ä¸”è¿˜å¯ä»¥é€šè¿‡å·¥å…·è°ƒç”¨ API æ”¯æŒä»»æ„æ™ºèƒ½ä½“ä»£ç†åº”ç”¨ç¨‹åºã€‚ /// Both models support hybrid reasoning modes, offering two execution modes: Thinking Mode for complex reasoning and tool usage, and Non-Thinking Mode for instant responses. These modes can be toggled via the thinking.typeparameter (with enabled and disabled settings), and dynamic thinking is enabled by default. // ä¸¤ç§æ¨¡å‹å‡æ”¯æŒæ··åˆæ¨ç†æ¨¡å¼ï¼Œæä¾›ä¸¤ç§æ‰§è¡Œæ¨¡å¼ï¼šç”¨äºå¤æ‚æ¨ç†å’Œå·¥å…·ä½¿ç”¨çš„æ€è€ƒæ¨¡å¼ï¼Œä»¥åŠç”¨äºå³æ—¶å“åº”çš„éæ€è€ƒæ¨¡å¼ã€‚è¿™äº›æ¨¡å¼å¯é€šè¿‡ thinking.type å‚æ•°ï¼ˆå…·æœ‰ enabled å’Œ disabled è®¾ç½®ï¼‰è¿›è¡Œåˆ‡æ¢ï¼ŒåŠ¨æ€æ€è€ƒæ¨¡å¼é»˜è®¤å¯ç”¨ã€‚ /// Properties /// - Input Modalities // è¾“å…¥æ–¹å¼ ///: Text // æ–‡æœ¬ /// - Output Modalitie // è¾“å‡ºæ¨¡å¼ ///: Text // æ–‡æœ¬ /// - Context Length // ä¸Šä¸‹æ–‡é•¿åº¦ ///: 128K /// - Maximum Output Tokens // æœ€å¤§è¾“å‡º Token ///: 96K /// Capability // èƒ½åŠ› /// - Deep Thinking // æ·±åº¦æ€è€ƒ ///: Enable deep thinking mode for more advanced reasoning and analysis // å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼ï¼Œè¿›è¡Œæ›´é«˜çº§çš„æ¨ç†å’Œåˆ†æ /// - Streaming Output // æµè¾“å‡º ///: Support real-time streaming responses to enhance user interaction experience // æ”¯æŒå®æ—¶æµå¼å“åº”ï¼Œä»¥å¢å¼ºç”¨æˆ·äº¤äº’ä½“éªŒ /// - Function Call // å‡½æ•°è°ƒç”¨ ///: Powerful tool invocation capabilities, enabling integration with various external toolsets // å¼ºå¤§çš„å·¥å…·è°ƒç”¨åŠŸèƒ½ï¼Œå¯ä¸å„ç§å¤–éƒ¨å·¥å…·é›†é›†æˆ /// - Context Caching // ä¸Šä¸‹æ–‡ç¼“å­˜ ///: Intelligent caching mechanism to optimize performance in long conversations // æ™ºèƒ½ç¼“å­˜æœºåˆ¶ï¼Œä¼˜åŒ–é•¿æ—¶é—´å¯¹è¯ä¸­çš„æ€§èƒ½ /// - Structured Output // ç»“æ„åŒ–è¾“å‡º ///: Support for structured output formats like JSON, facilitating system integration // æ”¯æŒ JSON ç­‰ç»“æ„åŒ–è¾“å‡ºæ ¼å¼ï¼Œä¾¿äºç³»ç»Ÿé›†æˆ /// The first-principle measure of AGI lies in integrating more general intelligence capabilities without compromising existing functions. GLM-4.5 represents our first complete realization of this concept. It combines advanced reasoning, coding, and agent capabilities within a single model, achieving a significant technological breakthrough by natively fusing reasoning, coding, and agent abilities to meet the complex demands of agent-based applications. // é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„é¦–è¦åŸåˆ™åœ¨äºæ•´åˆæ›´é€šç”¨çš„æ™ºèƒ½èƒ½åŠ›ï¼ŒåŒæ—¶åˆä¸æŸå®³ç°æœ‰åŠŸèƒ½ã€‚GLM-4.5 ä»£è¡¨äº†æˆ‘ä»¬å¯¹è¿™ä¸€æ¦‚å¿µçš„é¦–æ¬¡å®Œæ•´å®ç°ã€‚å®ƒå°†é«˜çº§æ¨ç†ã€ç¼–ç å’Œæ™ºèƒ½ä½“èƒ½åŠ›æ•´åˆåˆ°ä¸€ä¸ªå•ä¸€æ¨¡å‹ä¸­ï¼Œé€šè¿‡åŸç”Ÿèåˆæ¨ç†ã€ç¼–ç å’Œæ™ºèƒ½ä½“èƒ½åŠ›ï¼Œæ»¡è¶³åŸºäºæ™ºèƒ½ä½“çš„åº”ç”¨çš„å¤æ‚éœ€æ±‚ï¼Œä»è€Œå®ç°äº†é‡å¤§çš„æŠ€æœ¯çªç ´ã€‚ /// To comprehensively evaluate the modelâ€™s general intelligence, we selected 12 of the most representative benchmark suites, including MMLU Pro, AIME24, MATH 500, SciCode, GPQA, HLE, LiveCodeBench, SWE-Bench, Terminal-bench, TAU-Bench, BFCL v3, and BrowseComp. Based on the aggregated average scores, GLM-4.5 ranks second globally among all models, first among domestic models, and first among open-source models. // ä¸ºäº†å…¨é¢è¯„ä¼°æ¨¡å‹çš„é€šç”¨æ™ºèƒ½ï¼Œæˆ‘ä»¬é€‰å–â€‹â€‹äº† 12 ä¸ªæœ€å…·ä»£è¡¨æ€§çš„åŸºå‡†æµ‹è¯•å¥—ä»¶ï¼ŒåŒ…æ‹¬ MMLU Proã€AIME24ã€MATH 500ã€SciCodeã€GPQAã€HLEã€LiveCodeBenchã€SWE-Benchã€Terminal-benchã€TAU-Benchã€BFCL v3 å’Œ BrowseCompã€‚åŸºäºç»¼åˆå¹³å‡å¾—åˆ†ï¼ŒGLM-4.5 åœ¨å…¨çƒæ‰€æœ‰æ¨¡å‹ä¸­æ’åç¬¬äºŒï¼Œåœ¨å›½å†…æ¨¡å‹ä¸­æ’åç¬¬ä¸€ï¼Œåœ¨å¼€æºæ¨¡å‹ä¸­æ’åç¬¬ä¸€ã€‚ /// ### Higher Parameter Efficiency // æ›´é«˜çš„å‚æ•°æ•ˆç‡ /// GLM-4.5 has half the number of parameters of DeepSeek-R1 and one-third that of Kimi-K2, yet it outperforms them on multiple standard benchmark tests. This is attributed to the higher parameter efficiency of GLM architecture. Notably, GLM-4.5-Air, with 106 billion total parameters and 12 billion active parameters, achieves a significant breakthroughâ€”surpassing models such as Gemini 2.5 Flash, Qwen3-235B, and Claude 4 Opus on reasoning benchmarks like Artificial Analysis, ranking among the top three domestic models in performance. // GLM-4.5 çš„å‚æ•°æ•°é‡ä»…ä¸º DeepSeek-R1 çš„ä¸€åŠï¼ŒKimi-K2 çš„ä¸‰åˆ†ä¹‹ä¸€ï¼Œä½†åœ¨å¤šé¡¹æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å´è¡¨ç°æ›´ä½³ã€‚è¿™å½’åŠŸäº GLM æ¶æ„æ›´é«˜çš„å‚æ•°æ•ˆç‡ã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒGLM-4.5-Air æ‹¥æœ‰ 1060 äº¿ä¸ªæ€»å‚æ•°å’Œ 120 äº¿ä¸ªæœ‰æ•ˆå‚æ•°ï¼Œå–å¾—äº†æ˜¾è‘—çªç ´â€”â€”åœ¨äººå·¥æ™ºèƒ½åˆ†æç­‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº† Gemini 2.5 Flashã€Qwen3-235B å’Œ Claude 4 Opus ç­‰å‹å·ï¼Œè·»èº«å›½å†…æ€§èƒ½å‰ä¸‰ã€‚ /// On charts such as SWE-Bench Verified, the GLM-4.5 series lies on the Pareto frontier for performance-to-parameter ratio, demonstrating that at the same scale, the GLM-4.5 series delivers optimal performance. // åœ¨ SWE-Bench Verified ç­‰å›¾è¡¨ä¸­ï¼ŒGLM-4.5 ç³»åˆ—ä½äºæ€§èƒ½å‚æ•°æ¯”çš„å¸•ç´¯æ‰˜å‰æ²¿ï¼Œè¿™è¡¨æ˜åœ¨ç›¸åŒçš„è§„æ¨¡ä¸‹ï¼ŒGLM-4.5 ç³»åˆ—æä¾›äº†æœ€ä½³æ€§èƒ½ã€‚ /// ### Low Cost, High Speed // ä½æˆæœ¬ã€é«˜é€Ÿåº¦ /// Beyond performance optimization, the GLM-4.5 series also achieves breakthroughs in cost and efficiency, resulting in pricing far lower than mainstream models: API call costs are as low as $0.2 per million input tokens and $1.1 per million output tokens. // é™¤äº†æ€§èƒ½ä¼˜åŒ–ä¹‹å¤–ï¼ŒGLM-4.5 ç³»åˆ—åœ¨æˆæœ¬å’Œæ•ˆç‡æ–¹é¢ä¹Ÿå–å¾—äº†çªç ´ï¼Œå› æ­¤ä»·æ ¼è¿œä½äºä¸»æµå‹å·ï¼šAPI è°ƒç”¨æˆæœ¬ä½è‡³æ¯ç™¾ä¸‡è¾“å…¥ä»¤ç‰Œ 0.2 ç¾å…ƒï¼Œæ¯ç™¾ä¸‡è¾“å‡ºä»¤ç‰Œ 1.1 ç¾å…ƒã€‚ /// At the same time, the high-speed version demonstrates a generation speed exceeding 100 tokens per second in real-world tests, supporting low-latency and high-concurrency deployment scenariosâ€”balancing cost-effectiveness with user interaction experience. // åŒæ—¶ï¼Œé«˜é€Ÿç‰ˆæœ¬åœ¨å®é™…æµ‹è¯•ä¸­å±•ç°å‡ºæ¯ç§’ç”Ÿæˆè¶…è¿‡ 100 ä¸ªä»¤ç‰Œçš„é€Ÿåº¦ï¼Œæ”¯æŒä½å»¶è¿Ÿå’Œé«˜å¹¶å‘éƒ¨ç½²åœºæ™¯ï¼Œå…¼é¡¾æˆæœ¬æ•ˆç›Šå’Œç”¨æˆ·äº¤äº’ä½“éªŒã€‚ /// ### Real-World Evaluation // å®é™…è¯„ä¼° /// Real-world performance matters more than leaderboard rankings. To evaluate GLM-4.5â€™s effectiveness in practical Agent Coding scenarios, we integrated it into Claude Code and benchmarked it against Claude 4 Sonnet, Kimi-K2, and Qwen3-Coder. // å®é™…æ€§èƒ½æ¯”æ’è¡Œæ¦œæ’åæ›´é‡è¦ã€‚ä¸ºäº†è¯„ä¼° GLM-4.5 åœ¨å®é™…æ™ºèƒ½ä½“ç¼–ç åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å°†å…¶é›†æˆåˆ° Claude Code ä¸­ï¼Œå¹¶ä¸ Claude 4 Sonnetã€Kimi-K2 å’Œ Qwen3-Coder è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ /// The evaluation consisted of 52 programming and development tasks spanning six major domains, executed in isolated container environments with multi-turn interaction tests. // è¯„ä¼°åŒ…æ‹¬ 52 é¡¹ç¼–ç¨‹å’Œå¼€å‘ä»»åŠ¡ï¼Œæ¶µç›–å…­ä¸ªä¸»è¦é¢†åŸŸï¼Œåœ¨éš”ç¦»çš„å®¹å™¨ç¯å¢ƒä¸­æ‰§è¡Œï¼Œå¹¶è¿›è¡Œå¤šè½®äº¤äº’æµ‹è¯•ã€‚ /// As shown in the results (below), GLM-4.5 demonstrates a strong competitive advantage over other open-source models, particularly in tool invocation reliability and task completion rate. While there remains room for improvement compared to Claude 4 Sonnet, GLM-4.5 delivers a largely comparable experience in most scenarios. // ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼ŒGLM-4.5 ç›¸è¾ƒäºå…¶ä»–å¼€æºæ¨¡å‹å±•ç°å‡ºæ˜¾è‘—çš„ç«äº‰ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯åœ¨å·¥å…·è°ƒç”¨å¯é æ€§å’Œä»»åŠ¡å®Œæˆç‡æ–¹é¢ã€‚è™½ç„¶ä¸ Claude 4 Sonnet ç›¸æ¯”ä»æœ‰æ”¹è¿›ç©ºé—´ï¼Œä½† GLM-4.5 åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹éƒ½èƒ½æä¾›åŸºæœ¬ç›¸å½“çš„ä½¿ç”¨ä½“éªŒã€‚ /// To ensure transparency, we have released all 52 test problems along with full agent trajectories for industry validation and reproducibility. // ä¸ºäº†ç¡®ä¿é€æ˜åº¦ï¼Œæˆ‘ä»¬å‘å¸ƒäº†å…¨éƒ¨ 52 ä¸ªæµ‹è¯•é—®é¢˜ä»¥åŠå®Œæ•´çš„æ™ºèƒ½ä½“ä»£ç†è½¨è¿¹ï¼Œ ä»¥ä¾›è¡Œä¸šéªŒè¯å’Œå¯å¤ç°ã€‚ /// ## Usage // ä½¿ç”¨ /// ### Web Development // ç½‘ç«™å¼€å‘ /// Core Capability: Coding Skills â†’ Intelligent code generation | Real-time code completion | Automated bug fixing // æ ¸å¿ƒåŠŸèƒ½ï¼š ç¼–ç æŠ€èƒ½ â†’ æ™ºèƒ½ä»£ç ç”Ÿæˆ | å®æ—¶ä»£ç è¡¥å…¨ | è‡ªåŠ¨é”™è¯¯ä¿®å¤ /// - Supports major languages including Python, JavaScript, and Java. // æ”¯æŒåŒ…æ‹¬ Pythonã€JavaScript å’Œ Java åœ¨å†…çš„ä¸»æµè¯­è¨€ã€‚ /// - Generates well-structured, scalable, high-quality code based on natural language instructions. // æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆç»“æ„è‰¯å¥½ã€å¯æ‰©å±•ã€é«˜è´¨é‡çš„ä»£ç ã€‚ /// - Focuses on real-world development needs, avoiding templated or generic outputs. // ä¸“æ³¨äºæ»¡è¶³å®é™…å¼€å‘éœ€æ±‚ï¼Œé¿å…æ¨¡æ¿åŒ–æˆ–é€šç”¨è¾“å‡ºã€‚ /// Use Case: Complete refactoring-level tasks within 1 hour; generate full product prototypes in 5 minutes. // ä½¿ç”¨åœºæ™¯ï¼š 1 å°æ—¶å†…å®Œæˆé‡æ„çº§åˆ«çš„ä»»åŠ¡ï¼›5 åˆ†é’Ÿå†…ç”Ÿæˆå®Œæ•´çš„äº§å“åŸå‹ã€‚ /// ### AI Assistant // äººå·¥æ™ºèƒ½åŠ©æ‰‹ /// Core Capabilities: Agent Abilities â†’ Autonomous task planning | Multi-tool orchestration | Dynamic environment interaction // æ ¸å¿ƒåŠŸèƒ½ï¼š æ™ºèƒ½ä½“èƒ½åŠ› â†’ è‡ªä¸»ä»»åŠ¡è§„åˆ’ | å¤šå·¥å…·åè°ƒ | åŠ¨æ€ç¯å¢ƒäº¤äº’ /// - Automatically decomposes complex tasks into clear, executable step-by-step plans. // è‡ªåŠ¨å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºæ¸…æ™°ã€å¯æ‰§è¡Œçš„åˆ†æ­¥è®¡åˆ’ã€‚ /// - Flexibly invokes development tools to complete coding, debugging, and validation in a one-stop workflow. // çµæ´»è°ƒç”¨å¼€å‘å·¥å…·ï¼Œåœ¨ä¸€ä¸ªå·¥ä½œæµç¨‹ä¸­å®Œæˆç¼–ç ã€è°ƒè¯•å’ŒéªŒè¯ã€‚ /// - Dynamically adjusts strategies based on real-time feedback, quickly adapting to task changes and continuously optimizing execution paths. // æ ¹æ®å®æ—¶åé¦ˆåŠ¨æ€è°ƒæ•´ç­–ç•¥ï¼Œå¿«é€Ÿé€‚åº”ä»»åŠ¡å˜åŒ–ï¼Œä¸æ–­ä¼˜åŒ–æ‰§è¡Œè·¯å¾„ã€‚ /// Use Case: In multi-module collaborative development projects, delivery cycles were shortened by 40%, and manpower investment was reduced by approximately 30%. // ä½¿ç”¨æ¡ˆä¾‹ï¼š åœ¨å¤šæ¨¡å—ååŒå¼€å‘é¡¹ç›®ä¸­ï¼Œäº¤ä»˜å‘¨æœŸç¼©çŸ­äº† 40%ï¼ŒäººåŠ›æŠ•å…¥å‡å°‘äº†çº¦ 30%ã€‚ /// ### Smart Office // æ™ºèƒ½åŠå…¬ /// Core Capability: PPT Creation â†’ Clear logic | Complete content | Effective visual presentation // æ ¸å¿ƒèƒ½åŠ›ï¼š PPT åˆ¶ä½œ â†’ é€»è¾‘æ¸…æ™° | å†…å®¹å®Œæ•´ | è§†è§‰æ•ˆæœå‡ºè‰² /// - Content Expansion by Theme: Generates multi-slide PPT content from a single title or central concept. // æŒ‰ä¸»é¢˜æ‰©å±•å†…å®¹ï¼š æ ¹æ®å•ä¸ªæ ‡é¢˜æˆ–ä¸­å¿ƒæ¦‚å¿µç”Ÿæˆå¤šå¼ å¹»ç¯ç‰‡çš„ PPT å†…å®¹ã€‚ /// - Logical Structure Organization: Automatically segments content into introduction, body, and conclusion modules with well-organized semantic flow. // é€»è¾‘ç»“æ„ç»„ç»‡ï¼š è‡ªåŠ¨å°†å†…å®¹åˆ†å‰²æˆå¼•è¨€ã€æ­£æ–‡å’Œç»“è®ºæ¨¡å—ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„è¯­ä¹‰æµç¨‹ã€‚ /// - Slide Layout Suggestions: Works with template systems to recommend optimal presentation styles for the generated content. // å¹»ç¯ç‰‡å¸ƒå±€å»ºè®®ï¼š ä¸æ¨¡æ¿ç³»ç»Ÿé…åˆä½¿ç”¨ï¼Œä¸ºç”Ÿæˆçš„å†…å®¹æ¨èæœ€ä½³æ¼”ç¤ºæ ·å¼ã€‚ /// Use Case: Suitable for office automation platforms, AI presentation tools, and other productivity-focused products. // ä½¿ç”¨åœºæ™¯ï¼š é€‚ç”¨äºåŠå…¬è‡ªåŠ¨åŒ–å¹³å°ã€AI æ¼”ç¤ºå·¥å…·å’Œå…¶ä»–ä»¥æé«˜ç”Ÿäº§åŠ›ä¸ºä¸­å¿ƒçš„äº§å“ã€‚ /// ### Intelligent Question Answering // æ™ºèƒ½é—®ç­” /// Core Capability: Model reasoning power â†’ Precise instruction parsing | Multi-turn logical reasoning | Domain knowledge integration // æ ¸å¿ƒèƒ½åŠ›ï¼š æ¨¡å‹æ¨ç†èƒ½åŠ› â†’ ç²¾ç¡®æŒ‡ä»¤è§£æ | å¤šè½®é€»è¾‘æ¨ç† | é¢†åŸŸçŸ¥è¯†æ•´åˆ /// - Deep Natural Language Understanding â€“ Accurately interprets natural language instructions, extracts key intents, and converts them into executable tasks. // æ·±åº¦è‡ªç„¶è¯­è¨€ç†è§£ â€”â€”å‡†ç¡®è§£é‡Šè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œæå–å…³é”®æ„å›¾ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå¯æ‰§è¡Œä»»åŠ¡ã€‚ /// - Complex Multi-Turn Reasoning â€“ Supports multi-step logical reasoning chains, efficiently handling composite problems involving cross-step dependencies and multiple variables. // å¤æ‚å¤šè½®æ¨ç† â€”â€”æ”¯æŒå¤šæ­¥éª¤é€»è¾‘æ¨ç†é“¾ï¼Œæœ‰æ•ˆå¤„ç†æ¶‰åŠè·¨æ­¥éª¤ä¾èµ–å…³ç³»å’Œå¤šä¸ªå˜é‡çš„å¤åˆé—®é¢˜ã€‚ /// - Domain Knowledge Fusion â€“ Integrates domain-specific expertise with contextual information to enhance reasoning accuracy and output stability. // é¢†åŸŸçŸ¥è¯†èåˆ â€”â€”å°†ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ä¸ä¸Šä¸‹æ–‡ä¿¡æ¯ç›¸ç»“åˆï¼Œä»¥æé«˜æ¨ç†å‡†ç¡®æ€§å’Œè¾“å‡ºç¨³å®šæ€§ã€‚ /// Use Case: In complex business workflows, accuracy improves by 60%, and reasoning efficiency improves by 70%. // ä½¿ç”¨æ¡ˆä¾‹ï¼š åœ¨å¤æ‚çš„ä¸šåŠ¡å·¥ä½œæµç¨‹ä¸­ï¼Œå‡†ç¡®ç‡æé«˜äº† 60% ï¼Œæ¨ç†æ•ˆç‡æé«˜äº† 70% ã€‚ /// ### Complex Text Translation // å¤æ‚æ–‡æœ¬ç¿»è¯‘ /// Core Capabilities: Translation Proficiency â†’ Strong contextual consistency | Accurate style preservation | Excellent handling of long passages // æ ¸å¿ƒèƒ½åŠ›ï¼š ç¿»è¯‘æ°´å¹³ â†’ ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å¼º | é£æ ¼ä¿ç•™å‡†ç¡® | é•¿ç¯‡æ®µè½å¤„ç†å‡ºè‰² /// - Long, Complex Sentence Translation: Maintains semantic coherence and structural accuracy, ideal for policy and academic materials. // é•¿å¥ã€å¤æ‚å¥ç¿»è¯‘ï¼š ä¿æŒè¯­ä¹‰è¿è´¯æ€§å’Œç»“æ„å‡†ç¡®æ€§ï¼Œæ˜¯æ”¿ç­–å’Œå­¦æœ¯ææ–™çš„ç†æƒ³é€‰æ‹©ã€‚ /// - Style Retention and Adaptation: Preserves the original tone or adapts to the target languageâ€™s commonly used expression style during translation. // é£æ ¼ä¿ç•™ä¸è°ƒæ•´ï¼š åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­ä¿ç•™åŸæ–‡çš„è¯­æ°”æˆ–è°ƒæ•´ä¸ºç›®æ ‡è¯­è¨€å¸¸ç”¨çš„è¡¨è¾¾é£æ ¼ã€‚ /// - Support for Low-Resource Languages and Informal Contexts: Preliminary coverage of 26 languages, with capabilities to translate social and informal texts. // æ”¯æŒä½èµ„æºè¯­è¨€å’Œéæ­£å¼è¯­å¢ƒï¼š åˆæ­¥è¦†ç›– 26 ç§è¯­è¨€ï¼Œå…·å¤‡ç¿»è¯‘ç¤¾äº¤å’Œéæ­£å¼æ–‡æœ¬çš„èƒ½åŠ›ã€‚ /// Use Cases: Suitable for publishing house translations, content localization for overseas markets, cross-border customer service, and social media platforms. // ä½¿ç”¨æ¡ˆä¾‹ï¼š é€‚ç”¨äºå‡ºç‰ˆç¤¾ç¿»è¯‘ã€æµ·å¤–å¸‚åœºå†…å®¹æœ¬åœ°åŒ–ã€è·¨å¢ƒå®¢æˆ·æœåŠ¡å’Œç¤¾äº¤åª’ä½“å¹³å°ã€‚ /// ### Content Creation // å†…å®¹åˆ›ä½œ /// Core Capability: Creative Writing â†’ Natural expression | Rich emotion | Complete structure // æ ¸å¿ƒèƒ½åŠ›ï¼š åˆ›æ„å†™ä½œ â†’ è‡ªç„¶è¡¨è¾¾ | é¥±å«æƒ…æ„Ÿ | ç»“æ„å®Œæ•´ /// - Generates coherent literary texts with clear narrative flow based on given themes, characters, or worldviews. // æ ¹æ®ç»™å®šçš„ä¸»é¢˜ã€äººç‰©æˆ–ä¸–ç•Œè§‚ï¼Œç”Ÿæˆå™äº‹æµç•…ã€é€»è¾‘è¿è´¯çš„æ–‡å­¦æ–‡æœ¬ã€‚ /// - Produces emotionally engaging copy tailored to audience profiles and product characteristics. // æ’°å†™èƒ½å¤Ÿå¼•èµ·æƒ…æ„Ÿå…±é¸£ã€ç¬¦åˆå—ä¼—ç¾¤ä½“å’Œäº§å“ç‰¹ç‚¹çš„æ–‡æ¡ˆã€‚ /// - Supports short videos and new media scripts aligned with platforms like Douyin and Xiaohongshu, integrating emotion control and narrative pacing. // æ”¯æŒä¸æŠ–éŸ³ã€å°çº¢ä¹¦ç­‰å¹³å°ç›¸å…¼å®¹çš„çŸ­è§†é¢‘å’Œæ–°åª’ä½“è„šæœ¬ï¼Œé›†æˆäº†æƒ…æ„Ÿæ§åˆ¶å’Œå™äº‹èŠ‚å¥æ§åˆ¶ã€‚ /// Use Case: Ideal for deployment in content creation platforms, marketing toolchains, or AI writing assistants to enhance content production efficiency and personalization. // ä½¿ç”¨æ¡ˆä¾‹ï¼š éå¸¸é€‚åˆéƒ¨ç½²åœ¨å†…å®¹åˆ›ä½œå¹³å°ã€è¥é”€å·¥å…·é“¾æˆ– AI å†™ä½œåŠ©æ‰‹ä¸Šï¼Œä»¥æé«˜å†…å®¹ç”Ÿäº§æ•ˆç‡å’Œä¸ªæ€§åŒ–ç¨‹åº¦ã€‚ /// ### Virtual Characters // è™šæ‹Ÿè§’è‰² /// Core Capability: Humanized Expression â†’ Natural tone | Accurate emotional conveyance | Consistent character behavior // æ ¸å¿ƒèƒ½åŠ›ï¼š äººæ€§åŒ–è¡¨è¾¾ â†’ è‡ªç„¶è¯­è°ƒ | ç²¾å‡†çš„æƒ…æ„Ÿä¼ è¾¾ | ä¸€è‡´çš„äººç‰©è¡Œä¸º /// - Role-Playing Dialogue System: Maintains consistent tone and behavior of the designated character across multi-turn conversations. // è§’è‰²æ‰®æ¼”å¯¹è¯ç³»ç»Ÿï¼š åœ¨å¤šå›åˆå¯¹è¯ä¸­ä¿æŒæŒ‡å®šè§’è‰²çš„ä¸€è‡´è¯­æ°”å’Œè¡Œä¸ºã€‚ /// - Emotionally Rich Copywriting: Delivers warm, relatable expressions suitable for building â€œhumanizedâ€ brands or companion-style user products. // æƒ…æ„Ÿä¸°å¯Œçš„æ–‡æ¡ˆï¼š ä¼ é€’æ¸©æš–ã€æ˜“äºç†è§£çš„è¡¨è¾¾æ–¹å¼ï¼Œé€‚åˆæ‰“é€ â€œäººæ€§åŒ–â€å“ç‰Œæˆ–ä¼´ä¾£å¼ç”¨æˆ·äº§å“ã€‚ /// - Virtual Persona Content Creation: Supports generation of content aligned with virtual streamers or character IPs, including social posts and fan interactions. // è™šæ‹Ÿäººç‰©å†…å®¹åˆ›ä½œï¼š æ”¯æŒç”Ÿæˆä¸è™šæ‹Ÿä¸»æ’­æˆ–è§’è‰² IP ç›¸ç¬¦çš„å†…å®¹ï¼ŒåŒ…æ‹¬ç¤¾äº¤å¸–å­å’Œç²‰ä¸äº’åŠ¨ã€‚ /// Use Case: Ideal for virtual humans, social AI, and brand personification operations. // åº”ç”¨åœºæ™¯ï¼š éå¸¸é€‚åˆè™šæ‹Ÿäººã€ç¤¾äº¤äººå·¥æ™ºèƒ½å’Œå“ç‰Œæ‹ŸäººåŒ–æ“ä½œã€‚"
[GLM-eval.ds/hf]: https://huggingface.co/datasets/zai-org/CC-Bench-trajectories "(25.3 MB) (260 Rows) CC-Bench Trajectories Overview // CC-Bench è½¨è¿¹æ¦‚è¿° /// To evaluate GLM-4.6's agentic coding capabilities in real-world scenarios, we developed CC-Bench-V1.1 using Claude Code as the agentic coding testbed. Building on CC-Bench-V1.0, we added 22 more challenging coding tasks and conducted comprehensive evaluations against Claude-Sonnet-4, GLM-4.5, Kimi-K2-0905, and DeepSeek-V3.1-Terminus. The benchmark comprises 74 coding tasks spanning frontend development, tool development, data analysis, testing, and algorithm implementation, with complete agentic trajectories captured for all models. // ä¸ºäº†è¯„ä¼° GLM-4.6 åœ¨å®é™…åœºæ™¯ä¸­çš„æ™ºèƒ½ä½“ç¼–ç èƒ½åŠ›ï¼Œæˆ‘ä»¬ä»¥ Claude Code ä¸ºæ™ºèƒ½ä½“ç¼–ç æµ‹è¯•å¹³å°ï¼Œå¼€å‘äº† CC-Bench-V1.1ã€‚åœ¨ CC-Bench-V1.0 çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¢åŠ äº† 22 é¡¹æ›´å…·æŒ‘æˆ˜æ€§çš„ç¼–ç ä»»åŠ¡ï¼Œå¹¶ä¸ Claude-Sonnet-4ã€GLM-4.5ã€Kimi-K2-0905 å’Œ DeepSeek-V3.1-Terminus è¿›è¡Œäº†å…¨é¢çš„å¯¹æ¯”è¯„ä¼°ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å« 74 é¡¹ç¼–ç ä»»åŠ¡ï¼Œæ¶µç›–å‰ç«¯å¼€å‘ã€å·¥å…·å¼€å‘ã€æ•°æ®åˆ†æã€æµ‹è¯•å’Œç®—æ³•å®ç°ï¼Œå¹¶è®°å½•äº†æ‰€æœ‰æ¨¡å‹çš„å®Œæ•´æ™ºèƒ½ä½“è¿è¡Œè½¨è¿¹ã€‚"
[GLM-V.src/gh]: https://github.com/zai-org/GLM-V.git "(Apache-2.0) (Languages: Python 100.0%) GLM-4.6V/4.5V/4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning // GLM-4.6V/4.5V/4.1V-Thinkingï¼šé¢å‘å¯æ‰©å±•å¼ºåŒ–å­¦ä¹ çš„å¤šåŠŸèƒ½å¤šæ¨¡æ€æ¨ç† /// Vision-language models (VLMs) have become a key cornerstone of intelligent systems. As real-world AI tasks grow increasingly complex, VLMs urgently need to enhance reasoning capabilities beyond basic multimodal perception â€” improving accuracy, comprehensiveness, and intelligence â€” to enable complex problem solving, long-context understanding, and multimodal agents. // è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å·²æˆä¸ºæ™ºèƒ½ç³»ç»Ÿçš„å…³é”®åŸºçŸ³ã€‚éšç€ç°å®ä¸–ç•Œäººå·¥æ™ºèƒ½ä»»åŠ¡æ—¥ç›Šå¤æ‚ï¼ŒVLM è¿«åˆ‡éœ€è¦æå‡æ¨ç†èƒ½åŠ›ï¼Œè¶…è¶ŠåŸºæœ¬çš„å¤šæ¨¡æ€æ„ŸçŸ¥â€”â€”æé«˜å‡†ç¡®æ€§ã€å…¨é¢æ€§å’Œæ™ºèƒ½æ€§â€”â€”ä»¥å®ç°å¤æ‚é—®é¢˜è§£å†³ã€é•¿ä¸Šä¸‹æ–‡ç†è§£å’Œå¤šæ¨¡æ€æ™ºèƒ½ä½“ã€‚ /// Through our open-source work, we aim to explore the technological frontier together with the community while empowering more developers to create exciting and innovative applications. // é€šè¿‡æˆ‘ä»¬çš„å¼€æºå·¥ä½œï¼Œæˆ‘ä»¬æ—¨åœ¨ä¸ç¤¾åŒºä¸€èµ·æ¢ç´¢æŠ€æœ¯å‰æ²¿ï¼ŒåŒæ—¶èµ‹èƒ½æ›´å¤šå¼€å‘è€…ï¼Œè®©ä»–ä»¬èƒ½å¤Ÿåˆ›é€ æ¿€åŠ¨äººå¿ƒã€å¯Œæœ‰åˆ›æ„çš„åº”ç”¨ç¨‹åºã€‚"
[GLM-4.6V-model.tensor/hf]: https://huggingface.co/zai-org/GLM-4.6V "(MIT) (Model size: 108B params; Tensor type: BF16Â·F32) (Topics: Image-Text-to-Text, Transformers, Safetensors, Chinese, English) (Tags: glm4v_moe, any-to-any, conversational) (arxiv: 2507.01006)"
[GLM-4.6V-model.intro/.site:blog]: https://z.ai/blog/glm-4.6v "GLM-4.6V: Open Source Multimodal Models with Native Tool Use // GLM-4.6Vï¼šåŸç”Ÿæ”¯æŒå·¥å…·ä½¿ç”¨çš„å¼€æºå¤šæ¨¡æ€æ¨¡å‹ /// Today, we officially introduce and open-source the GLM-4.6V seriesâ€”our latest iteration in multimodal large language models. The release includes two versions: GLM-4.6V (106B), a foundation model designed for cloud and high-performance cluster scenarios, and GLM-4.6V-Flash (9B), a lightweight model optimized for local deployment and low-latency applications. // ä»Šå¤©ï¼Œæˆ‘ä»¬æ­£å¼å‘å¸ƒå¹¶å¼€æº GLM-4.6V ç³»åˆ—â€”â€”æˆ‘ä»¬æœ€æ–°çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æ­¤æ¬¡å‘å¸ƒåŒ…å«ä¸¤ä¸ªç‰ˆæœ¬ï¼š GLM-4.6V (106B) ï¼Œä¸€ä¸ªé¢å‘äº‘ç«¯å’Œé«˜æ€§èƒ½é›†ç¾¤åœºæ™¯çš„åŸºç¡€æ¨¡å‹ï¼›ä»¥åŠ GLM-4.6V-Flash (9B) ï¼Œä¸€ä¸ªé’ˆå¯¹æœ¬åœ°éƒ¨ç½²å’Œä½å»¶è¿Ÿåº”ç”¨ä¼˜åŒ–çš„è½»é‡çº§æ¨¡å‹ã€‚ /// GLM-4.6V scales its context window to 128k tokens in training, and achieves SoTA performance in visual understanding and reasoning among models of similar parameter scales. Crucially, we integrate native Function Calling capabilities for the first time. This effectively bridges the gap between \"visual perception\" and \"executable action,\" providing a unified technical foundation for multimodal agents in real-world business scenarios. // GLM-4.6V åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†ä¸Šä¸‹æ–‡çª—å£æ‰©å±•åˆ° 128k ä¸ª tokenï¼Œå¹¶åœ¨è§†è§‰ç†è§£å’Œæ¨ç†æ–¹é¢è¾¾åˆ°äº†åŒç­‰å‚æ•°è§„æ¨¡æ¨¡å‹ä¸­çš„æœ€ä½³æ°´å¹³ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬é¦–æ¬¡é›†æˆäº†åŸç”Ÿå‡½æ•°è°ƒç”¨åŠŸèƒ½ã€‚è¿™æœ‰æ•ˆåœ°å¼¥åˆäº†â€œè§†è§‰æ„ŸçŸ¥â€å’Œâ€œå¯æ‰§è¡ŒåŠ¨ä½œâ€ä¹‹é—´çš„é¸¿æ²Ÿï¼Œä¸ºç°å®ä¸–ç•Œå•†ä¸šåœºæ™¯ä¸­çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“æä¾›äº†ç»Ÿä¸€çš„æŠ€æœ¯åŸºç¡€ã€‚ /// Native Multimodal Tool Use // åŸç”Ÿå¤šæ¨¡æ€å·¥å…·çš„ä½¿ç”¨ /// Traditional tool use in LLMs often relies on pure text, requiring multiple intermediate conversions when dealing with images, videos, or complex documentsâ€”a process that potentially leads to information loss and increases system complexity. // LLM ä¸­çš„ä¼ ç»Ÿå·¥å…·ä½¿ç”¨é€šå¸¸ä¾èµ–äºçº¯æ–‡æœ¬ï¼Œåœ¨å¤„ç†å›¾åƒã€è§†é¢‘æˆ–å¤æ‚æ–‡æ¡£æ—¶éœ€è¦å¤šæ¬¡ä¸­é—´è½¬æ¢â€”â€”è¿™ä¸€è¿‡ç¨‹å¯èƒ½ä¼šå¯¼è‡´ä¿¡æ¯ä¸¢å¤±å¹¶å¢åŠ ç³»ç»Ÿå¤æ‚æ€§ã€‚ /// GLM-4.6V is equipped with native multimodal tool calling capability: // GLM-4.6V å…·å¤‡åŸç”Ÿå¤šæ¨¡æ€å·¥å…·è°ƒç”¨åŠŸèƒ½ï¼š /// - Multimodal Input: Images, screenshots, and document pages can be passed directly as tool parameters without being converted to textual descriptions in advance, thus avoiding information loss and largely simplifying pipeline. // å¤šæ¨¡æ€è¾“å…¥ï¼š å›¾åƒã€å±å¹•æˆªå›¾å’Œæ–‡æ¡£é¡µé¢å¯ä»¥ç›´æ¥ä½œä¸ºå·¥å…·å‚æ•°ä¼ é€’ï¼Œæ— éœ€äº‹å…ˆè½¬æ¢ä¸ºæ–‡æœ¬æè¿°ï¼Œä»è€Œé¿å…ä¿¡æ¯ä¸¢å¤±å¹¶å¤§å¤§ç®€åŒ–æµç¨‹ã€‚ /// - Multimodal Output: The model can visually comprehend results returned by toolsâ€”such as searching results, statistical charts, rendered web screenshots, or retrieved product imagesâ€”and incorporate them into subsequent reasoning chain, as well as final output. // å¤šæ¨¡æ€è¾“å‡ºï¼š è¯¥æ¨¡å‹èƒ½å¤Ÿç›´è§‚åœ°ç†è§£å·¥å…·è¿”å›çš„ç»“æœï¼ˆä¾‹å¦‚æœç´¢ç»“æœã€ç»Ÿè®¡å›¾è¡¨ã€æ¸²æŸ“çš„ç½‘é¡µæˆªå›¾æˆ–æ£€ç´¢åˆ°çš„äº§å“å›¾åƒï¼‰ï¼Œå¹¶å°†å®ƒä»¬çº³å…¥åç»­çš„æ¨ç†é“¾ä»¥åŠæœ€ç»ˆè¾“å‡ºä¸­ã€‚ /// This native support allows GLM-4.6V to close the loop from perception to understanding to execution, enabling complex tasks, such as rich-text content creation and visual web search. // è¿™ç§åŸç”Ÿæ”¯æŒä½¿ GLM-4.6V èƒ½å¤Ÿå®Œæˆä»æ„ŸçŸ¥åˆ°ç†è§£å†åˆ°æ‰§è¡Œçš„é—­ç¯ï¼Œä»è€Œå®ç°å¤æ‚çš„ä»»åŠ¡ï¼Œä¾‹å¦‚å¯Œæ–‡æœ¬å†…å®¹åˆ›å»ºå’Œè§†è§‰ç½‘ç»œæœç´¢ã€‚"
[GLM-4.6V-model.intro/.docs]: https://docs.z.ai/guides/vlm/glm-4.6v "GLM-4.6V series are Z.aiâ€™s iterations in a multimodal large language model. GLM-4.6V scales its context window to 128k tokens in training, and achieves SoTA performance in visual understanding among models of similar parameter scales. Crucially, GLM-4.6V integrate native Function Calling capabilities for the first time. This effectively bridges the gap between â€œvisual perceptionâ€ and â€œexecutable action,â€ providing a unified technical foundation for multimodal agents in real-world business scenarios. // GLM-4.6V ç³»åˆ—æ˜¯ Z.ai åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é¢†åŸŸçš„æœ€æ–°è¿­ä»£æˆæœã€‚GLM-4.6V åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†ä¸Šä¸‹æ–‡çª—å£æ‰©å±•è‡³ 12.8 ä¸‡ä¸ª tokenï¼Œå¹¶åœ¨åŒç­‰å‚æ•°è§„æ¨¡çš„æ¨¡å‹ä¸­å®ç°äº†è§†è§‰ç†è§£æ–¹é¢çš„æœ€ä½³æ€§èƒ½ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒGLM-4.6V é¦–æ¬¡é›†æˆäº†åŸç”Ÿå‡½æ•°è°ƒç”¨åŠŸèƒ½ã€‚è¿™æœ‰æ•ˆåœ°å¼¥åˆäº†â€œè§†è§‰æ„ŸçŸ¥â€å’Œâ€œå¯æ‰§è¡ŒåŠ¨ä½œâ€ä¹‹é—´çš„é¸¿æ²Ÿï¼Œä¸ºç°å®ä¸–ç•Œå•†ä¸šåœºæ™¯ä¸­çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“æä¾›äº†ç»Ÿä¸€çš„æŠ€æœ¯åŸºç¡€ã€‚ /// GLM-4.6V: /// - Positioning // å®šä½ ///: Flagship, Highest Performance // æ——èˆ°çº§ï¼Œæœ€é«˜æ€§èƒ½ /// - Input Modality // è¾“å…¥æ–¹å¼ ///: Video / Image / Text / File // è§†é¢‘/å›¾åƒ/æ–‡æœ¬/æ–‡ä»¶ /// - Output Modality // è¾“å‡ºæ–¹å¼ ///: Text // æ–‡æœ¬ /// - Context Length // ä¸Šä¸‹æ–‡é•¿åº¦ ///: 128K /// GLM-4.6V-FlashX: /// - Positioning // å®šä½ ///: Lightweight, High-Speed,and Affordable // è½»ä¾¿ã€é«˜é€Ÿã€ä»·æ ¼å®æƒ  /// - Input Modality // è¾“å…¥æ–¹å¼ ///: Video / Image / Text / File // è§†é¢‘/å›¾åƒ/æ–‡æœ¬/æ–‡ä»¶ /// - Output Modality // è¾“å‡ºæ–¹å¼ ///: Text // æ–‡æœ¬ /// - Context Length // ä¸Šä¸‹æ–‡é•¿åº¦ ///: 128K /// GLM-4.6V-Flash: /// - Positioning // å®šä½ ///: Lightweight, Completely Free // è½»é‡çº§ï¼Œå®Œå…¨å…è´¹ /// - Input Modality // è¾“å…¥æ–¹å¼ ///: Video / Image / Text / File // è§†é¢‘/å›¾åƒ/æ–‡æœ¬/æ–‡ä»¶ /// - Output Modality // è¾“å‡ºæ–¹å¼ ///: Text  æ–‡æœ¬ /// - Context Length // ä¸Šä¸‹æ–‡é•¿åº¦ ///: 128K"
[GLM-4.5v.paper/arxiv]: https://arxiv.org/abs/2507.01006 "(License: CC BY 4.0) [Submitted on 1 Jul 2025 (v1), last revised 15 Aug 2025 (this version, v5)] { Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) } (arXiv: 2507.01006 [cs.CV]) (doi: 10.48550/arXiv.2507.01006) GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning // GLM-4.5V å’Œ GLM-4.1V-Thinkingï¼šè¿ˆå‘å…·æœ‰å¯æ‰©å±•å¼ºåŒ–å­¦ä¹ çš„å¤šåŠŸèƒ½å¤šæ¨¡æ€æ¨ç† /// We present GLM-4.1V-Thinking and GLM-4.5V, a family of vision-language models (VLMs) designed to advance general-purpose multimodal understanding and reasoning. In this report, we share our key findings in the development of the reasoning-centric training framework. We first develop a capable vision foundation model with significant potential through large-scale pre-training, which arguably sets the upper bound for the final performance. We then propose Reinforcement Learning with Curriculum Sampling (RLCS) to unlock the full potential of the model, leading to comprehensive capability enhancement across a diverse range of tasks, including STEM problem solving, video understanding, content recognition, coding, grounding, GUI-based agents, and long document interpretation. In a comprehensive evaluation across 42 public benchmarks, GLM-4.5V achieves state-of-the-art performance on nearly all tasks among open-source models of similar size, and demonstrates competitive or even superior results compared to closed-source models such as Gemini-2.5-Flash on challenging tasks including Coding and GUI Agents. Meanwhile, the smaller GLM-4.1V-9B-Thinking remains highly competitive-achieving superior results to the much larger Qwen2.5-VL-72B on 29 benchmarks. We open-source both GLM-4.1V-9B-Thinking and GLM-4.5V. Code, models and more information are released at this https URL (. gh: zai-org/GLM-V.git). // æˆ‘ä»¬æå‡ºäº† GLM-4.1V-Thinking å’Œ GLM-4.5Vï¼Œè¿™æ˜¯ä¸€ç³»åˆ—è§†è§‰è¯­è¨€æ¨¡å‹ (VLM)ï¼Œæ—¨åœ¨æå‡é€šç”¨å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚åœ¨æœ¬æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬å°†åˆ†äº«æˆ‘ä»¬åœ¨å¼€å‘ä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„è®­ç»ƒæ¡†æ¶æ–¹é¢çš„ä¸»è¦å‘ç°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒå¼€å‘äº†ä¸€ä¸ªå…·æœ‰å·¨å¤§æ½œåŠ›çš„å¼ºå¤§è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥è¯´æ˜¯æœ€ç»ˆæ€§èƒ½çš„ä¸Šé™ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè¯¾ç¨‹é‡‡æ ·çš„å¼ºåŒ–å­¦ä¹  (RLCS) æ–¹æ³•æ¥å……åˆ†å‘æŒ¥æ¨¡å‹çš„æ½œåŠ›ï¼Œä»è€Œå…¨é¢æå‡æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬ STEM é—®é¢˜è§£å†³ã€è§†é¢‘ç†è§£ã€å†…å®¹è¯†åˆ«ã€ç¼–ç ã€æ¥åœ°ã€åŸºäº GUI çš„æ™ºèƒ½ä½“ä»¥åŠé•¿æ–‡æ¡£è§£è¯»ã€‚åœ¨å¯¹ 42 ä¸ªå…¬å¼€åŸºå‡†æµ‹è¯•çš„å…¨é¢è¯„ä¼°ä¸­ï¼ŒGLM-4.5V åœ¨å‡ ä¹æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†åŒç­‰è§„æ¨¡å¼€æºæ¨¡å‹ä¸­çš„æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨åŒ…æ‹¬ç¼–ç å’Œ GUI æ™ºèƒ½ä½“ä»£ç†åœ¨å†…çš„æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸Šï¼Œå…¶ç»“æœä¸ Gemini-2.5-Flash ç­‰é—­æºæ¨¡å‹ç›¸æ¯”ï¼Œå±•ç°å‡ºç«äº‰åŠ›ç”šè‡³æ›´ä¼˜ã€‚åŒæ—¶ï¼Œè§„æ¨¡è¾ƒå°çš„ GLM-4.1V-9B-Thinking ä¹Ÿä¿æŒäº†æå¼ºçš„ç«äº‰åŠ›ï¼Œåœ¨ 29 ä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¼˜äºè§„æ¨¡æ›´å¤§çš„ Qwen2.5-VL-72B çš„ç»“æœã€‚"
[site/ai]: https://z.ai/ "Z.ai"
[app-chat.wui/.site]: https://chat.z.ai/ "â€œHi, I'm Z.aiâ€"
[GLM-TTS-app-main.cloud/.site]: https://audio.z.ai/ "æ‰“ç ´æ–‡å­—è¾¹ç•Œï¼Œâ€œå£°â€ä¸´å…¶å¢ƒ // Voice That Brings Words Alive. /// - è¯­éŸ³åˆæˆ // Voice Synthesis /// - éŸ³è‰²å…‹éš† // Voice Cloning"
[GLM-TTS-app-voicelib.cloud/.site]: https://audio.z.ai/voices "(: cloud app :) éŸ³è‰²åº“ // Voice Library /// é»˜è®¤éŸ³è‰² // Default Voices /// - æ´»æ³¼å¥³å£° // Lila ///: ä¸€ä½æ´»æ³¼ã€å¼€æœ—ä¸”æ™®é€šè¯æ ‡å‡†çš„å¥³æ€§å£°éŸ³ // A cheerful, standard-pronunciation female voice /// - é€šç”¨ç”·å£° // Ethan ///: ä¸€ä½é˜³å…‰ã€çˆ½æœ—ä¸”æ™®é€šè¯æ ‡å‡†çš„ç”·æ€§å£°éŸ³ // A sunny, standard-pronunciation male voice /// - æ¸©æŸ”å¥³å£° // Chloe ///: ä¸€ä½æ¸©æŸ”ã€çŸ¥æ€§ä¸”æ™®é€šè¯æ ‡å‡†çš„å¥³æ€§å£°éŸ³ // A gentle, elegant, intelligent female voice"
[GLM-TTS.src/gh]: https://github.com/zai-org/GLM-TTS.git "(Apache-2.0) (Languages: Python 99.6%, Shell 0.4%) GLM-TTS: Controllable & Emotion-Expressive Zero-shot TTS with Multi-Reward Reinforcement Learning // GLM-TTSï¼šå¯æ§ä¸”èƒ½è¡¨è¾¾æƒ…æ„Ÿçš„é›¶æ ·æœ¬ TTSï¼Œå…·æœ‰å¤šå¥–åŠ±å¼ºåŒ–å­¦ä¹ åŠŸèƒ½ /// Model Introduction // æ¨¡å‹ä»‹ç» /// GLM-TTS is a high-quality text-to-speech (TTS) synthesis system based on large language models, supporting zero-shot voice cloning and streaming inference. This system adopts a two-stage architecture: first, it uses LLM to generate speech token sequences, then uses Flow model to convert tokens into high-quality audio waveforms. By introducing a Multi-Reward Reinforcement Learning framework, GLM-TTS can generate more expressive and emotional speech, significantly improving the expressiveness of traditional TTS systems. // GLM-TTS æ˜¯ä¸€æ¬¾åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„é«˜è´¨é‡æ–‡æœ¬è½¬è¯­éŸ³ (TTS) åˆæˆç³»ç»Ÿï¼Œæ”¯æŒé›¶æ ·æœ¬è¯­éŸ³å…‹éš†å’Œæµå¼æ¨ç†ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨ä¸¤é˜¶æ®µæ¶æ„ï¼šé¦–å…ˆï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) ç”Ÿæˆè¯­éŸ³æ ‡è®°åºåˆ—ï¼›ç„¶åï¼Œä½¿ç”¨ Flow æ¨¡å‹å°†æ ‡è®°è½¬æ¢ä¸ºé«˜è´¨é‡éŸ³é¢‘æ³¢å½¢ã€‚é€šè¿‡å¼•å…¥å¤šå¥–åŠ±å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ŒGLM-TTS å¯ä»¥ç”Ÿæˆæ›´å…·è¡¨ç°åŠ›å’Œæƒ…æ„Ÿçš„è¯­éŸ³ï¼Œæ˜¾è‘—æå‡äº†ä¼ ç»Ÿ TTS ç³»ç»Ÿçš„è¡¨ç°åŠ›ã€‚ /// Features // ç‰¹å¾ /// - Zero-shot Voice Cloning: Clone any speaker's voice with just 3-10 seconds of prompt audio // é›¶æ­¥éª¤è¯­éŸ³å…‹éš† ï¼šåªéœ€ 3-10 ç§’æç¤ºéŸ³é¢‘å³å¯å…‹éš†ä»»ä½•è¯´è¯è€…çš„å£°éŸ³ /// - RL-enhanced Emotion Control: Achieve more natural emotional expression and prosody control through multi-reward reinforcement learning framework // å¼ºåŒ–å­¦ä¹ å¢å¼ºçš„æƒ…ç»ªæ§åˆ¶ ï¼šé€šè¿‡å¤šå¥–åŠ±å¼ºåŒ–å­¦ä¹ æ¡†æ¶å®ç°æ›´è‡ªç„¶çš„æƒ…ç»ªè¡¨è¾¾å’ŒéŸµå¾‹æ§åˆ¶ /// - Streaming Inference: Support real-time streaming audio generation, suitable for interactive applications // æµå¼æ¨ç† ï¼šæ”¯æŒå®æ—¶æµå¼éŸ³é¢‘ç”Ÿæˆï¼Œé€‚ç”¨äºäº¤äº’å¼åº”ç”¨ /// - High-quality Synthesis: Generate natural and expressive speech with quality comparable to commercial systems // é«˜è´¨é‡è¯­éŸ³åˆæˆ ï¼šç”Ÿæˆè‡ªç„¶æµç•…ã€å¯Œæœ‰è¡¨ç°åŠ›çš„è¯­éŸ³ï¼Œè´¨é‡å¯ä¸å•†ä¸šç³»ç»Ÿåª²ç¾ã€‚ /// - Multi-language Support: Primarily supports Chinese, while also supporting English mixed text // å¤šè¯­è¨€æ”¯æŒ ï¼šä¸»è¦æ”¯æŒä¸­æ–‡ï¼ŒåŒæ—¶ä¹Ÿæ”¯æŒè‹±æ–‡æ··åˆæ–‡æœ¬ã€‚ /// - Phoneme-level Modeling: Support phoneme-level text-to-speech conversion // éŸ³ç´ çº§å»ºæ¨¡ ï¼šæ”¯æŒéŸ³ç´ çº§æ–‡æœ¬è½¬è¯­éŸ³è½¬æ¢ /// - Flexible Inference Methods: Support multiple sampling strategies and inference modes // çµæ´»çš„æ¨ç†æ–¹æ³• ï¼šæ”¯æŒå¤šç§é‡‡æ ·ç­–ç•¥å’Œæ¨ç†æ¨¡å¼ (arxiv: 2512.14291)"
[GLM-TTS-model.tensor/hf]: https://huggingface.co/zai-org/GLM-TTS "(MIT) (Inference Providers: Text-to-Speech) (Topics: Text-to-Speech, Safetensors, Chinese, English) (Tags: llm, tts, zero-shot, voice-cloning, reinforcement-learning, flow-matching) GLM-TTS: Controllable & Emotion-Expressive Zero-shot TTS // GLM-TTSï¼šå¯æ§ä¸”èƒ½è¡¨è¾¾æƒ…æ„Ÿçš„é›¶é‡‡æ · TTS (arxiv: 2512.14291)"
[GLM-TTS.paper/arxiv]: https://arxiv.org/abs/2512.14291 "(License: CC BY 4.0) [Submitted on 16 Dec 2025] { Sound (cs.SD) } (doi: 10.48550/arXiv.2512.14291) GLM-TTS Technical Report // GLM-TTS æŠ€æœ¯æŠ¥å‘Š /// This work proposes GLM-TTS, a production-level TTS system designed for efficiency, controllability, and high-fidelity speech generation. GLM-TTS follows a two-stage architecture, consisting of a text-to-token autoregressive model and a token-to-waveform diffusion model. With only 100k hours of training data, GLM-TTS achieves state-of-the-art performance on multiple open-source benchmarks. To meet production requirements, GLM-TTS improves speech quality through an optimized speech tokenizer with fundamental frequency constraints and a GRPO-based multi-reward reinforcement learning framework that jointly optimizes pronunciation, speaker similarity, and expressive prosody. In parallel, the system enables efficient and controllable deployment via parameter-efficient LoRA-based voice customization and a hybrid phoneme-text input scheme that provides precise pronunciation control. Our code is available at this https URL (. gh: zai-org/GLM-TTS.git). Real-time speech synthesis demos are provided via this http URL (. web: z.ai) (this http URL (. web: audio.z.ai)), the Zhipu Qingyan app/web (this http URL (. web: chatglm.cn)). // æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º GLM-TTS çš„ç”Ÿäº§çº§æ–‡æœ¬è½¬è¯­éŸ³ (TTS) ç³»ç»Ÿï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆã€å¯æ§ä¸”é«˜ä¿çœŸçš„è¯­éŸ³ç”Ÿæˆã€‚GLM-TTS é‡‡ç”¨ä¸¤é˜¶æ®µæ¶æ„ï¼ŒåŒ…å«ä¸€ä¸ªæ–‡æœ¬åˆ°è¯å…ƒçš„è‡ªå›å½’æ¨¡å‹å’Œä¸€ä¸ªè¯å…ƒåˆ°æ³¢å½¢çš„æ‰©æ•£æ¨¡å‹ã€‚ä»…éœ€ 10 ä¸‡å°æ—¶çš„è®­ç»ƒæ•°æ®ï¼ŒGLM-TTS å³å¯åœ¨å¤šä¸ªå¼€æºåŸºå‡†æµ‹è¯•ä¸­å–å¾—æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸ºäº†æ»¡è¶³ç”Ÿäº§éœ€æ±‚ï¼ŒGLM-TTS é€šè¿‡ä¼˜åŒ–çš„è¯­éŸ³åˆ†è¯å™¨ï¼ˆå…·æœ‰åŸºé¢‘çº¦æŸï¼‰å’ŒåŸºäº GRPO çš„å¤šå¥–åŠ±å¼ºåŒ–å­¦ä¹ æ¡†æ¶æ¥æå‡è¯­éŸ³è´¨é‡ï¼Œè¯¥æ¡†æ¶è”åˆä¼˜åŒ–äº†å‘éŸ³ã€è¯´è¯äººç›¸ä¼¼åº¦å’Œå¯Œæœ‰è¡¨ç°åŠ›çš„éŸµå¾‹ã€‚åŒæ—¶ï¼Œè¯¥ç³»ç»Ÿé€šè¿‡å‚æ•°é«˜æ•ˆçš„åŸºäº LoRa çš„è¯­éŸ³å®šåˆ¶å’Œæ··åˆéŸ³ç´ -æ–‡æœ¬è¾“å…¥æ–¹æ¡ˆå®ç°äº†é«˜æ•ˆä¸”å¯æ§çš„éƒ¨ç½²ï¼Œä»è€Œæä¾›ç²¾ç¡®çš„å‘éŸ³æ§åˆ¶ã€‚"
[AutoGLM.site/.site]: https://autoglm.z.ai/blog/ "AutoGLM Goes Open Source // AutoGLM å¼€æº /// Unlocking the AI Phone for Everyone // è®©æ¯ä¸ªäººéƒ½èƒ½ä½¿ç”¨äººå·¥æ™ºèƒ½æ‰‹æœº /// ### I. What exactly have we been trying to achieve? // ä¸€ã€æˆ‘ä»¬ç©¶ç«Ÿæƒ³è¦å®ç°ä»€ä¹ˆç›®æ ‡ï¼Ÿ /// For a long time, we have been obsessed with answering one single question: // é•¿æœŸä»¥æ¥ï¼Œæˆ‘ä»¬ä¸€ç›´æ‰§ç€äºå›ç­”ä¸€ä¸ªé—®é¢˜ï¼š ///: If AI is truly an â€œAssistantâ€, can it actually pick up a phone like a human and finish a task from start to finish? // å¦‚æœäººå·¥æ™ºèƒ½çœŸçš„èƒ½æˆä¸ºâ€œåŠ©æ‰‹â€ï¼Œå®ƒçœŸçš„èƒ½åƒäººä¸€æ ·æ‹¿èµ·æ‰‹æœºï¼Œä»å¤´åˆ°å°¾å®Œæˆä¸€é¡¹ä»»åŠ¡å—ï¼Ÿ /// In our vision, AI shouldn't just live inside a chat box. It should step out and walk into the Apps you actually use every day: // åœ¨æˆ‘ä»¬çœ‹æ¥ï¼Œäººå·¥æ™ºèƒ½ä¸åº”è¯¥ä»…ä»…å­˜åœ¨äºèŠå¤©æ¡†ä¸­ã€‚å®ƒåº”è¯¥èµ°å‡ºèŠå¤©æ¡†ï¼Œèå…¥ä½ æ¯å¤©å®é™…ä½¿ç”¨çš„å„ç§åº”ç”¨ç¨‹åºä¸­ï¼š /// - It should help you navigate a food delivery app, from selection to checkout; // å®ƒåº”è¯¥èƒ½å¸®åŠ©ä½ é¡ºåˆ©ä½¿ç”¨å¤–å–åº”ç”¨ç¨‹åºï¼Œä»é€‰é¤åˆ°ç»“è´¦ï¼› /// - It should manage your cloud phone, batch-processing notifications, likes, and comments; // å®ƒå¯ä»¥ç®¡ç†ä½ çš„äº‘ç«¯æ‰‹æœºï¼Œæ‰¹é‡å¤„ç†é€šçŸ¥ã€ç‚¹èµå’Œè¯„è®ºï¼› /// - It should handle the sales, customer service, and attendance workflowsâ€”automatically finishing those repetitive actions you dread doing yourself. // å®ƒåº”è¯¥èƒ½å¤Ÿå¤„ç†é”€å”®ã€å®¢æˆ·æœåŠ¡å’Œè€ƒå‹¤å·¥ä½œæµç¨‹â€”â€”è‡ªåŠ¨å®Œæˆé‚£äº›ä½ è®¨åŒè‡ªå·±åšçš„é‡å¤æ€§å·¥ä½œã€‚ /// This is what AutoGLM is about: Teaching AI the true art of \"Device Agency\". // è¿™å°±æ˜¯ AutoGLM çš„æ„ä¹‰æ‰€åœ¨ï¼š æ•™ä¼š AI çœŸæ­£çš„â€œè®¾å¤‡è‡ªä¸»æ€§â€è‰ºæœ¯ã€‚ /// ### II. 32 Months: From Chaos to Control // äºŒã€32ä¸ªæœˆï¼šä»æ··ä¹±åˆ°æŒæ§ /// To put it simply: We wanted AutoGLM to not just â€œspeakâ€, but to â€œactâ€. // ç®€å•æ¥è¯´ï¼š æˆ‘ä»¬å¸Œæœ› AutoGLM ä¸ä»…èƒ½â€œè¯´è¯â€ï¼Œè¿˜èƒ½â€œè¡ŒåŠ¨â€ã€‚ /// To make that sentence a reality, we started from scratch in April 2023â€”back when most people hadn't even heard of Large Language Modelsâ€”and spent 32 months exploring every single detail. // ä¸ºäº†å®ç°è¿™ä¸€æ„¿æ™¯ï¼Œæˆ‘ä»¬ä» 2023 å¹´ 4 æœˆå¼€å§‹ï¼Œå½“æ—¶å¤§å¤šæ•°äººç”šè‡³è¿˜æ²¡æœ‰å¬è¯´è¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶èŠ±äº† 32 ä¸ªæœˆçš„æ—¶é—´æ¢ç´¢æ¯ä¸€ä¸ªç»†èŠ‚ã€‚ /// #### 1. From â€œRandom Tapsâ€ to â€œPrecision Controlâ€ // 1. ä»â€œéšæœºæ•²å‡»â€åˆ°â€œç²¾å‡†æ§åˆ¶â€ /// In our earliest versions, the system built on the large model only understood basic actions like â€œtapâ€ or â€œswipeâ€. It could occasionally finish a short workflow, but more often, it would get lost in nonsensical operations or fall into infinite loops. // åœ¨æˆ‘ä»¬æœ€æ—©çš„ç‰ˆæœ¬ä¸­ï¼ŒåŸºäºå¤§å‹æ¨¡å‹æ„å»ºçš„ç³»ç»Ÿåªèƒ½ç†è§£â€œç‚¹å‡»â€æˆ–â€œæ»‘åŠ¨â€ç­‰åŸºæœ¬æ“ä½œã€‚å®ƒå¶å°”å¯ä»¥å®Œæˆä¸€ä¸ªç®€çŸ­çš„å·¥ä½œæµç¨‹ï¼Œä½†æ›´å¤šæ—¶å€™ï¼Œå®ƒä¼šé™·å…¥æ— æ„ä¹‰çš„æ“ä½œæˆ–æ— é™å¾ªç¯ä¸­ã€‚ /// To fix this, we spent nearly a year mapping out every possible failure. We tried to turn those â€œclumsy handsâ€ into â€œcapable handsâ€: // ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬èŠ±äº†å°†è¿‘ä¸€å¹´çš„æ—¶é—´ï¼Œæ¢³ç†äº†æ¯ä¸€ç§å¯èƒ½å‡ºç°çš„æ•…éšœã€‚æˆ‘ä»¬åŠªåŠ›æŠŠé‚£äº›â€œç¬¨æ‰‹ç¬¨è„šâ€çš„å®¶ä¼™å˜æˆâ€œèƒ½å¹²çš„å®¶ä¼™â€ï¼š /// - We built a comprehensive Phone Use capability framework; // æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„æ‰‹æœºä½¿ç”¨èƒ½åŠ›æ¡†æ¶ ï¼› /// - We abstracted the most fundamental actions: clicking, scrolling, typing, taking screenshots, and UI understanding; // æˆ‘ä»¬æç‚¼å‡ºäº†æœ€åŸºæœ¬çš„æ“ä½œï¼šç‚¹å‡»ã€æ»šåŠ¨ã€è¾“å…¥ã€æˆªå±å’Œç”¨æˆ·ç•Œé¢ç†è§£ï¼› /// - We taught the model to decompose a natural language command into a stable, replayable sequence of operations; // æˆ‘ä»¬æ•™ä¼šè¯¥æ¨¡å‹å°†è‡ªç„¶è¯­è¨€å‘½ä»¤åˆ†è§£ä¸ºç¨³å®šçš„ã€å¯é‡æ”¾çš„æ“ä½œåºåˆ—ï¼› /// - We taught it to navigate the â€œenvironmental frictionâ€ of the real world: network jitters, intrusive pop-ups, and the chaos of ad overlays. // æˆ‘ä»¬æ•™ä¼šå®ƒå¦‚ä½•åº”å¯¹ç°å®ä¸–ç•Œä¸­çš„ â€œç¯å¢ƒæ‘©æ“¦â€ ï¼šç½‘ç»œæŠ–åŠ¨ã€çƒ¦äººçš„å¼¹å‡ºçª—å£å’Œæ··ä¹±çš„å¹¿å‘Šå åŠ ã€‚ /// On October 25, 2024, we released the first AutoGLM capable of stably completing a full operation chain on a real device. It was regarded by the industry as the world's first AI Agent with true Phone Use capabilities. // 2024 å¹´ 10 æœˆ 25 æ—¥ ï¼Œæˆ‘ä»¬å‘å¸ƒäº†é¦–æ¬¾èƒ½å¤Ÿåœ¨çœŸæœºä¸Šç¨³å®šå®Œæˆå®Œæ•´æ“ä½œé“¾çš„ AutoGLMã€‚å®ƒè¢«ä¸šç•Œèª‰ä¸ºå…¨çƒé¦–æ¬¾å…·å¤‡çœŸæ­£æ‰‹æœºä½¿ç”¨èƒ½åŠ›çš„ AI æ™ºèƒ½ä½“ã€‚ /// #### 2. The First â€œDigital Cash Giftâ€ Sent by AI // 2. äººå·¥æ™ºèƒ½å‘é€çš„é¦–ä»½â€œæ•°å­—ç°é‡‘ç¤¼ç‰©â€ /// In November 2024, AutoGLM achieved a milestone: it sent the first AI-automated â€Red Packetâ€œ (digital cash gift) in human history. // 2024 å¹´ 11 æœˆ ï¼ŒAutoGLM å®ç°äº†ä¸€ä¸ªé‡Œç¨‹ç¢‘ï¼šå®ƒå‘é€äº†äººç±»å†å²ä¸Šç¬¬ä¸€ä¸ª AI è‡ªåŠ¨å‘é€çš„â€œçº¢åŒ…â€ï¼ˆæ•°å­—ç°é‡‘ç¤¼ç‰©ï¼‰ã€‚ /// It wasn't a script, and it wasn't an internal API call. It was the AI â€œseeingâ€ the screen, â€œunderstandingâ€ the context, and clicking through the banking interface step-by-step. // è¿™ä¸æ˜¯è„šæœ¬ï¼Œä¹Ÿä¸æ˜¯å†…éƒ¨ API è°ƒç”¨ã€‚è€Œæ˜¯äººå·¥æ™ºèƒ½â€œçœ‹åˆ°â€å±å¹•ï¼Œâ€œç†è§£â€ä¸Šä¸‹æ–‡ï¼Œç„¶åä¸€æ­¥ä¸€æ­¥åœ°ç‚¹å‡»å®Œæˆé“¶è¡Œç•Œé¢æ“ä½œã€‚ /// To us, this was a signal: From now on, many interactions on your phone can finally be fully handed over to AI. // å¯¹æˆ‘ä»¬æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªä¿¡å·ï¼š ä»ç°åœ¨å¼€å§‹ï¼Œæ‰‹æœºä¸Šçš„è®¸å¤šäº¤äº’æœ€ç»ˆå¯ä»¥å®Œå…¨äº¤ç»™äººå·¥æ™ºèƒ½æ¥å®Œæˆã€‚ /// #### 3. Moving to the Cloud: A Safer Sandbox // 3. è¿ç§»åˆ°äº‘ç«¯ï¼šæ›´å®‰å…¨çš„æ²™ç®± /// In 2025, we released AutoGLM 2.0. We validated the scaling laws of Reinforcement Learning (RL), introducing MobileRL, ComputerRL, and AgentRL algorithms. This allowed AutoGLM to learn simultaneously across thousands of virtual environments, drastically expanding the Agent's accuracy and generalization capabilities. // 2025 å¹´ï¼Œæˆ‘ä»¬å‘å¸ƒäº† AutoGLM 2.0 ã€‚æˆ‘ä»¬éªŒè¯äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ‰©å±•è§„å¾‹ï¼Œå¹¶å¼•å…¥äº† MobileRLã€ComputerRL å’Œ AgentRL ç®—æ³•ã€‚è¿™ä½¿å¾— AutoGLM èƒ½å¤ŸåŒæ—¶åœ¨æ•°åƒä¸ªè™šæ‹Ÿç¯å¢ƒä¸­è¿›è¡Œå­¦ä¹ ï¼Œæå¤§åœ°æå‡äº†æ™ºèƒ½ä½“çš„å‡†ç¡®ç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚ /// More importantly, we didn't want the Agent to operate recklessly on a user's real phone or personal WeChat. So, we chose to place it in a virtual phone, detached from the user's physical reality: // æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›è¯¥æ™ºèƒ½ä½“ç¨‹åºåœ¨ç”¨æˆ·çš„çœŸå®æ‰‹æœºæˆ–ä¸ªäººå¾®ä¿¡ä¸Šéšæ„è¿è¡Œã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€‰æ‹©å°†å…¶æ”¾ç½®åœ¨è™šæ‹Ÿæ‰‹æœºä¸­ï¼Œä½¿å…¶ä¸ç”¨æˆ·çš„ç‰©ç†ç°å®è„±é’©ï¼š /// - This phone runs in the cloud; // è¿™æ¬¾æ‰‹æœºè¿è¡Œåœ¨äº‘ç«¯ï¼› /// - Every action can be replayed, audited, and intervened upon; // æ¯ä¸€ä¸ªè¡Œä¸ºéƒ½å¯ä»¥è¢«é‡ç°ã€å®¡æ ¸å’Œå¹²é¢„ï¼› /// - Truly sensitive data remains strictly isolated. // çœŸæ­£æ•æ„Ÿçš„æ•°æ®ä¼šè¢«ä¸¥æ ¼éš”ç¦»ã€‚ /// The intuition behind this design is simple: Before AI learns to use a phone, we must ensure it doesn't reach into places it shouldn't touch. // è¿™ç§è®¾è®¡èƒŒåçš„ç›´è§‰å¾ˆç®€å•ï¼š åœ¨äººå·¥æ™ºèƒ½å­¦ä¼šä½¿ç”¨æ‰‹æœºä¹‹å‰ï¼Œæˆ‘ä»¬å¿…é¡»ç¡®ä¿å®ƒä¸ä¼šè§¦ç¢°åˆ°ä¸åº”è¯¥è§¦ç¢°çš„åœ°æ–¹ã€‚"
[AutoGLM.src/gh]: https://github.com/zai-org/Open-AutoGLM.git "(Apache-2.0) (Languages: Python 100.0%) An Open Phone Agent Model & Framework. Unlocking the AI Phone for Everyone // å¼€æ”¾å¼ç”µè¯æ™ºèƒ½ä½“æ¨¡å‹å’Œæ¡†æ¶ã€‚è®©æ¯ä¸ªäººéƒ½èƒ½ä½¿ç”¨äººå·¥æ™ºèƒ½ç”µè¯ã€‚ /// Phone Agent æ˜¯ä¸€ä¸ªåŸºäº AutoGLM æ„å»ºçš„æ‰‹æœºç«¯æ™ºèƒ½åŠ©ç†æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿä»¥å¤šæ¨¡æ€æ–¹å¼ç†è§£æ‰‹æœºå±å¹•å†…å®¹ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨åŒ–æ“ä½œå¸®åŠ©ç”¨æˆ·å®Œæˆä»»åŠ¡ã€‚ç³»ç»Ÿé€šè¿‡ ADB(Android Debug Bridge)æ¥æ§åˆ¶è®¾å¤‡ï¼Œä»¥è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå±å¹•æ„ŸçŸ¥ï¼Œå†ç»“åˆæ™ºèƒ½è§„åˆ’èƒ½åŠ›ç”Ÿæˆå¹¶æ‰§è¡Œæ“ä½œæµç¨‹ã€‚ç”¨æˆ·åªéœ€ç”¨è‡ªç„¶è¯­è¨€æè¿°éœ€æ±‚ï¼Œå¦‚â€œæ‰“å¼€å°çº¢ä¹¦æœç´¢ç¾é£Ÿâ€ï¼ŒPhone Agent å³å¯è‡ªåŠ¨è§£ææ„å›¾ã€ç†è§£å½“å‰ç•Œé¢ã€è§„åˆ’ä¸‹ä¸€æ­¥åŠ¨ä½œå¹¶å®Œæˆæ•´ä¸ªæµç¨‹ã€‚ç³»ç»Ÿè¿˜å†…ç½®æ•æ„Ÿæ“ä½œç¡®è®¤æœºåˆ¶ï¼Œå¹¶æ”¯æŒåœ¨ç™»å½•æˆ–éªŒè¯ç åœºæ™¯ä¸‹è¿›è¡Œäººå·¥æ¥ç®¡ã€‚åŒæ—¶ï¼Œå®ƒæä¾›è¿œç¨‹ ADB è°ƒè¯•èƒ½åŠ›ï¼Œå¯é€šè¿‡ WiFi æˆ–ç½‘ç»œè¿æ¥è®¾å¤‡ï¼Œå®ç°çµæ´»çš„è¿œç¨‹æ§åˆ¶ä¸å¼€å‘ã€‚ // Phone Agent is a mobile intelligent assistant framework built on AutoGLM. It understands phone screen content in a multimodal manner and helps users complete tasks through automated operations. The system controls devices via ADB (Android Debug Bridge), perceives screens using vision-language models, and generates and executes operation workflows through intelligent planning. Users simply describe their needs in natural language, such as \"Open eBay and search for wireless earphones.\" and Phone Agent will automatically parse the intent, understand the current interface, plan the next action, and complete the entire workflow. The system also includes a sensitive operation confirmation mechanism and supports manual takeover during login or verification code scenarios. Additionally, it provides remote ADB debugging capabilities, allowing device connection via WiFi or network for flexible remote control and development."
[AutoGLM-Phone-9B-model.tensor/hf]: https://huggingface.co/zai-org/AutoGLM-Phone-9B "(MIT) (Model size: 934k params; Tensor type: BF16; Inference Providers: Image-Text-to-Text) (Topics: Image-Text-to-Text, Transformers, Safetensors, Chinese) (Tags: glm4v, any-to-any, agent, conversational)  (arxiv: 2509.18119, 2411.00820) (Model tree: hf:zai-org/GLM-4-9B-0414 -> hf:zai-org/GLM-4.1V-9B-Base -> .)"
[AutoGLM-Phone-9B-Multilingual-model.tensor/hf]: https://huggingface.co/zai-org/AutoGLM-Phone-9B-Multilingual "(MIT) (Model size: 934k params; Tensor type: BF16; Inference Providers: Image-Text-to-Text) (Topics: Image-Text-to-Text, Transformers, Safetensors, Chinese) (Tags: glm4v, any-to-any, agent, conversational)  (arxiv: 2509.18119, 2411.00820) (Model tree: hf:zai-org/GLM-4-9B-0414 -> hf:zai-org/GLM-4.1V-9B-Base -> .)"
[AutoGLM-2411.00820.paper/arxiv]: https://arxiv.org/abs/2411.00820 "(License: CC BY 4.0) [Submitted on 28 Oct 2024] (arXiv: 2411.00820 [cs.HC]) { Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) } (doi: 10.48550/arXiv.2411.00820) AutoGLM: Autonomous Foundation Agents for GUIs /// AutoGLMï¼šç”¨äº GUI çš„è‡ªä¸»åŸºç¡€æ™ºèƒ½ä½“ /// We present AutoGLM, a new series in the ChatGLM family, designed to serve as foundation agents for autonomous control of digital devices through Graphical User Interfaces (GUIs). While foundation models excel at acquiring human knowledge, they often struggle with decision-making in dynamic real-world environments, limiting their progress toward artificial general intelligence. This limitation underscores the importance of developing foundation agents capable of learning through autonomous environmental interactions by reinforcing existing models. Focusing on Web Browser and Phone as representative GUI scenarios, we have developed AutoGLM as a practical foundation agent system for real-world GUI interactions. Our approach integrates a comprehensive suite of techniques and infrastructures to create deployable agent systems suitable for user delivery. Through this development, we have derived two key insights: First, the design of an appropriate \"intermediate interface\" for GUI control is crucial, enabling the separation of planning and grounding behaviors, which require distinct optimization for flexibility and accuracy respectively. Second, we have developed a novel progressive training framework that enables self-evolving online curriculum reinforcement learning for AutoGLM. Our evaluations demonstrate AutoGLM's effectiveness across multiple domains. For web browsing, AutoGLM achieves a 55.2% success rate on VAB-WebArena-Lite (improving to 59.1% with a second attempt) and 96.2% on OpenTable evaluation tasks. In Android device control, AutoGLM attains a 36.2% success rate on AndroidLab (VAB-Mobile) and 89.7% on common tasks in popular Chinese APPs. // æˆ‘ä»¬æ¨å‡ºäº† ChatGLM ç³»åˆ—çš„æ–°æˆå‘˜ AutoGLMï¼Œæ—¨åœ¨ä½œä¸ºåŸºç¡€ä»£ç†ï¼Œé€šè¿‡å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰å®ç°å¯¹æ•°å­—è®¾å¤‡çš„è‡ªä¸»æ§åˆ¶ã€‚åŸºç¡€æ¨¡å‹è™½ç„¶æ“…é•¿è·å–äººç±»çŸ¥è¯†ï¼Œä½†åœ¨åŠ¨æ€çš„çœŸå®ç¯å¢ƒä¸­è¿›è¡Œå†³ç­–æ—¶å¾€å¾€åŠ›ä¸ä»å¿ƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å‘é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAAIï¼‰å‘å±•çš„è¿›ç¨‹ã€‚è¿™ä¸€å±€é™æ€§å‡¸æ˜¾äº†å¼€å‘èƒ½å¤Ÿé€šè¿‡è‡ªä¸»ç¯å¢ƒäº¤äº’è¿›è¡Œå­¦ä¹ çš„åŸºç¡€ä»£ç†çš„é‡è¦æ€§ï¼Œè€Œè¿™ç§å­¦ä¹ æ˜¯é€šè¿‡å¼ºåŒ–ç°æœ‰æ¨¡å‹å®ç°çš„ã€‚æˆ‘ä»¬ä»¥ Web æµè§ˆå™¨å’Œæ‰‹æœºä½œä¸ºå…¸å‹çš„ GUI åœºæ™¯ï¼Œå¼€å‘äº† AutoGLMï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªé€‚ç”¨äºçœŸå®ä¸–ç•Œ GUI äº¤äº’çš„å®ç”¨åŸºç¡€ä»£ç†ç³»ç»Ÿã€‚æˆ‘ä»¬çš„æ–¹æ³•é›†æˆäº†ä¸€å¥—å…¨é¢çš„æŠ€æœ¯å’ŒåŸºç¡€è®¾æ–½ï¼Œä»¥åˆ›å»ºå¯éƒ¨ç½²çš„ä»£ç†ç³»ç»Ÿï¼Œå¹¶äº¤ä»˜ç»™ç”¨æˆ·ã€‚é€šè¿‡è¿™é¡¹å¼€å‘ï¼Œæˆ‘ä»¬è·å¾—äº†ä¸¤ä¸ªå…³é”®è§è§£ï¼šé¦–å…ˆï¼Œè®¾è®¡ä¸€ä¸ªåˆé€‚çš„â€œä¸­é—´ç•Œé¢â€å¯¹äº GUI æ§åˆ¶è‡³å…³é‡è¦ï¼Œå®ƒèƒ½å¤Ÿå°†è§„åˆ’è¡Œä¸ºå’Œå®é™…æ“ä½œè¡Œä¸ºåˆ†ç¦»ï¼Œè€Œè¿™ä¸¤ç§è¡Œä¸ºåˆ†åˆ«éœ€è¦é’ˆå¯¹çµæ´»æ€§å’Œå‡†ç¡®æ€§è¿›è¡Œä¸åŒçš„ä¼˜åŒ–ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°é¢–çš„æ¸è¿›å¼è®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ”¯æŒ AutoGLM çš„è‡ªæ¼”åŒ–åœ¨çº¿è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒAutoGLM åœ¨å¤šä¸ªé¢†åŸŸå‡è¡¨ç°å‡ºè‰²ã€‚åœ¨ç½‘é¡µæµè§ˆæ–¹é¢ï¼ŒAutoGLM åœ¨ VAB-WebArena-Lite ä¸Šçš„æˆåŠŸç‡ä¸º 55.2%ï¼ˆç¬¬äºŒæ¬¡å°è¯•åæå‡è‡³ 59.1%ï¼‰ï¼Œåœ¨ OpenTable è¯„ä¼°ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡ä¸º 96.2%ã€‚åœ¨ Android è®¾å¤‡æ§åˆ¶æ–¹é¢ï¼ŒAutoGLM åœ¨ AndroidLab (VAB-Mobile) ä¸Šçš„æˆåŠŸç‡ä¸º 36.2%ï¼Œåœ¨çƒ­é—¨ä¸­å›½ APP çš„å¸¸è§ä»»åŠ¡ä¸­çš„æˆåŠŸç‡ä¸º 89.7%ã€‚"
[MobileRL-2509.18119.paper/arxiv]: https://arxiv.org/abs/2509.18119 "(License: CC BY 4.0) [Submitted on 10 Sep 2025 (v1), last revised 24 Oct 2025 (this version, v2)] (arXiv:2509.18119 [cs.LG]) { Machine Learning (cs.LG); Artificial Intelligence (cs.AI) } (doi: 10.48550/arXiv.2509.18119) MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents // MobileRLï¼šé¢å‘ç§»åŠ¨ GUI æ™ºèƒ½ä½“çš„åœ¨çº¿æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  /// Building general-purpose graphical user interface (GUI) agents has become increasingly promising with the progress in vision language models. However, developing effective mobile GUI agents with reinforcement learning (RL) remains challenging due to the heavy-tailed distribution of task difficulty and the inefficiency of large-scale environment sampling. We present an online agentic reinforcement learning framework MobileRL to enhance GUI agents in mobile environments. Its core component is the Difficulty-ADAptive GRPO (ADAGRPO) algorithm. In ADAGRPO, we design difficulty-adaptive positive replay and failure curriculum filtering to adapt the model to different task difficulties. We introduce the shortest-path reward adjustment strategy to reshape rewards concerning the task length in multi-turn agentic tasks. Those strategies jointly stabilize RL training, improve sample efficiency, and generate strong performance across diverse mobile apps and tasks. We apply MOBILERL to two open models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B model achieves state-of-the-art results in terms of success rates on both AndroidWorld (80.2%) and AndroidLab (53.6%). The MOBILERL framework is open-sourced at: this https URL (. gh: THUDM/MobileRL.git). // éšç€è§†è§‰è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œæ„å»ºé€šç”¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†å˜å¾—è¶Šæ¥è¶Šæœ‰å‰æ™¯ã€‚ç„¶è€Œï¼Œç”±äºä»»åŠ¡éš¾åº¦åˆ†å¸ƒçš„é‡å°¾ç‰¹æ€§ä»¥åŠå¤§è§„æ¨¡ç¯å¢ƒé‡‡æ ·çš„ä½æ•ˆæ€§ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¼€å‘é«˜æ•ˆçš„ç§»åŠ¨ GUI ä»£ç†ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨çº¿ä»£ç†å¼ºåŒ–å­¦ä¹ æ¡†æ¶ MobileRLï¼Œæ—¨åœ¨å¢å¼ºç§»åŠ¨ç¯å¢ƒä¸‹çš„ GUI ä»£ç†æ€§èƒ½ã€‚å…¶æ ¸å¿ƒç»„ä»¶æ˜¯éš¾åº¦è‡ªé€‚åº” GRPOï¼ˆADAGRPOï¼‰ç®—æ³•ã€‚åœ¨ ADAGRPO ä¸­ï¼Œæˆ‘ä»¬è®¾è®¡äº†éš¾åº¦è‡ªé€‚åº”çš„æ­£å‘å›æ”¾å’Œå¤±è´¥è¯¾ç¨‹è¿‡æ»¤æœºåˆ¶ï¼Œä»¥ä½¿æ¨¡å‹é€‚åº”ä¸åŒçš„ä»»åŠ¡éš¾åº¦ã€‚æˆ‘ä»¬å¼•å…¥äº†æœ€çŸ­è·¯å¾„å¥–åŠ±è°ƒæ•´ç­–ç•¥ï¼Œæ ¹æ®å¤šè½®ä»£ç†ä»»åŠ¡ä¸­çš„ä»»åŠ¡é•¿åº¦è°ƒæ•´å¥–åŠ±ã€‚è¿™äº›ç­–ç•¥å…±åŒä½œç”¨ï¼Œç¨³å®šäº†å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæé«˜äº†é‡‡æ ·æ•ˆç‡ï¼Œå¹¶åœ¨å„ç§ç§»åŠ¨åº”ç”¨å’Œä»»åŠ¡ä¸­å®ç°äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚æˆ‘ä»¬å°† MobileRL åº”ç”¨äºä¸¤ä¸ªå¼€æºæ¨¡å‹ï¼ˆQwen2.5-VL-7B-Instruct å’Œ GLM-4.1V-9B-Baseï¼‰ã€‚æœ€ç»ˆå¾—åˆ°çš„ MOBILERL-9B æ¨¡å‹åœ¨ AndroidWorld (80.2%) å’Œ AndroidLab (53.6%) ä¸Šçš„æˆåŠŸç‡å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚"
[MobileRL.src/gh]: https://github.com/THUDM/MobileRL.git "(MIT) (Languages: Python 99.3%, Other 0.7%) MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents // MobileRLï¼šé¢å‘ç§»åŠ¨ GUI æ™ºèƒ½ä½“çš„åœ¨çº¿æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  /// TL;DR. We introduce MobileRL, an online agentic reinforcement learning framework that turns general-purpose vision-language models into strong mobile GUI agents. By combining a staged reasoning warm-up with difficulty-adaptive online RL, MobileRL achieves state-of-the-art success rates on AndroidWorld and AndroidLab. // ç®€è€Œè¨€ä¹‹ï¼Œ æˆ‘ä»¬æ¨å‡ºäº† MobileRLï¼Œä¸€ä¸ªåœ¨çº¿æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå°†é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹è½¬åŒ–ä¸ºå¼ºå¤§çš„ç§»åŠ¨ GUI æ™ºèƒ½ä½“ã€‚MobileRL ç»“åˆäº†åˆ†é˜¶æ®µæ¨ç†é¢„çƒ­å’Œéš¾åº¦è‡ªé€‚åº”çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œåœ¨ AndroidWorld å’Œ AndroidLab ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æˆåŠŸç‡ã€‚ /// Abstract /// Building general-purpose graphical user interface (GUI) agents has become increasingly promising with the progress in vision language models. However, developing effective mobile GUI agents with reinforcement learning (RL) remains challenging due to the heavy-tailed distribution of task difficulty and the inefficiency of large-scale environment sampling. We present an online agentic reinforcement learning framework MobileRL to enhance GUI agents in mobile environments. Its core component is the Difficulty-ADAptive GRPO (ADAGRPO) algorithm. In ADAGRPO, we design difficulty-adaptive positive replay and failure curriculum filtering to adapt the model to different task difficulties. We introduce the shortest-path reward adjustment strategy to reshape rewards concerning the task length in multi-turn agentic tasks. Those strategies jointly stabilize RL training, improve sample efficiency, and generate strong performance across diverse mobile apps and tasks. We apply MOBILERL to two open models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B model achieves state-of-the-art results in terms of success rates on both AndroidWorld (80.2%) and AndroidLab (53.6%). // éšç€è§†è§‰è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œæ„å»ºé€šç”¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†å˜å¾—è¶Šæ¥è¶Šæœ‰å‰æ™¯ã€‚ç„¶è€Œï¼Œç”±äºä»»åŠ¡éš¾åº¦åˆ†å¸ƒçš„é‡å°¾ç‰¹æ€§ä»¥åŠå¤§è§„æ¨¡ç¯å¢ƒé‡‡æ ·çš„ä½æ•ˆæ€§ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¼€å‘é«˜æ•ˆçš„ç§»åŠ¨ GUI ä»£ç†ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨çº¿ä»£ç†å¼ºåŒ–å­¦ä¹ æ¡†æ¶ MobileRLï¼Œæ—¨åœ¨å¢å¼ºç§»åŠ¨ç¯å¢ƒä¸‹çš„ GUI ä»£ç†æ€§èƒ½ã€‚å…¶æ ¸å¿ƒç»„ä»¶æ˜¯éš¾åº¦è‡ªé€‚åº” GRPOï¼ˆADAGRPOï¼‰ç®—æ³•ã€‚åœ¨ ADAGRPO ä¸­ï¼Œæˆ‘ä»¬è®¾è®¡äº†éš¾åº¦è‡ªé€‚åº”çš„æ­£å‘å›æ”¾å’Œå¤±è´¥è¯¾ç¨‹è¿‡æ»¤æœºåˆ¶ï¼Œä»¥ä½¿æ¨¡å‹é€‚åº”ä¸åŒçš„ä»»åŠ¡éš¾åº¦ã€‚æˆ‘ä»¬å¼•å…¥äº†æœ€çŸ­è·¯å¾„å¥–åŠ±è°ƒæ•´ç­–ç•¥ï¼Œæ ¹æ®å¤šè½®ä»£ç†ä»»åŠ¡ä¸­çš„ä»»åŠ¡é•¿åº¦è°ƒæ•´å¥–åŠ±ã€‚è¿™äº›ç­–ç•¥å…±åŒä½œç”¨ï¼Œç¨³å®šäº†å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæé«˜äº†é‡‡æ ·æ•ˆç‡ï¼Œå¹¶åœ¨å„ç§ç§»åŠ¨åº”ç”¨å’Œä»»åŠ¡ä¸­å®ç°äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚æˆ‘ä»¬å°† MobileRL åº”ç”¨äºä¸¤ä¸ªå¼€æºæ¨¡å‹ï¼ˆQwen2.5-VL-7B-Instruct å’Œ GLM-4.1V-9B-Baseï¼‰ã€‚ç”±æ­¤å¾—åˆ°çš„ MOBILERL-9B æ¨¡å‹åœ¨ AndroidWorld (80.2%) å’Œ AndroidLab (53.6%) ä¸Šçš„æˆåŠŸç‡å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚ /// Method /// Mobile GUI agents must follow complex instructions, reason over cluttered screens, and act under sparse, delayed rewardsâ€”all while task difficulty is heavy-tailed and environment sampling is expensive. // ç§»åŠ¨ GUI ä»£ç†å¿…é¡»éµå¾ªå¤æ‚çš„æŒ‡ä»¤ï¼Œåœ¨æ‚ä¹±çš„å±å¹•ä¸Šè¿›è¡Œæ¨ç†ï¼Œå¹¶åœ¨ç¨€ç–ã€å»¶è¿Ÿçš„å¥–åŠ±ä¸‹é‡‡å–è¡ŒåŠ¨â€”â€”æ‰€æœ‰è¿™äº›éƒ½æ˜¯åœ¨ä»»åŠ¡éš¾åº¦å…·æœ‰é‡å°¾åˆ†å¸ƒä¸”ç¯å¢ƒé‡‡æ ·æˆæœ¬é«˜æ˜‚çš„æƒ…å†µä¸‹å®Œæˆçš„ã€‚ /// MobileRL addresses these challenges with a two-stage recipe: // MobileRL é€šè¿‡ä¸¤é˜¶æ®µæ–¹æ¡ˆåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼š /// Reasoning Warm-up: // æ¨ç†çƒ­èº«ï¼š /// 1. reasoning-free sft on large expert data. // åŸºäºå¤§å‹ä¸“å®¶æ•°æ®çš„æ— æ¨ç† SFT ã€‚ /// 1. - reasoning sft to inject and polish rationale-driven planning and transparency. // æ¨ç† sft æ³¨å…¥å¹¶å®Œå–„ç†æ€§é©±åŠ¨çš„è§„åˆ’å’Œé€æ˜åº¦ã€‚ /// 2. Online Agentic RL (Difficultyâ€“Adaptive GRPO, AdaGRPO): // åœ¨çº¿æ™ºèƒ½å¼ºåŒ–å­¦ä¹ ï¼ˆéš¾åº¦è‡ªé€‚åº” GRPOï¼ŒAdaGRPOï¼‰ï¼š /// 2. - Adaptive Positive Replay (AdaPR): store high-quality trajectories and re-use them efficiently. // è‡ªé€‚åº”æ­£å‘å›æ”¾ï¼ˆAdaPRï¼‰ï¼š å­˜å‚¨é«˜è´¨é‡è½¨è¿¹å¹¶é«˜æ•ˆåœ°é‡å¤ä½¿ç”¨å®ƒä»¬ã€‚ /// 2. - Failure Curriculum Filtering (FCF): prune low-quality rollouts and focus learning on actionable tasks. // å¤±è´¥è¯¾ç¨‹ç­›é€‰ï¼ˆFCFï¼‰ï¼š å‰”é™¤ä½è´¨é‡çš„æ¨å¹¿æ´»åŠ¨ï¼Œå¹¶å°†å­¦ä¹ é‡ç‚¹æ”¾åœ¨å¯æ“ä½œçš„ä»»åŠ¡ä¸Šã€‚ /// 2. - Shortest-Path Reward Adjustment (SPA): reward shaping that stabilizes credit assignment for long-horizon interactions. // æœ€çŸ­è·¯å¾„å¥–åŠ±è°ƒæ•´ï¼ˆSPAï¼‰ï¼š ä¸€ç§å¥–åŠ±å¡‘é€ æ–¹æ³•ï¼Œç”¨äºç¨³å®šé•¿æœŸäº¤äº’çš„ä¿¡ç”¨åˆ†é…ã€‚ /// Citation /// (~ @misc{xu2025mobilerlonlineagenticreinforcement, title={MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents}, author={Yifan Xu and Xiao Liu and Xinghan Liu and Jiaqi Fu and Hanchen Zhang and Bohao Jing and Shudan Zhang and Yuting Wang and Wenyi Zhao and Yuxiao Dong}, year={2025}, eprint={2509.18119}, archivePrefix={arXiv}, primaryClass={cs.LG}, url={https://arxiv.org/abs/2509.18119} }) (~ @misc{xu2024androidlabtrainingsystematicbenchmarking, title={AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents}, author={Yifan Xu and Xiao Liu and Xueqiao Sun and Siyi Cheng and Hao Yu and Hanyu Lai and Shudan Zhang and Dan Zhang and Jie Tang and Yuxiao Dong}, year={2024}, eprint={2410.24024}, archivePrefix={arXiv}, primaryClass={cs.AI}, url={https://arxiv.org/abs/2410.24024} })"
[MobileRL-model.tensor/hf]: https://huggingface.co/xuyifan/MobileRL-9B "(Apache-2.0) (Model size: 10B params; Tensor type: BF16) (Topics: Safetensors, English) (Tags: glm4v, agent) (arxiv: 2509.18119) (Model tree: hf:zai-org/GLM-4-9B-0414 -> hf:zai-org/GLM-4.1V-9B-Base -> .)"
[AndroidLab-2410.24024.paper/arxiv]: https://arxiv.org/abs/2410.24024 "(License: CC BY 4.0) [Submitted on 31 Oct 2024 (v1), last revised 4 Nov 2024 (this version, v2)] { Artificial Intelligence (cs.AI) } (doi: 10.48550/arXiv.2410.24024) AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents // AndroidLabï¼šAndroid è‡ªä¸»ä»£ç†çš„è®­ç»ƒå’Œç³»ç»ŸåŒ–åŸºå‡†æµ‹è¯• /// Autonomous agents have become increasingly important for interacting with the real world. Android agents, in particular, have been recently a frequently-mentioned interaction method. However, existing studies for training and evaluating Android agents lack systematic research on both open-source and closed-source models. In this work, we propose AndroidLab as a systematic Android agent framework. It includes an operation environment with different modalities, action space, and a reproducible benchmark. It supports both large language models (LLMs) and multimodal models (LMMs) in the same action space. AndroidLab benchmark includes predefined Android virtual devices and 138 tasks across nine apps built on these devices. By using the AndroidLab environment, we develop an Android Instruction dataset and train six open-source LLMs and LMMs, lifting the average success rates from 4.59% to 21.50% for LLMs and from 1.93% to 13.28% for LMMs. AndroidLab is open-sourced and publicly available at this https URL (. gh: THUDM/Android-Lab.git). // è‡ªä¸»ä»£ç†åœ¨ä¸ç°å®ä¸–ç•Œäº¤äº’æ–¹é¢å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç‰¹åˆ«æ˜¯ Android ä»£ç†ï¼Œè¿‘å¹´æ¥å·²æˆä¸ºä¸€ç§å¤‡å—å…³æ³¨çš„äº¤äº’æ–¹å¼ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ Android ä»£ç†è®­ç»ƒå’Œè¯„ä¼°ç ”ç©¶ç¼ºä¹å¯¹å¼€æºå’Œé—­æºæ¨¡å‹çš„ç³»ç»Ÿæ€§ç ”ç©¶ã€‚æœ¬æ–‡æå‡ºäº† AndroidLabï¼Œä¸€ä¸ªç³»ç»ŸåŒ–çš„ Android ä»£ç†æ¡†æ¶ã€‚å®ƒåŒ…å«ä¸€ä¸ªå…·æœ‰ä¸åŒæ¨¡æ€çš„æ“ä½œç¯å¢ƒã€ä¸€ä¸ªåŠ¨ä½œç©ºé—´ä»¥åŠä¸€ä¸ªå¯å¤ç°çš„åŸºå‡†æµ‹è¯•ã€‚å®ƒæ”¯æŒåœ¨åŒä¸€åŠ¨ä½œç©ºé—´ä¸­è¿è¡Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ã€‚AndroidLab åŸºå‡†æµ‹è¯•åŒ…å«é¢„å®šä¹‰çš„ Android è™šæ‹Ÿè®¾å¤‡ä»¥åŠåŸºäºè¿™äº›è®¾å¤‡æ„å»ºçš„ä¹ä¸ªåº”ç”¨ç¨‹åºçš„ 138 ä¸ªä»»åŠ¡ã€‚åˆ©ç”¨ AndroidLab ç¯å¢ƒï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ª Android æŒ‡ä»¤æ•°æ®é›†ï¼Œå¹¶è®­ç»ƒäº†å…­ä¸ªå¼€æºçš„ LLM å’Œ LMM æ¨¡å‹ï¼Œå°† LLM çš„å¹³å‡æˆåŠŸç‡ä» 4.59%æå‡è‡³ 21.50%ï¼Œå°† LMM çš„å¹³å‡æˆåŠŸç‡ä» 1.93%æå‡è‡³ 13.28%ã€‚"
[AndroidLab.src/gh]: https://github.com/THUDM/Android-Lab.git "(MIT) (Languages: Python 100.0%) AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents // AndroidLabï¼šAndroid è‡ªä¸»ä»£ç†çš„è®­ç»ƒå’Œç³»ç»ŸåŒ–åŸºå‡†æµ‹è¯• /// We develop a systematic Android agent frameworkï¼Œnamed AndroidLab. It includes an operation environment and a reproducible benchmark. AndroidLab benchmark includes predefined Android virtual devices and 138 tasks across nine apps built on these devices. // æˆ‘ä»¬å¼€å‘äº†ä¸€å¥—åä¸º AndroidLab çš„ç³»ç»ŸåŒ– Android ä»£ç†æ¡†æ¶ã€‚å®ƒåŒ…å«ä¸€ä¸ªè¿è¡Œç¯å¢ƒå’Œä¸€ä¸ªå¯å¤ç°çš„åŸºå‡†æµ‹è¯•ã€‚AndroidLab åŸºå‡†æµ‹è¯•åŒ…å«é¢„å®šä¹‰çš„ Android è™šæ‹Ÿè®¾å¤‡ï¼Œä»¥åŠæ„å»ºåœ¨è¿™äº›è®¾å¤‡ä¸Šçš„ä¹ä¸ªåº”ç”¨ç¨‹åºçš„ 138 ä¸ªä»»åŠ¡ã€‚ /// This repository is the code framework for the operation environment and benchmark section. We provide two execution modes: AVD on Mac (arm64) and Docker on Linux (x86_64). You can freely add or modify new tasks or Android images according to our framework. We offer a complete evaluation framework that can be used to assess the performance of various Android agents. // æœ¬ä»“åº“æ˜¯è¿è¡Œç¯å¢ƒå’ŒåŸºå‡†æµ‹è¯•éƒ¨åˆ†çš„ä»£ç æ¡†æ¶ã€‚æˆ‘ä»¬æä¾›ä¸¤ç§æ‰§è¡Œæ¨¡å¼ï¼šMac ä¸Šçš„ AVDï¼ˆarm64ï¼‰å’Œ Linux ä¸Šçš„ Dockerï¼ˆx86_64ï¼‰ã€‚æ‚¨å¯ä»¥æ ¹æ®æˆ‘ä»¬çš„æ¡†æ¶è‡ªç”±æ·»åŠ æˆ–ä¿®æ”¹æ–°çš„ä»»åŠ¡æˆ– Android é•œåƒã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„è¯„ä¼°æ¡†æ¶ï¼Œå¯ç”¨äºè¯„ä¼°å„ç§ Android ä»£ç†çš„æ€§èƒ½ã€‚ /// We have also open-sourced the Android Instruct dataset mentioned in the paper. Please refer to here (. gh: . @main/docs/instruction_tuning.md) for more details. // æˆ‘ä»¬è¿˜å¼€æºäº†è®ºæ–‡ä¸­æåˆ°çš„ Android Instruct æ•°æ®é›†ã€‚è¯¦æƒ…è¯·å‚è§æ­¤å¤„ ã€‚ /// Benchmark Components // åŸºå‡†ç»„ä»¶ /// In our experiment, we utilized a range of apps to conduct various tests. The following mobile apps are chosen: // åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ç³»åˆ—åº”ç”¨ç¨‹åºæ¥è¿›è¡Œå„ç§æµ‹è¯•ã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬é€‰æ‹©çš„ç§»åŠ¨åº”ç”¨ç¨‹åºï¼š /// - Bluecoins: A personal finance management app used for tracking expenses and income. // Bluecoins ï¼šä¸€æ¬¾ç”¨äºè·Ÿè¸ªæ”¯å‡ºå’Œæ”¶å…¥çš„ä¸ªäººè´¢åŠ¡ç®¡ç†åº”ç”¨ç¨‹åºã€‚ /// - Calendar: A calendar app helps in organizing schedules and setting reminders. // æ—¥å† ï¼šæ—¥å†åº”ç”¨ç¨‹åºæœ‰åŠ©äºå®‰æ’æ—¥ç¨‹å’Œè®¾ç½®æé†’ã€‚ /// - Cantook: An e-book reader for storing, managing, and reading e-books. // Cantook ï¼šä¸€æ¬¾ç”¨äºå­˜å‚¨ã€ç®¡ç†å’Œé˜…è¯»ç”µå­ä¹¦çš„ç”µå­ä¹¦é˜…è¯»å™¨ã€‚ /// - Clock: A clock app for displaying the time, setting alarms, and using a stopwatch. // æ—¶é’Ÿ ï¼šä¸€æ¬¾ç”¨äºæ˜¾ç¤ºæ—¶é—´ã€è®¾ç½®é—¹é’Ÿå’Œä½¿ç”¨ç§’è¡¨çš„æ—¶é’Ÿåº”ç”¨ç¨‹åºã€‚ /// - Contacts: A contact management app for storing and organizing contact information. // è”ç³»äºº ï¼šä¸€æ¬¾ç”¨äºå­˜å‚¨å’Œæ•´ç†è”ç³»äººä¿¡æ¯çš„è”ç³»äººç®¡ç†åº”ç”¨ç¨‹åºã€‚ /// - Maps.me: An offline map app for navigation and exploring locations. // Maps.me ï¼šä¸€æ¬¾ç”¨äºå¯¼èˆªå’Œæ¢ç´¢åœ°ç‚¹çš„ç¦»çº¿åœ°å›¾åº”ç”¨ç¨‹åºã€‚ /// - PiMusic: A music player app for organizing and playing locally stored music files. // PiMusic ï¼šä¸€æ¬¾ç”¨äºæ•´ç†å’Œæ’­æ”¾æœ¬åœ°å­˜å‚¨çš„éŸ³ä¹æ–‡ä»¶çš„éŸ³ä¹æ’­æ”¾å™¨åº”ç”¨ç¨‹åºã€‚ /// - Settings: A settings app for configuring device settings and preferences. // è®¾ç½® ï¼šç”¨äºé…ç½®è®¾å¤‡è®¾ç½®å’Œåå¥½çš„åº”ç”¨ç¨‹åºã€‚ /// - Zoom: A video conferencing app for hosting and joining online meetings. // Zoom ï¼šä¸€æ¬¾ç”¨äºä¸»æŒå’Œå‚åŠ åœ¨çº¿ä¼šè®®çš„è§†é¢‘ä¼šè®®åº”ç”¨ç¨‹åºã€‚ /// The selection of these apps underwent multiple iterations to ensure their suitability for our evaluation purposes. A key criterion for the final selection was that each app must function independently, without requiring an internet connection or user account login. This ensures that the evaluations can be consistently replicated under the same conditions, eliminating external dependencies and reducing the risk of privacy breaches. Consequently, this approach maintains the reliability and reproducibility of our results. // ä¸ºäº†ç¡®ä¿è¿™äº›åº”ç”¨ç¨‹åºç¬¦åˆæˆ‘ä»¬çš„è¯„ä¼°ç›®çš„ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¤šæ¬¡ç­›é€‰ã€‚æœ€ç»ˆå…¥é€‰çš„å…³é”®æ ‡å‡†æ˜¯æ¯ä¸ªåº”ç”¨ç¨‹åºå¿…é¡»èƒ½å¤Ÿç‹¬ç«‹è¿è¡Œï¼Œæ— éœ€ç½‘ç»œè¿æ¥æˆ–ç”¨æˆ·ç™»å½•ã€‚è¿™ç¡®ä¿äº†è¯„ä¼°å¯ä»¥åœ¨ç›¸åŒæ¡ä»¶ä¸‹è¿›è¡Œï¼Œä»è€Œæ¶ˆé™¤å¤–éƒ¨ä¾èµ–æ€§å¹¶é™ä½éšç§æ³„éœ²çš„é£é™©ã€‚å› æ­¤ï¼Œè¿™ç§æ–¹æ³•ä¿è¯äº†æˆ‘ä»¬ç»“æœçš„å¯é æ€§å’Œå¯å¤ç°æ€§ã€‚ /// Citation /// (~ @misc{xu2024androidlabtrainingsystematicbenchmarking, title={AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents}, author={Yifan Xu and Xiao Liu and Xueqiao Sun and Siyi Cheng and Hao Yu and Hanyu Lai and Shudan Zhang and Dan Zhang and Jie Tang and Yuxiao Dong}, year={2024}, eprint={2410.24024}, archivePrefix={arXiv}, primaryClass={cs.AI}, url={https://arxiv.org/abs/2410.24024} })"
[cn:site/cn]: https://zhipuai.cn "æ™ºè°±æ˜¯ç”±æ¸…åå¤§å­¦è®¡ç®—æœºç³»æŠ€æœ¯æˆæœè½¬åŒ–è€Œæ¥çš„å…¬å¸ï¼Œè‡´åŠ›äºæ‰“é€ æ–°ä¸€ä»£è®¤çŸ¥æ™ºèƒ½é€šç”¨æ¨¡å‹ã€‚"
[cn:app-chat:GLM.wui/cn]: https://chatglm.cn/ "æ™ºè°±æ¸…è¨€: åŸºäºGLMæ¨¡å‹å¼€å‘, æ”¯æŒå¤šè½®å¯¹è¯, å…·å¤‡å†…å®¹åˆ›ä½œã€ä¿¡æ¯å½’çº³æ€»ç»“ç­‰èƒ½åŠ›"
[cn:app-chat:AutoGLM.wui/cn]: https://autoglm.zhipuai.cn/ "æƒ³åšç‚¹ä»€ä¹ˆï¼Ÿ /// AutoGLM æ˜¯æ™ºè°±AIæ¨å‡ºçš„Agentæ™ºèƒ½ä½“ /// å®ƒä¸ºæ‚¨éƒ¨ç½²äº†ä¸€å°æ™ºèƒ½ä½“æ‰‹æœºä¸æ™ºèƒ½ä½“ç”µè„‘ã€‚ /// åªéœ€å‘å¸ƒæŒ‡ä»¤ï¼Œå®ƒå°±èƒ½åœ¨äº‘ç«¯å¸®æ‚¨æ‰§è¡ŒAppæ“ä½œã€ç½‘é¡µä»»åŠ¡å’Œè·¨åº”ç”¨å·¥ä½œæµã€‚ /// æ”¯æŒä¸€å¥è¯å®ç°ã€Œäº‘ç«¯æ“ä½œ+è‡ªåŠ¨æ‰§è¡Œã€ /// æœ‰å®ƒçš„ååŠ©ï¼Œæ‚¨å¯ä»¥å°†ç¹æ‚çš„åº”ç”¨ä¸å†—é•¿çš„æ“ä½œéƒ½æ”¾è‡³äº‘ç«¯å¤„ç†ã€‚ /// æ‰‹æœºç˜¦èº«ï¼Œäººç±»å‡è´Ÿã€‚è®¾å¤‡å°†é€€å›å·¥å…·çš„ä½ç½®ï¼Œè€Œç”Ÿæ´»å°†å›åˆ°ä½ æ‰‹ä¸­ã€‚"

