[src/gh]: https://github.com/OpenMOSS/MOSS.git "(Code: Apache-2.0; Model: AGPL-3.0) (Languages: Python 100.0%) An open-source tool-augmented conversational language model from Fudan University // 来自复旦大学的开源工具增强型对话语言模型"
[blog:intro/ghio:txsun1997]: https://txsun1997.github.io/blogs/moss.html "Authors: Tianxiang Sun (txsun19@fudan.edu.cn) and Xipeng Qiu (xpqiu@fudan.edu.cn), Fudan University // 作者：孙天翔（txsun19@fudan.edu.cn）和邱锡鹏（xpqiu@fudan.edu.cn），复旦大学 /// Contributors: Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Hang Yan, Xiangyang Liu, Yunfan Shao, Qiong Tang, Xingjian Zhao, Ke Chen, Yining Zheng, Zhejian Zhou, Ruixiao Li, Jun Zhan, Yunhua Zhou, Linyang Li, Xiaogui Yang, Lingling Wu, Zhangyue Yin, Xuanjing Huang, Xipeng Qiu // 贡献者：孙天翔、张晓天、何正富、李鹏、程庆元、严航、刘向阳、邵云帆、唐琼、赵兴健、陈科、郑一宁、周哲健、李瑞晓、战俊、周云华、李林阳、杨晓鬼、吴玲玲、尹张悦、黄宣静、邱锡鹏 /// Acknowledgement: TensorChord & Mosec // 致谢：TensorChord & Mosec /// Released on Feb 20, 2023. // 2023 年 2 月 20 日发布。"
[paper/springer]: https://link.springer.com/article/10.1007/s11633-024-1502-8 "Conversational large language models (LLMs) such as ChatGPT and GPT-4 have recently exhibited remarkable capabilities across various domains, capturing widespread attention from the public. To facilitate this line of research, in this paper, we report the development of MOSS, an open-sourced conversational LLM that contains 16 B parameters and can perform a variety of instructions in multi-turn interactions with humans. The base model of MOSS is pre-trained on large-scale unlabeled English, Chinese, and code data. To optimize the model for dialogue, we generate 1.1 M synthetic conversations based on user prompts collected through our earlier versions of the model API. We then perform preference-aware training on preference data annotated from AI feedback. Evaluation results on real-world use cases and academic benchmarks demonstrate the effectiveness of the proposed approaches. In addition, we present an effective practice to augment MOSS with several external tools. Through the development of MOSS, we have established a complete technical roadmap for large language models from pre-training, supervised fine-tuning to alignment, verifying the feasibility of chatGPT under resource-limited conditions and providing a reference for both the academic and industrial communities. Model weights and code are publicly available at https://github.com/OpenMOSS/MOSS. // 对话式大型语言模型（LLMs）如 ChatGPT 和 GPT-4 最近在各个领域展现出卓越的能力，引起了公众的广泛关注。为了促进这一领域的研究，在本文中，我们报告了 MOSS 的开发，MOSS 是一个开源的对话式 LLM，包含 16B 参数，能够在多轮人机交互中执行各种指令。MOSS 的基础模型在大规模未标记的英语、中文和代码数据上进行预训练。为了优化模型用于对话，我们基于通过我们早期版本的模型 API 收集的用户提示生成 110 万条合成对话。然后我们对从 AI 反馈中标注的偏好数据进行偏好感知训练。在真实用例和学术基准上的评估结果证明了所提出方法的有效性。此外，我们还展示了一种通过多个外部工具增强 MOSS 的有效方法。 在 MOSS 的开发过程中，我们建立了一套完整的大语言模型技术路线图，涵盖预训练、监督微调和对齐等阶段，验证了在资源受限条件下 chatGPT 的可行性，并为学术界和工业界提供了参考。模型权重和代码已公开发布在 https://github.com/OpenMOSS/MOSS。"
[try/fastnlp.top]: https://moss.fastnlp.top/ "Try MOSS // 体验 MOSS"
