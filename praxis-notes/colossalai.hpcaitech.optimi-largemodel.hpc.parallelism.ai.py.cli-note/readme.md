[src/gh]: https://github.com/hpcaitech/ColossalAI.git "(Apache-2.0) (Languages: Python 93.3%, Cuda 2.5%, HTML 1.9%, C++ 1.2%, Shell 0.9%, C 0.2%) Making large AI models cheaper, faster and more accessible // 让大型人工智能模型更便宜、更快捷、更易于使用 /// Colossal-AI // 巨型人工智能 /// Skip the setup. Access a powerful, pre-configured Colossal-AI environment on HPC-AI Cloud. // 无需设置。即可在 HPC-AI 云上访问功能强大、预配置的 Colossal-AI 环境。 /// Train your models and scale your AI workload in one click! // 一键训练模型并扩展您的 AI 工作负载！ /// - NVIDIA Blackwell B200s: Experience the next generation of AI performance (See Benchmarks). Now available on cloud from $2.47/hr. // NVIDIA Blackwell B200s ：体验新一代 AI 性能（ 参见基准测试 ）。现已推出云端版本，价格低至 2.47 美元/小时 。 /// - Cost-Effective H200 Cluster: Get premier performance with on-demand rental from just $1.99/hr. // 经济实惠的 H200 集群 ：按需租赁， 每小时仅需 1.99 美元 ，即可获得卓越的性能。 /// Features // 特征 /// Colossal-AI provides a collection of parallel components for you. We aim to support you to write your distributed deep learning models just like how you write your model on your laptop. We provide user-friendly tools to kickstart distributed training and inference in a few lines. // Colossal-AI 为您提供一系列并行组件。我们的目标是帮助您像在笔记本电脑上编写模型一样编写分布式深度学习模型。我们提供用户友好的工具，只需几行代码即可快速启动分布式训练和推理。 /// - Parallelism strategies // 平行策略 /// - - Data Parallelism // 数据并行 /// - - Pipeline Parallelism // 管道并行 /// - - 1D, 2D (. arxiv: 2104.05343), 2.5D (. arxiv: 2105.14500), 3D (. arxiv: 2105.14450) Tensor Parallelism // 1D, 2D, 2.5D, 3D 张量并行性 /// - - Sequence Parallelism (. arxiv: 2105.13120) // 序列平行性 /// - - Zero Redundancy Optimizer (ZeRO) (. arxiv: 1910.02054) // 零冗余优化器 (ZeRO) /// - - Auto-Parallelism (. arxiv: 2302.02599) // 自动并行 /// - Heterogeneous Memory Management // 异构内存管理 /// - - PatrickStar (. arxiv: 2108.05818) // 派大星 /// - Friendly Usage // 友好使用 /// - - Parallelism based on the configuration file // 基于配置文件的并行性 (arxiv: 2110.14883, 2302.02599)"
[site/org]: https://colossalai.org/ "Unmatched Speed and Scale // 无与伦比的速度和规模 /// Learn about the distributed techniques of Colossal-AI to maximize the runtime performance of your large neural networks. // 了解 Colossal-AI 的分布式技术，以最大限度地提高大型神经网络的运行性能。"
[cli.pip/pypi]: https://pypi.org/project/colossalai/ "(: pip install -- colossalai) (License: Apache Software License (Apache Software License 2.0)) (Requires: Python >=3.6) An integrated large-scale model training system with efficient parallelization techniques // 具有高效并行化技术的集成式大规模模型训练系统 (src: gh:hpcaitech/ColossalAI.git)"
[cli.oci/dockerhub]: https://hub.docker.com/r/hpcaitech/colossalai "(: docker pull -- docker.io/hpcaitech/colossalai)"
[paper-2110.14883/arxiv]: https://arxiv.org/abs/2110.14883 "[Submitted on 28 Oct 2021 (v1), last revised 5 Oct 2023 (this version, v3)] { Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Distributed, Parallel, and Cluster Computing (cs.DC) } (doi: 10.48550/arXiv.2110.14883) Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training // Colossal-AI：用于大规模并行训练的统一深度学习系统 /// The success of Transformer models has pushed the deep learning model scale to billions of parameters. Due to the limited memory resource of a single GPU, However, the best practice for choosing the optimal parallel strategy is still lacking, since it requires domain expertise in both deep learning and parallel computing. // Transformer 模型的成功推动深度学习模型的参数规模达到数十亿。然而，由于单个 GPU 的内存资源有限，目前仍缺乏选择最佳并行策略的最佳实践，因为这需要同时具备深度学习和并行计算领域的专业知识。 /// The Colossal-AI system addressed the above challenge by introducing a unified interface to scale your sequential code of model training to distributed environments. It supports parallel training methods such as data, pipeline, tensor, and sequence parallelism, as well as heterogeneous training methods integrated with zero redundancy optimizer. Compared to the baseline system, Colossal-AI can achieve up to 2.76 times training speedup on large-scale models. // Colossal-AI 系统通过引入统一接口来应对上述挑战，从而将模型训练的顺序代码扩展到分布式环境。它支持数据并行、流水线并行、张量并行和序列并行等并行训练方法，以及集成了零冗余优化器的异构训练方法。与基线系统相比，Colossal-AI 在大规模模型上可实现高达 2.76 倍的训练速度提升。"
[paper-2104.05343/arxiv]: https://arxiv.org/abs/2104.05343 "[Submitted on 12 Apr 2021] { Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC) } (doi: 10.48550/arXiv.2104.05343) An Efficient 2D Method for Training Super-Large Deep Learning Models // 一种高效的二维方法用于训练超大型深度学习模型 /// Huge neural network models have shown unprecedented performance in real-world applications. However, due to memory constraints, model parallelism must be utilized to host large models that would otherwise not fit into the memory of a single device. Previous methods like Megatron partition the parameters of the entire model among multiple devices, while each device has to accommodate the redundant activations in forward and backward pass. In this work, we propose Optimus, a highly efficient and scalable 2D-partition paradigm of model parallelism that would facilitate the training of infinitely large language models. In Optimus, activations are partitioned and distributed among devices, further reducing redundancy. In terms of isoefficiency, Optimus significantly outperforms Megatron. On 64 GPUs of TACC Frontera, Optimus achieves 1.48X speedup for training, 1.78X speedup for inference, and 8X increase in maximum batch size over Megatron. Optimus surpasses Megatron in scaling efficiency by a great margin. // 大型神经网络模型在实际应用中展现了前所未有的性能。然而，由于内存限制，必须利用模型并行来运行大型模型，否则这些模型无法装入单个设备的内存。以往的方法，例如 Megatron，将整个模型的参数划分到多个设备上，而每个设备都必须处理前向传播和反向传播中的冗余激活值。本文提出了一种高效且可扩展的二维划分模型并行范式 Optimus，它能够促进无限大型语言模型的训练。在 Optimus 中，激活值被划分并分布到各个设备上，从而进一步减少了冗余。在等效效率方面，Optimus 显著优于 Megatron。在 TACC Frontera 的 64 个 GPU 上，Optimus 的训练速度比 Megatron 快 1.48 倍，推理速度快 1.78 倍，最大批处理大小也提高了 8 倍。Optimus 在扩展效率方面也远超 Megatron。"
[paper-2104.05343.src/gh]: https://github.com/xuqifan897/Optimus.git "(Languages: Python 91.0%, C++ 7.9%, Other 1.1%) Optimus // 擎天柱"
[paper-2105.14500/arxiv]: https://arxiv.org/abs/2105.14500 "[Submitted on 30 May 2021 (v1), last revised 1 Sep 2022 (this version, v2)] { Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) } (doi: 10.48550/arXiv.2105.14500, 10.1145/3545008.3545087 (. ~web: dl.acm.org/doi/10.1145/3545008.3545087)) Tesseract: Parallelize the Tensor Parallelism Efficiently // Tesseract：高效并行化张量并行 /// Together with the improvements in state-of-the-art accuracies of various tasks, deep learning models are getting significantly larger. However, it is extremely difficult to implement these large models because limited GPU memory makes it impossible to fit large models into a single GPU or even a GPU server. Besides, it is highly necessary to reduce the training time for large models. Previous methods like Megatron-LM implemented a 1-Dimensional distributed method to use GPUs to speed up the training. However, these methods have a high communication overhead and a low scaling efficiency on large-scale clusters. To solve these problems, we propose Tesseract, a highly scalable tensor parallelism with a novel design. It increases efficiency by reducing communication overhead and lowers the memory required for each GPU. By introducing the novel dimension into tensor parallelism, Tesseract greatly increases the memory capacity of tensor parallelism. Concretely, this new dimension furthermore increases the degree of tensor parallelism. Compared to previous 1-D and 2-D methods, Tesseract manages to reduce the communication cost on each layer, resulting in speedups of 1.38x and 1.53x respectively with strong scaling. In weak scaling experiments, Tesseract achieves a maximum of 4.0/1.7 times inference speedup and 3.4/1.7 times throughput improvement compared to 1-D/2-D methods, respectively. By introducing Tesseract, we offer a more efficient and scalable way to implement large deep learning models with limited GPU resources. // 随着各种任务的精度不断提高，深度学习模型的规模也显著增大。然而，由于 GPU 内存有限，大型模型难以加载到单个 GPU 甚至 GPU 服务器上，因此实现这些大型模型极其困难。此外，缩短大型模型的训练时间也至关重要。以往的方法，例如 Megatron-LM，采用了一维分布式方法利用 GPU 加速训练。然而，这些方法通信开销高，且在大规模集群上的扩展效率低。为了解决这些问题，我们提出了 Tesseract，一种具有新颖设计的、高度可扩展的张量并行机制。它通过降低通信开销和减少每个 GPU 所需的内存来提高效率。通过在张量并行中引入新的维度，Tesseract 极大地提升了张量并行的内存容量。具体而言，这一新维度进一步提高了张量并行的程度。与之前的 1D 和 2D 方法相比，Tesseract 成功降低了每一层的通信开销，在强扩展性下分别实现了 1.38 倍和 1.53 倍的加速。在弱扩展性实验中，Tesseract 的推理速度最高可提升 4.0 倍/1.7 倍，吞吐量最高可提升 3.4 倍/1.7 倍（与 1D/2D 方法相比）。Tesseract 的引入为在 GPU 资源有限的情况下实现大型深度学习模型提供了一种更高效、更具可扩展性的方法。"
[paper-2105.14450/arxiv]: https://arxiv.org/abs/2105.14450 "[Submitted on 30 May 2021] { Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG); Performance (cs.PF) } (doi: 10.48550/arXiv.2105.14450) (Comments: Technical Report of NUS HPC-AI Lab (. web: ai.comp.nus.edu.sg). The leading two authors have equal contributions // 新加坡国立大学高性能计算人工智能实验室技术报告。两位主要作者贡献相同。) Maximizing Parallelism in Distributed Training for Huge Neural Networks // 最大化大规模神经网络分布式训练中的并行性 /// The recent Natural Language Processing techniques have been refreshing the state-of-the-art performance at an incredible speed. Training huge language models is therefore an imperative demand in both industry and academy. However, huge language models impose challenges to both hardware and software. Graphical processing units (GPUs) are iterated frequently to meet the exploding demand, and a variety of ASICs like TPUs are spawned. However, there is still a tension between the fast growth of the extremely huge models and the fact that Moore's law is approaching the end. To this end, many model parallelism techniques are proposed to distribute the model parameters to multiple devices, so as to alleviate the tension on both memory and computation. Our work is the first to introduce a 3-dimensional model parallelism for expediting huge language models. By reaching a perfect load balance, our approach presents smaller memory and communication cost than existing state-of-the-art 1-D and 2-D model parallelism. Our experiments on 64 TACC's V100 GPUs show that our 3-D parallelism outperforms the 1-D and 2-D parallelism with 2.32x and 1.57x speedup, respectively. // 近年来，自然语言处理技术以惊人的速度不断刷新着现有技术的性能。因此，训练大型语言模型已成为工业界和学术界的迫切需求。然而，大型语言模型对硬件和软件都提出了挑战。为了满足爆炸式增长的需求，图形处理器（GPU）不断迭代更新，各种专用集成电路（ASIC），例如 TPU，也应运而生。然而，超大型模型的快速增长与摩尔定律即将失效之间仍然存在着矛盾。为此，人们提出了许多模型并行技术，将模型参数分配到多个设备上，以缓解内存和计算资源的压力。我们的工作首次引入了三维模型并行技术，用于加速大型语言模型的训练。通过实现完美的负载均衡，我们的方法比现有的最先进的一维和二维模型并行技术具有更低的内存和通信成本。我们在 64 个 TACC V100 GPU 上进行的实验表明，我们的 3D 并行计算比 1D 并行计算和 2D 并行计算分别快 2.32 倍和 1.57 倍。"
[paper-2105.13120/arxiv]: https://arxiv.org/abs/2105.13120 "[Submitted on 26 May 2021 (v1), last revised 21 May 2022 (this version, v3)] { Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC) } (doi: 10.48550/arXiv.2105.13120) Sequence Parallelism: Long Sequence Training from System Perspective // 序列并行性：从系统角度看长序列训练 /// Transformer achieves promising results on various tasks. However, self-attention suffers from quadratic memory requirements with respect to the sequence length. Existing work focuses on reducing time and space complexity from an algorithm perspective. In this work, we propose sequence parallelism, a memory-efficient parallelism method to help us break input sequence length limitation and train with longer sequences on GPUs efficiently. Our approach is compatible with most existing parallelisms (e.g. data parallelism, pipeline parallelism and tensor parallelism), which means our sequence parallelism makes 4D parallelism possible. More importantly, we no longer require a single device to hold the whole sequence. That is, with sparse attention, our sequence parallelism enables us to train transformer with infinite long sequence. Specifically, we split the input sequence into multiple chunks and feed each chunk into its corresponding device (i.e. GPU). To compute the attention output, we integrated ring-style communication with self-attention calculation and proposed Ring Self-Attention (RSA). Experiments show that sequence parallelism performs well when scaling with batch size and sequence length. Compared with tensor parallelism, our approach achieved 13.7× and 3.0× maximum batch size and sequence length respectively when scaling up to 64 NVIDIA P100 GPUs. With sparse attention, sequence can handle sequence with over 114K tokens, which is over 27× longer than existing sparse attention works holding the whole sequence on a single device. // Transformer 在各种任务上都取得了令人瞩目的成果。然而，自注意力机制的内存需求与序列长度呈平方级关系。现有工作主要从算法角度出发，致力于降低时间和空间复杂度。本文提出了一种内存高效的并行方法——序列并行，以突破输入序列长度的限制，并在 GPU 上高效地训练更长的序列。我们的方法与大多数现有的并行机制（例如数据并行、流水线并行和张量并行）兼容，这意味着我们的序列并行能够实现 4D 并行。更重要的是，我们不再需要单个设备来存储整个序列。也就是说，结合稀疏注意力机制，我们的序列并行能够训练无限长的 Transformer 序列。具体而言，我们将输入序列分割成多个块，并将每个块馈送到其对应的设备（即 GPU）。为了计算注意力输出，我们将环形通信与自注意力计算相结合，提出了环形自注意力（RSA）算法。实验表明，序列并行在处理批量大小和序列长度方面均表现良好。与张量并行相比，我们的方法在扩展到 64 个 NVIDIA P100 GPU 时，分别实现了 13.7× 和 3.0× 的最大批处理大小和序列长度。借助稀疏注意力机制，序列可以处理超过 114K 个 token 的序列，比现有稀疏注意力机制在单个设备上处理整个序列的方法长 27 倍以上。"
[paper-1910.02054/arxiv]: https://arxiv.org/abs/1910.02054 "[Submitted on 4 Oct 2019 (v1), last revised 13 May 2020 (this version, v3)] { Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (stat.ML) } (doi: 10.48550/arXiv.1910.02054) ZeRO: Memory Optimizations Toward Training Trillion Parameter Models // ZeRO：面向训练万亿参数模型的内存优化 /// Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware. // 大型深度学习模型能够显著提升准确率，但训练数十亿乃至数万亿个参数却极具挑战性。现有的解决方案，例如数据并行和模型并行，在将这些模型适配到有限的设备内存中，同时还要兼顾计算、通信和开发效率方面，存在着根本性的局限性。我们开发了一种名为零冗余优化器 (ZeRO) 的全新解决方案，旨在优化内存，从而大幅提升训练速度，并提高可高效训练的模型规模。ZeRO 消除了数据并行和模型并行训练中的内存冗余，同时保持了低通信量和高计算粒度，使我们能够以持续的高效率，根据设备数量按比例扩展模型规模。我们对内存需求和通信量的分析表明：ZeRO 有潜力利用现有硬件，处理超过 1 万亿个参数。 /// We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create the world's largest language model (Turing-NLG, 17B parameters) with record breaking accuracy. // 我们实现了 ZeRO 并对其进行了评估：它在 400 个 GPU 上以超线性加速训练超过 1000 亿参数的大型模型，吞吐量达到 15 Petaflops。这比现有技术在模型规模上提高了 8 倍，在性能上提高了 10 倍。在易用性方面，ZeRO 可以训练高达 130 亿参数的大型模型（例如，大于 Megatron GPT 的 83 亿参数和 T5 的 110 亿参数），而无需使用模型并行化，这对于科学家来说应用起来更加困难。最后，研究人员利用 ZeRO 的系统突破，创建了世界上最大的语言模型（Turing-NLG，170 亿参数），并取得了破纪录的准确率。"
[paper-2302.02599/arxiv]: https://arxiv.org/abs/2302.02599 "[Submitted on 6 Feb 2023 (v1), last revised 22 Feb 2023 (this version, v2)] { Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC) } (doi: 10.48550/arXiv.2302.02599) Colossal-Auto: Unified Automation of Parallelization and Activation Checkpoint for Large-scale Models // Colossal-Auto：大规模模型并行化和激活检查点的统一自动化 /// In recent years, large-scale models have demonstrated state-of-the-art performance across various domains. However, training such models requires various techniques to address the problem of limited computing power and memory on devices such as GPUs. Some commonly used techniques include pipeline parallelism, tensor parallelism, and activation checkpointing. While existing works have focused on finding efficient distributed execution plans (Zheng et al. 2022) and activation checkpoint scheduling (Herrmann et al. 2019, Beaumont et al. 2021}, there has been no method proposed to optimize these two plans jointly. Moreover, ahead-of-time compilation relies heavily on accurate memory and computing overhead estimation, which is often time-consuming and misleading. Existing training systems and machine learning pipelines either physically execute each operand or estimate memory usage with a scaled input tensor. To address these challenges, we introduce a system that can jointly optimize distributed execution and gradient checkpointing plans. Additionally, we provide an easy-to-use symbolic profiler that generates memory and computing statistics for any PyTorch model with a minimal time cost. Our approach allows users to parallelize their model training on the given hardware with minimum code change based. // 近年来，大规模模型在各个领域都展现出了卓越的性能。然而，训练这类模型需要多种技术来解决诸如 GPU 等设备上计算能力和内存有限的问题。一些常用的技术包括流水线并行、张量并行和激活检查点。尽管现有研究主要集中于寻找高效的分布式执行计划（Zheng et al. 2022）和激活检查点调度（Herrmann et al. 2019, Beaumont et al. 2021），但尚未有方法能够同时优化这两个计划。此外，提前编译严重依赖于精确的内存和计算开销估计，而这通常既耗时又容易产生误差。现有的训练系统和机器学习流水线要么实际执行每个操作数，要么使用缩放后的输入张量来估计内存使用情况。为了应对这些挑战，我们提出了一种能够联合优化分布式执行计划和梯度检查点计划的系统。此外，我们还提供了一个易于使用的符号分析器，能够以最小的时间成本为任何 PyTorch 模型生成内存和计算统计信息。我们的方法允许用户在给定硬件上以最小的代码更改实现模型训练的并行化。"
[paper-2108.05818/arxiv]: https://arxiv.org/abs/2108.05818 "[Submitted on 12 Aug 2021 (v1), last revised 1 Aug 2022 (this version, v4)] { Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC) } (doi: 10.48550/arXiv.2108.05818, 10.1109/TPDS.2022.3219819 (. web: ieeexplore.ieee.org/document/9940581)) PatrickStar: Parallel Training of Pre-trained Models via Chunk-based Memory Management // PatrickStar：基于块内存管理的预训练模型并行训练 /// The pre-trained model (PTM) is revolutionizing Artificial Intelligence (AI) technology. However, the hardware requirement of PTM training is prohibitively high, making it a game for a small proportion of people. Therefore, we proposed PatrickStar system to lower the hardware requirements of PTMs and make them accessible to everyone. PatrickStar uses the CPU-GPU heterogeneous memory space to store the model data. Different from existing works, we organize the model data in memory chunks and dynamically distribute them in the heterogeneous memory. Guided by the runtime memory statistics collected in a warm-up iteration, chunks are orchestrated efficiently in heterogeneous memory and generate lower CPU-GPU data transmission volume and higher bandwidth utilization. Symbiosis with the Zero Redundancy Optimizer, PatrickStar scales to multiple GPUs on multiple nodes. % using data parallelism. The system can train tasks on bigger models and larger batch sizes, which cannot be accomplished by existing works. Experimental results show that PatrickStar extends model scales 2.27 and 2.5 times of DeepSpeed, and consistently exhibits significantly higher execution speed. PatricStar also successfully runs the 175B GPT3 training task on a 32 GPU cluster. // 预训练模型 (PTM) 正在革新人工智能 (AI) 技术。然而，PTM 训练对硬件的要求极高，使其仅适用于少数人。因此，我们提出了 PatrickStar 系统，旨在降低 PTM 的硬件要求，使其惠及所有人。PatrickStar 使用 CPU-GPU 异构内存空间来存储模型数据。与现有工作不同，我们将模型数据组织成内存块，并动态地将其分布在异构内存中。在预热迭代中收集的运行时内存统计信息的指导下，数据块在异构内存中得到高效调度，从而降低 CPU-GPU 数据传输量并提高带宽利用率。结合零冗余优化器，PatrickStar 可以利用数据并行性扩展到多个节点上的多个 GPU。该系统能够训练更大模型和更大批次大小的任务，这是现有工作无法实现的。实验结果表明，PatrickStar 的模型规模分别是 DeepSpeed 的 2.27 倍和 2.5 倍，并且始终展现出显著更高的执行速度。 PatricStar 还成功地在 32 个 GPU 的集群上运行了 175B GPT3 训练任务。"
[paper-2108.05818.src/gh]: https://github.com/Tencent/PatrickStar.git "(BSD-3-Clause) (Languages: Python 89.4%, C++ 10.6%) PatrickStar enables Larger, Faster, Greener Pretrained Models for NLP and democratizes AI for everyone. // PatrickStar 为自然语言处理 (NLP) 提供更大、更快、更环保的预训练模型，并使人工智能惠及所有人。 /// PatrickStar: Parallel Training of Large Language Models via a Chunk-based Memory Management // PatrickStar：基于块的内存管理实现大型语言模型的并行训练 /// Meeting PatrickStar // 遇见派大星 /// Pre-Trained Models (PTM) are becoming the hotspot of both NLP research and industry application. However, the training of PTMs requires enormous hardware resources, making it only accessible to a small portion of people in the AI community. Now, PatrickStar will make PTM training available to everyone! // 预训练模型（PTM）正成为自然语言处理（NLP）研究和产业应用的热点。然而，PTM 的训练需要大量的硬件资源，这使得它只有人工智能社区中的一小部分人能够使用。现在， PatrickStar 将让所有人都能进行 PTM 训练！ /// Out-of-memory error (OOM) is the nightmare of every engineer training PTMs. We often have to introduce more GPUs to store the model params to prevent such errors. PatrickStar brings a better solution for such problem. With the heterogeneous training (DeepSpeed Zero Stage 3 also uses it), PatrickStar could fully use both the CPU and GPU memory so that you could use fewer GPUs to train larger models. // 内存溢出错误 (OOM) 是每位训练 PTM 的工程师的噩梦。我们通常需要引入更多 GPU 来存储模型参数以避免此类错误。PatrickStar 为此类问题提供了一个更好的解决方案。借助异构训练 （DeepSpeed Zero Stage 3 也采用了该技术），PatrickStar 可以充分利用 CPU 和 GPU 内存，从而可以使用更少的 GPU 来训练更大的模型。 /// System Design // 系统设计 /// The idea of Patrick is like this. The non-model data (mainly activations) varies during training, but the current heterogeneous training solutions are statically splitting the model data to CPU and GPU. To better use the GPU, PatrickStar proposes a dynamic memory scheduling with the help of a chunk-based memory management module. The memory management of PatrickStar supports offloading everything but the current computing part of the model to the CPU to save GPU. In addition, chunk-based memory management is efficient for collective communication when scaling to multiple GPUs. // Patrick 的理念是这样的：非模型数据（主要是激活值）在训练过程中会发生变化，但当前的异构训练方案却将模型数据静态地分配到 CPU 和 GPU 上。为了更好地利用 GPU，PatrickStar 提出了一种基于块的内存管理模块，并实现了动态内存调度。PatrickStar 的内存管理机制支持将除当前计算部分之外的所有任务卸载到 CPU，从而节省 GPU 资源。此外，基于块的内存管理在扩展到多个 GPU 时，能够有效地进行协同通信。"
[knowsby]: https://huggingface.co/docs/safetensors "Safetensors is being used widely at leading AI enterprises, such as Hugging Face, EleutherAI, and StabilityAI. Here is a non-exhaustive list of projects that are using safetensors: // Safetensors 已被 Hugging Face 、 EleutherAI 和 StabilityAI 等领先的 AI 企业广泛采用。以下列出部分使用 Safetensors 的项目： /// ... /// - hpcaitech/ColossalAI"

