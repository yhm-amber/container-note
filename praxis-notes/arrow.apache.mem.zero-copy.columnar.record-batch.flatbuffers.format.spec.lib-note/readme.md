[arrow.site/apache.org]: https://arrow.apache.org/ "The universal columnar format and multi-language toolbox for fast data interchange and in-memory analytics // 通用列式格式和多语言工具箱，用于快速数据交换和内存内分析 /// What is Arrow? // 什么是 Arrow？ /// - Format // 格式 ///: Apache Arrow defines a language-independent columnar memory format for flat and nested data, organized for efficient analytic operations on modern hardware like CPUs and GPUs. The Arrow memory format also supports zero-copy reads for lightning-fast data access without serialization overhead. // Apache Arrow 定义了一种语言无关的列式内存格式，用于扁平化和嵌套数据，该格式针对现代硬件（如 CPU 和 GPU）上的高效分析操作进行了组织。Arrow 内存格式还支持零拷贝读取，实现闪电般快速的数据访问，无需序列化开销。 ///; [Learn more](https://arrow.apache.org/overview/ 'Apache Arrow Overview  Apache Arrow 概述') about the design or [read the specification](https://arrow.apache.org/docs/format/Columnar.html 'Arrow Columnar Format // Arrow 列格式'). // 了解更多设计信息或阅读规范。 /// - Libraries // 库 ///: Arrow's libraries implement the format and provide building blocks for a range of [use cases](https://arrow.apache.org/use_cases/ 'Use Cases // 用例'), including high performance analytics. [Many popular projects](https://arrow.apache.org/powered_by/ 'Project and Product Names Using “Apache Arrow” // 使用“Apache Arrow”的项目和产品名称') use Arrow to ship columnar data efficiently or as the basis for analytic engines. // Arrow 的库实现了该格式，并为多种用例（包括高性能分析）提供了构建模块。许多流行的项目使用 Arrow 高效地传输列式数据，或将其作为分析引擎的基础。 ///; Libraries are available for C, C++, C#, Go, Java, JavaScript, Julia, MATLAB, Python, R, Ruby, Rust, and Swift. See how to install and get started. // 库支持 C、C++、C#、Go、Java、JavaScript、Julia、MATLAB、Python、R、Ruby、Rust 和 Swift。查看如何安装和开始使用。 /// - Ecosystem // 生态系统 ///: Apache Arrow is software created by and for the developer community. We are dedicated to open, kind communication and consensus decisionmaking. Our committers come from a range of organizations and backgrounds, and we welcome all to participate with us. // Apache Arrow 是由开发者社区创建并服务于开发者社区的软件。我们致力于开放、友善的沟通和共识决策。我们的贡献者来自不同的组织和背景，我们欢迎所有人参与其中。 ///; [Learn more](https://arrow.apache.org/community/ 'Apache Arrow Community // Apache Arrow 社区') about how you can ask questions and get involved in the Arrow project. // 了解更多关于如何提问和参与 Arrow 项目的信息。 /// Apache Arrow Overview // Apache Arrow 概述 ///: Apache Arrow is a multi-language toolbox for building high performance applications that process and transport large data sets. It is designed to both improve the performance of analytical algorithms and the efficiency of moving data from one system or programming language to another. // Apache Arrow 是一个多语言工具箱，用于构建处理和传输大型数据集的高性能应用程序。它旨在提高分析算法的性能，并提高数据从一个系统或编程语言移动到另一个系统或编程语言的效率。 ///; A critical component of Apache Arrow is its in-memory columnar format, a standardized, language-agnostic specification for representing structured, table-like datasets in-memory. This data format has a rich data type system (included nested and user-defined data types) designed to support the needs of analytic database systems, data frame libraries, and more. // Apache Arrow 的一个关键组件是其内存列式格式，这是一种标准化的、与语言无关的规范，用于在内存中表示结构化的、类似表格的数据集。这种数据格式具有丰富的数据类型系统（包括嵌套和用户定义的数据类型），旨在支持分析数据库系统、数据帧库等的需求。 /// - Columnar is Fast // 列式存储速度快 ///: The Apache Arrow format allows computational routines and execution engines to maximize their efficiency when scanning and iterating large chunks of data. In particular, the contiguous columnar layout enables vectorization using the latest SIMD (Single Instruction, Multiple Data) operations included in modern processors. // Apache Arrow 格式允许计算例程和执行引擎在扫描和迭代大量数据时最大化其效率。特别是，连续的列式布局能够利用现代处理器中包含的最新 SIMD（单指令，多数据）操作进行向量化。 /// - Standardization Saves // 标准化节省 ///: Without a standard columnar data format, every database and language has to implement its own internal data format. This generates a lot of waste. Moving data from one system to another involves costly serialization and deserialization. In addition, common algorithms must often be rewritten for each data format. // 如果没有标准的列式数据格式，每个数据库和语言都必须实现自己的内部数据格式。这会产生大量浪费。将数据从一个系统移动到另一个系统涉及昂贵的序列化和反序列化。此外，常见的算法通常必须为每种数据格式重新编写。 ///; Arrow's in-memory columnar data format is an out-of-the-box solution to these problems. Systems that use or support Arrow can transfer data between them at little-to-no cost. Moreover, they don't need to implement custom connectors for every other system. On top of these savings, a standardized memory format facilitates reuse of libraries of algorithms, even across languages. // Arrow 的内存列式数据格式是解决这些问题的即用型方案。使用或支持 Arrow 的系统可以在它们之间以几乎零成本传输数据。此外，它们无需为每个其他系统实现自定义连接器。除了这些节省之外，标准化的内存格式促进了算法库的复用，甚至跨语言复用。 /// - Arrow Libraries // Arrow 库 ///: The Arrow project contains libraries that enable you to work with data in the Arrow columnar format in many languages. The C++, C#, Go, Java, JavaScript, Julia, Rust, and Swift libraries contain distinct implementations of the Arrow format. These libraries are integration-tested against each other to ensure their fidelity to the format. In addition, Arrow libraries for C (GLib), MATLAB, Python, R, and Ruby are built on top of the C++ library. // Arrow 项目包含多个语言的库，使您能够在多种语言中处理 Arrow 列表格式数据。C++、C#、Go、Java、JavaScript、Julia、Rust 和 Swift 库都包含对 Arrow 格式的不同实现。这些库相互集成测试以确保它们对格式的忠实性。此外，C (GLib)、MATLAB、Python、R 和 Ruby 的 Arrow 库都建立在 C++ 库之上。 ///; These official libraries enable third-party projects to work with Arrow data without having to implement the Arrow columnar format themselves. They also contain many software components that assist with systems problems related to getting data in and out of remote storage systems and moving Arrow-formatted data over network interfaces, among other use cases. // 这些官方库使第三方项目能够在无需自己实现 Arrow 列表格式的情况下处理 Arrow 数据。它们还包含许多软件组件，用于协助与从远程存储系统中获取和传输数据以及在网络接口上移动 Arrow 格式数据相关的系统问题，以及其他用例。 /// Arrow Columnar Format // Arrow 列格式 ///: The Arrow columnar format includes a language-agnostic in-memory data structure specification, metadata serialization, and a protocol for serialization and generic data transport. // Arrow 列式格式包括语言无关的内存数据结构规范、元数据序列化以及用于序列化和通用数据传输的协议。 ///; This document is intended to provide adequate detail to create a new implementation of the columnar format without the aid of an existing implementation. We utilize Google’s Flatbuffers project for metadata serialization, so it will be necessary to refer to the project’s Flatbuffers protocol definition files while reading this document. // 本文档旨在提供足够的细节，以便在不依赖现有实现的情况下创建列式格式的新实现。我们使用 Google 的 Flatbuffers 项目进行元数据序列化，因此在阅读本文档时，将有必要参考该项目的 Flatbuffers 协议定义文件。 ///; The columnar format has some key features: // 列式格式具有一些关键特性： ///; - Data adjacency for sequential access (scans) // 数据邻近性，便于顺序访问（扫描） ///; - O(1) (constant-time) random access [1] // O(1)（常数时间）随机访问 1 ///; - SIMD and vectorization-friendly // SIMD 和向量化友好 ///; - Relocatable without “pointer swizzling”, allowing for true zero-copy access in shared memory // 无需“指针调换”，支持在共享内存中进行真正的零拷贝访问 ///; The Arrow columnar format provides analytical performance and data locality guarantees in exchange for comparatively more expensive mutation operations. This document is concerned only with in-memory data representation and serialization details; issues such as coordinating mutation of data structures are left to be handled by implementations. // Arrow 列式格式以相对更昂贵的变异操作为代价，提供了分析性能和数据局部性保证。本文档仅关注内存中的数据表示和序列化细节；数据结构变异等协调问题留给实现者处理。"
[arrow.docs/.site]: https://arrow.apache.org/docs "Apache Arrow is a universal columnar format and multi-language toolbox for fast data interchange and in-memory analytics. // Apache Arrow 是一种通用的列式格式和多语言工具箱，用于快速数据交换和内存内分析。 /// The project specifies a language-independent column-oriented memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. The project houses an actively developed collection of libraries in many languages for solving problems related to data transfer and in-memory analytical processing. This includes such topics as: // 该项目指定了一种语言无关的列式内存格式，用于扁平化和层次化数据，并针对现代硬件的高效分析操作进行组织。该项目包含许多语言中积极开发的库，用于解决与数据传输和内存内分析处理相关的问题。这包括以下主题： /// - Zero-copy shared memory and RPC-based data movement // 零拷贝共享内存和基于 RPC 的数据传输 /// - Reading and writing file formats (like CSV, Apache ORC, and Apache Parquet) // 读取和写入文件格式（如 CSV、Apache ORC 和 Apache Parquet） /// - In-memory analytics and query processing // 内存中分析和查询处理"
[arrow.lib:{{cpp-impl+cpp-binds:r,python,ruby}}.src/gh]: https://github.com/apache/arrow.git "(Apache-2.0) (Languages: C++ 71.1%, Python 7.9%, Cython 4.5%, Ruby 4.2%, R 4.0%, C 2.3%, Other 6.0%) Apache Arrow is the universal columnar format and multi-language toolbox for fast data interchange and in-memory analytics // Apache Arrow 是用于快速数据交换和内存中分析的通用列式格式和多语言工具箱 /// Major components of the project include: // 该项目的主要组件包括： /// - The Arrow Columnar Format: a standard and efficient in-memory representation of various datatypes, plain or nested // Arrow 列式格式：一种标准且高效的内存中各种数据类型的表示方式，可以是简单的或嵌套的 /// - The Arrow IPC Format: an efficient serialization of the Arrow format and associated metadata, for communication between processes and heterogeneous environments // Arrow IPC 格式：Arrow 格式及其相关元数据的有效序列化，用于进程和异构环境之间的通信 /// - ADBC (Arrow Database Connectivity) ↗ (. gh: apache/arrow-adbc.git): Arrow-powered API, drivers, and libraries for access to databases and query engines // ADBC（Arrow Database Connectivity） ↗ ：基于 Arrow 的 API、驱动程序和库，用于访问数据库和查询引擎 /// - The Arrow Flight RPC protocol (. gh: apache/arrow.git @main/format/Flight.proto): based on the Arrow IPC format, a building block for remote services exchanging Arrow data with application-defined semantics (for example a storage server or a database) // Arrow Flight RPC 协议：基于 Arrow IPC 格式，用于远程服务交换具有应用定义语义的 Arrow 数据的构建模块（例如存储服务器或数据库） /// - C++ libraries /// - C bindings using GLib /// - .NET libraries /// - Gandiva (. gh: apache/arrow.git @main/cpp/src/gandiva): an LLVM-based (. https://llvm.org/) Arrow expression compiler, part of the C++ codebase // Gandiva：一个基于 LLVM 的 Arrow 表达式编译器，是 C++代码库的一部分 /// - Go libraries ↗ /// - Java libraries ↗ /// - JavaScript libraries /// - Julia implementation /// - Python libraries /// - R libraries /// - Ruby libraries /// - Rust libraries ↗ /// - Swift libraries ↗ /// The ↗ icon denotes that this component of the project is maintained in a separate repository. // ↗ 图标表示该项目此组件在单独的存储库中进行维护。"
[arrow.lib:r<cpp-impl>.r/cran]: https://cran.r-project.org/web/packages/arrow/ "(Version: 	22.0.0) (Depends: 	R (≥ 4.1)) (Imports: 	assertthat, bit64 (≥ 0.9-7), glue, methods, purrr, R6, rlang (≥ 1.0.0), stats, tidyselect (≥ 1.0.0), utils, vctrs) (LinkingTo: 	cpp11 (≥ 0.4.2)) (Suggests: 	blob, curl, cli, DBI, dbplyr, decor, distro, dplyr, duckdb (≥ 0.2.8), hms, jsonlite, knitr, lubridate, pillar, pkgload, reticulate, rmarkdown, stringi, stringr, sys, testthat (≥ 3.1.0), tibble, tzdb, withr) (Published: 	2025-10-29) (DOI: 	10.32614/CRAN.package.arrow) (Author: 	Neal Richardson [aut], Ian Cook [aut], Nic Crane [aut], Dewey Dunnington ORCID iD [aut], Romain François ORCID iD [aut], Jonathan Keane [aut, cre], Bryce Mecum [aut], Dragoș Moldovan-Grünfeld [aut], Jeroen Ooms [aut], Jacob Wujciak-Jens [aut], Javier Luraschi [ctb], Karl Dunkle Werner ORCID iD [ctb], Jeffrey Wong [ctb], Apache Arrow [aut, cph]) (Maintainer: 	Jonathan Keane <jkeane at gmail.com>) (License: 	Apache License (≥ 2.0)) (NeedsCompilation: 	yes) (SystemRequirements: 	C++17; for AWS S3 support on Linux, libcurl and openssl (optional); cmake >= 3.26 (build-time only, and only for full source build)) arrow: Integration to 'Apache' 'Arrow' // arrow: 与 'Apache' 'Arrow' 集成 /// 'Apache' 'Arrow' <https://arrow.apache.org/> is a cross-language development platform for in-memory data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. This package provides an interface to the 'Arrow C++' library. // 'Apache' 'Arrow' < https://arrow.apache.org/> 是一个跨语言开发平台，用于内存中的数据。它指定了一种标准化的、与语言无关的列式内存格式，用于扁平化和层次化数据，并组织为在现代硬件上进行高效分析操作。该软件包提供了对 'Arrow C++' 库的接口。 (src: gh:apache/arrow.git)"
[arrow.lib:r<cpp-impl>.site/.docs]: https://arrow.apache.org/docs/r/ "(: install.packages('arrow')) (License: Apache License (>= 2.0))"
[arrow.lib:python<cpp-impl>.pip/pypi]: https://pypi.org/project/pyarrow/ "(: pip install -U -- pyarrow) (Apache-2.0) (Requires: Python >=3.10) (src: gh:apache/arrow.git)"
[arrow.lib:python<cpp-impl>.site/.docs]: https://arrow.apache.org/docs/r/ "PyArrow - Apache Arrow Python bindings"
[arrow.lib:julia-impl.src/gh]: https://github.com/apache/arrow-julia.git "(Apache-2.0) (Languages: Julia 95.7%, Shell 3.6%, Other 0.7%) Official Julia implementation of Apache Arrow // Apache Arrow 的官方 Julia 实现 /// This is a pure Julia implementation of the Apache Arrow data standard. This package provides Julia AbstractVector objects for referencing data that conforms to the Arrow standard. This allows users to seamlessly interface Arrow formatted data with a great deal of existing Julia code. // 这是一个 Apache Arrow 数据标准的纯 Julia 实现。该软件包提供了 Julia AbstractVector 对象，用于引用符合 Arrow 标准的数据。这允许用户无缝地使用大量现有的 Julia 代码与 Arrow 格式的数据进行交互。"
[arrow.lib:julia-impl.julia/juliahub]: https://juliahub.com/ui/Packages/General/Arrow/ "(: Pkg.add('Arrow')) (Apache-2.0) (src: gh:apache/arrow-julia.git)"
[arrow.lib:julia-impl.site/.site]: https://arrow.apache.org/julia/ "A pure Julia implementation of the apache arrow memory format specification. // Apache Arrow 内存格式规范的纯 Julia 实现。 /// This implementation supports the 1.0 version of the specification, including support for: // 该实现支持规范的 1.0 版本，包括对以下内容的支持： /// - All primitive data types // 所有原始数据类型 /// - All nested data types // 所有嵌套数据类型 /// - Dictionary encodings, nested dictionary encodings, and messages // 字典编码、嵌套字典编码和消息 /// - Extension types // 扩展类型 /// - Streaming, file, record batch, and replacement and isdelta dictionary messages // 流式传输、文件、记录批处理以及替换和 isdelta 字典消息 /// - Buffer compression/decompression via the standard LZ4 frame and Zstd formats // 通过标准 LZ4 框架和 Zstd 格式进行缓冲区压缩/解压缩 /// It currently doesn't include support for: // 目前不包括对以下内容的支持： /// - Tensors or sparse tensors // 张量或稀疏张量 /// - Flight RPC /// - C data interface // C 数据接口 /// Third-party data formats: // 第三方数据格式： /// - csv and parquet support via the existing CSV.jl and Parquet.jl packages // 通过现有的 CSV.jl 和 Parquet.jl 包支持 csv 和 parquet /// - Other Tables.jl-compatible packages automatically supported (DataFrames.jl, JSONTables.jl, JuliaDB.jl, SQLite.jl, MySQL.jl, JDBC.jl, ODBC.jl, XLSX.jl, etc.) // 其他与 Tables.jl 兼容的包自动支持（ DataFrames.jl、JSONTables.jl、JuliaDB.jl、SQLite.jl、MySQL.jl、JDBC.jl、ODBC.jl、XLSX.jl 等） /// - No current Julia packages support ORC or Avro data formats // 目前没有 Julia 包支持 ORC 或 Avro 数据格式"
[arrow.lib:swift-impl.src/gh]: https://github.com/apache/arrow-swift.git "(Apache-2.0) (Languages: Swift 95.8%, Shell 2.6%, Other 1.6%) Official Swift implementation of Apache Arrow // Apache Arrow 官方 Swift 实现"
[arrow.lib:swift-impl.spi~/swiftpackageindex]: https://swiftpackageindex.com/apache/arrow-swift/ "(~ spi-playgrounds://open?dependencies=apache/arrow-swift) (Apache-2.0) (i: 1 library) (i: No executables) (i: No plugins) (i: No macros) (i: Zero data race safety errors) (src: gh:apache/arrow-swift.git)"
[arrow.lib:swift-impl.site/.site]: https://arrow.apache.org/swift/ "Official Swift implementation of Apache Arrow"
[arrow.lib:go-impl.src/gh]: https://github.com/apache/arrow-go.git "(Apache-2.0) (Languages: Assembly 75.1%, Go 23.6%, C 0.8%, C++ 0.2%, Makefile 0.1%, Shell 0.1%, Other 0.1%) Official Go implementation of Apache Arrow // Apache Arrow 官方 Go 实现 /// Apache Arrow is a cross-language development platform for in-memory data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and inter-process communication. // Apache Arrow 是一个跨语言的内存数据开发平台。它指定了一种标准化的、与语言无关的列式内存格式，用于扁平化和层次化数据，并组织为在现代硬件上进行高效分析操作。它还提供了计算库和零拷贝流式消息传递和进程间通信。 /// The arrow package makes extensive use of c2goasm (. gh: minio/c2goasm.git) to leverage LLVM's advanced optimizer and generate PLAN9 assembly functions from C/C++ code. The arrow package can be compiled without these optimizations using the noasm build tag. Alternatively, by configuring an environment variable, it is possible to dynamically configure which architecture optimizations are used at runtime. We use the cpu package (. https://pkg.go.dev/golang.org/x/sys/cpu) to check dynamically for these features. // arrow 包大量使用 c2goasm 来利用 LLVM 的高级优化器，并将 C/C++ 代码生成 PLAN9 汇编函数。可以使用 noasm 构建标签来编译 arrow 包而不使用这些优化。或者，通过配置环境变量，可以在运行时动态配置使用的架构优化。我们使用 (cpu)[ https://pkg.go.dev/golang.org/x/sys/cpu] 包来动态检查这些特性。"
[arrow.lib:go-impl.go/pkg.go.dev]: https://pkg.go.dev/github.com/apache/arrow-go/v18/arrow "(: go get -u -- github.com/apache/arrow-go/v18/arrow) (License: Apache-2.0, BSD-3-Clause) (Imports: 22) (Imported by: 337) (src: gh:apache/arrow-go.git)"
[parquet.lib:go-impl.go/pkg.go.dev]: https://pkg.go.dev/github.com/apache/arrow-go/v18/parquet "(: go get -u -- github.com/apache/arrow-go/v18/parquet) (License: Apache-2.0, BSD-3-Clause) (Imports: 15) (Imported by: 28) (src: gh:apache/arrow-go.git)"
[parquet_reader.cli:go-impl.go/pkg.go.dev]: https://pkg.go.dev/github.com/apache/arrow-go/v18/parquet/cmd/parquet_reader "(: go install -- github.com/apache/arrow-go/v18/parquet/cmd/parquet_reader@latest) (License: Apache-2.0, BSD-3-Clause) (Imports: 17) (Imported by: 0) (src: gh:apache/arrow-go.git)"
[parquet_schema.cli:go-impl.go/pkg.go.dev]: https://pkg.go.dev/github.com/apache/arrow-go/v18/parquet/cmd/parquet_schema "(: go install -- github.com/apache/arrow-go/v18/parquet/cmd/parquet_schema@latest) (License: Apache-2.0, BSD-3-Clause) (Imports: 5) (Imported by: 0) (src: gh:apache/arrow-go.git)"
[arrow.lib:go-impl.site/.site]: https://arrow.apache.org/go/ "(: go get -u -- github.com/apache/arrow-go/v18) (License: Apache-2.0, BSD-3-Clause) (src: gh:apache/arrow-go.git)"
[arrow.lib:typescript-impl.src/gh]: https://github.com/apache/arrow-js.git "(Apache-2.0) (Languages: TypeScript 88.1%, JavaScript 10.1%, Shell 1.7%, Python 0.1%) Official JavaScript implementation of Apache Arrow // Apache Arrow 官方 JavaScript 实现 /// Apache Arrow is a columnar memory layout specification for encoding vectors and table-like containers of flat and nested data. The Arrow spec aligns columnar data in memory to minimize cache misses and take advantage of the latest SIMD (Single input multiple data) and GPU operations on modern processors. // Apache Arrow 是一种列式内存布局规范，用于编码扁平或嵌套数据的向量和表格状容器。Arrow 规范将列式数据对齐到内存中，以最小化缓存未命中，并利用现代处理器的最新 SIMD（单输入多数据）和 GPU 操作。 /// Apache Arrow is the emerging standard for large in-memory columnar data (Spark, Pandas, Drill, Graphistry, ...). By standardizing on a common binary interchange format, big data systems can reduce the costs and friction associated with cross-system communication. // Apache Arrow 是大型内存列式数据的涌现标准（Spark、Pandas、Drill、Graphistry、...）。通过采用通用的二进制交换格式，大数据系统可以减少跨系统通信的成本和摩擦。 /// The bundles we compile support moderns browser released in the last 5 years. This includes supported versions of Firefox, Chrome, Edge, and Safari. We do not actively support Internet Explorer. Apache Arrow also works on maintained versions of Node. // 我们编译的包支持过去 5 年内发布的现代浏览器。这包括 Firefox、Chrome、Edge 和 Safari 的支持版本。我们不积极支持 Internet Explorer。Apache Arrow 也适用于维护版本的 Node。"
[arrow.lib:typescript-impl.npm/npmjs.com]: https://npmjs.com/package/apache-arrow "(: npm i -- apache-arrow # [[combined]] es2015/CommonJS/ESModules/UMD + esnext/UMD) (Apache-2.0) (Unpacked Size: 5.37 MB) (Total Files: 1035) (src: gh:apache/arrow-js.git)"
[arrow:ts.lib:typescript-impl.npm/npmjs.com]: https://npmjs.com/package/@apache-arrow/ts "(: npm i -- @apache-arrow/ts # [[standalone]] TypeScript package) (Apache-2.0) (Unpacked Size: 1.11 MB) (Total Files: 154) (src: gh:apache/arrow-js.git)"
[arrow:es5-cjs.lib:typescript-impl.npm/npmjs.com]: https://npmjs.com/package/@apache-arrow/es5-cjs "(: npm i -- @apache-arrow/es5-cjs # [[standalone]] es5/CommonJS package) (Apache-2.0) (Unpacked Size: 3.02 MB) (Total Files: 736) (src: gh:apache/arrow-js.git)"
[arrow:es5-esm.lib:typescript-impl.npm/npmjs.com]: https://npmjs.com/package/@apache-arrow/es5-esm "(: npm i -- @apache-arrow/es5-esm # [[standalone]] es5/ESModules package) (Apache-2.0) (Unpacked Size: 2.92 MB) (Total Files: 736) (src: gh:apache/arrow-js.git)"
[arrow:es5-umd.lib:typescript-impl.npm/npmjs.com]: https://npmjs.com/package/@apache-arrow/es5-umd "(: npm i -- @apache-arrow/es5-umd # [[standalone]] es5/UMD package) (Apache-2.0) (Unpacked Size: 2.49 MB) (Total Files: 467) (src: gh:apache/arrow-js.git)"
[arrow:es2015-cjs.lib:typescript-impl.npm/npmjs.com]: https://npmjs.com/package/@apache-arrow/es2015-cjs "(: npm i -- @apache-arrow/es2015-cjs # [[standalone]] es2015/CommonJS package) (Apache-2.0) (Unpacked Size: 2.8 MB) (Total Files: 736) (src: gh:apache/arrow-js.git)"
[arrow:es2015-esm.lib:typescript-impl.npm/npmjs.com]: https://npmjs.com/package/@apache-arrow/es2015-esm "(: npm i -- @apache-arrow/es2015-esm # [[standalone]] es2015/ESModules package) (Apache-2.0) (Unpacked Size: 2.7 MB) (Total Files: 736) (src: gh:apache/arrow-js.git)"
[arrow:es2015-umd.lib:typescript-impl.npm/npmjs.com]: https://npmjs.com/package/@apache-arrow/es2015-umd "(: npm i -- @apache-arrow/es2015-umd # [[standalone]] es2015/UMD package) (Apache-2.0) (Unpacked Size: 2.21 MB) (Total Files: 456) (src: gh:apache/arrow-js.git)"
[arrow:esnext-cjs.lib:typescript-impl.npm/npmjs.com]: https://npmjs.com/package/@apache-arrow/esnext-cjs "(: npm i -- @apache-arrow/esnext-cjs # [[standalone]] esNext/CommonJS package) (Apache-2.0) (Unpacked Size: 2.78 MB) (Total Files: 736) (src: gh:apache/arrow-js.git)"
[arrow:esnext-esm.lib:typescript-impl.npm/npmjs.com]: https://npmjs.com/package/@apache-arrow/esnext-esm "(: npm i -- @apache-arrow/esnext-esm # [[standalone]] esNext/ESModules package) (Apache-2.0) (Unpacked Size: 2.68 MB) (Total Files: 736) (src: gh:apache/arrow-js.git)"
[arrow:esnext-umd.lib:typescript-impl.npm/npmjs.com]: https://npmjs.com/package/@apache-arrow/esnext-umd "(: npm i -- @apache-arrow/esnext-umd # [[standalone]] esNext/UMD package) (Apache-2.0) (Unpacked Size: 2.13 MB) (Total Files: 447) (src: gh:apache/arrow-js.git)"
[arrow.lib:typescript-impl.site/.site]: https://arrow.apache.org/js/current/ "Apache Arrow JavaScript"
[arrow.lib:java-impl.src/gh]: https://github.com/apache/arrow-java.git "(Apache-2.0) (Languages: Java 97.6%, C++ 1.5%, Shell 0.5%, Python 0.2%, CMake 0.2%, C 0.0%) Official Java implementation of Apache Arrow // Apache Arrow 官方 Java 实现 /// Arrow uses Google's Flatbuffers to transport metadata. The java version of the library requires the generated flatbuffer classes can only be used with the same version that generated them. Arrow packages a version of the arrow-vector module that shades flatbuffers and arrow-format into a single JAR. Using the classifier \"shade-format-flatbuffers\" in your pom.xml will make use of this JAR, you can then exclude/resolve the original dependency to a version of your choosing. // Arrow 使用 Google 的 FlatBuffers 来传输元数据。该库的 Java 版本要求生成的 flatbuffer 类只能与生成它们的相同版本一起使用。Arrow 将一个版本的 arrow-vector 模块打包，将 flatbuffers 和 arrow-format 阴影为单个 JAR。在您的 pom.xml 中使用分类器 \"shade-format-flatbuffers\" 将会使用这个 JAR，然后您可以排除/解析为任意您选择的版本的原依赖。"
[parquet.lib:java-impl.src/gh]: https://github.com/apache/parquet-java.git "(Apache-2.0) (Languages: Java 99.7%, Other 0.3%) Apache Parquet Java /// Parquet is an active project, and new features are being added quickly. Here are a few features: // Parquet 是一个活跃的项目，新特性正在快速添加。以下是一些特性： /// - Type-specific encoding // 类型特定的编码 /// - Hive integration (deprecated) // Hive 集成（已弃用） /// - Pig integration (deprecated) // 猪集成（已弃用） /// - Cascading integration (deprecated) // 级联集成（已弃用） /// - Crunch integration // Crunch 集成 /// - Apache Arrow integration // Apache Arrow 集成 /// - Scrooge integration (deprecated) // Scrooge 集成（已弃用） /// - Impala integration (non-nested) // Impala 集成（非嵌套） /// - Java Map/Reduce API /// - Native Avro support // 原生 Avro 支持 /// - Native Thrift support // 原生 Thrift 支持 /// - Native Protocol Buffers support // 原生 Protocol Buffers 支持 /// - Complex structure support // 复杂结构支持 /// - Run-length encoding (RLE) // 运行长度编码 (RLE) /// - Bit Packing // 位打包 /// - Adaptive dictionary encoding // 自适应字典编码 /// - Predicate pushdown // 谓词下推 /// - Column stats // 列统计 /// - Delta encoding // Delta 编码 /// - Index pages // 索引页面 /// - Scala DSL (deprecated) // Scala DSL（已弃用） /// - Java Vector API support (experimental) // Java 向量 API 支持（实验性） /// Parquet-Java has supported Java Vector API to speed up reading, to enable this feature: // Parquet-Java 已支持 Java 向量 API 以加速读取，要启用此功能： /// - Java 17+, 64-bit // Java 17+，64 位 /// - Requiring the CPU to support instruction sets: // 需要 CPU 支持下列指令集： /// - - avx512vbmi /// - - avx512_vbmi2 /// - To build the jars: ./mvnw clean package -P vector-plugins // 构建 jar 文件： ./mvnw clean package -P vector-plugins /// - For Apache Spark to enable this feature: // 要使 Apache Spark 启用此功能： /// - - Build parquet and replace the parquet-encoding-{VERSION}.jar on the spark jars folder // 构建 Parquet 并替换 spark jars 文件夹中的 parquet-encoding-{VERSION}.jar /// - - Build parquet-encoding-vector and copy parquet-encoding-vector-{VERSION}.jar to the spark jars folder // 构建 parquet-encoding-vector，并将 parquet-encoding-vector-{VERSION}.jar 复制到 Spark 的 jars 文件夹中 /// - - Edit spark class#VectorizedRleValuesReader, function#readNextGroup refer to parquet class#ParquetReadRouter, function#readBatchUsing512Vector // 编辑 spark 类 #VectorizedRleValuesReader，函数 #readNextGroup 参考 parquet 类 #ParquetReadRouter，函数 #readBatchUsing512Vector /// - - Build spark with maven and replace spark-sql_2.12-{VERSION}.jar on the spark jars folder // 使用 maven 构建 spark 并替换 spark jars 文件夹中的 spark-sql_2.12-{VERSION}.jar /// Thrift integration is provided by the parquet-thrift (. gh: apache/parquet-java.git @master/parquet-thrift) sub-project. // Thrift 集成由 parquet-thrift 子项目提供。 /// Avro conversion is implemented via the parquet-avro (. gh: apache/parquet-java.git @master/parquet-avro) sub-project. // Avro 转换通过 parquet-avro 子项目实现。 /// Protobuf conversion is implemented via the parquet-protobuf (. gh: apache/parquet-java.git @master/parquet-protobuf) sub-project. // Protobuf 转换通过 parquet-protobuf 子项目实现。"
[arrow.lib:java-impl.leiningen/jitpack.io]: https://jitpack.io/#apache/arrow-java.git "({ (:repositories [[\"jitpack\" \"https://jitpack.io\"]]) (:dependencies [[com.github.apache/arrow-java.git \"Tag\"]]) }) (Apache-2.0) (src: gh:apache/parquet-java.git)"
[parquet.lib:java-impl.leiningen/jitpack.io]: https://jitpack.io/#apache/parquet-java.git "({ (:repositories [[\"jitpack\" \"https://jitpack.io\"]]) (:dependencies [[com.github.apache/parquet-java.git \"Tag\"]]) }) (Apache-2.0) (src: gh:apache/parquet-java.git)"
[arrow.lib:java-impl.site/.site]: https://arrow.apache.org/java/current/ "Java Implementation // Java 实现 ¶ /// This is the documentation of the Java API of Apache Arrow. For more details on the Arrow format and other language bindings see the parent documentation. // 这是 Apache Arrow Java API 的文档。有关 Arrow 格式和其他语言绑定的更多详细信息，请参阅父文档。"
[arrow.lib:rust-impl.src/gh]: https://github.com/apache/arrow-rs.git "(Apache-2.0) (Languages: Rust 99.6%, Other 0.4%) Official Rust implementation of Apache Arrow // Apache Arrow 的官方 Rust 实现 /// Welcome to the Rust implementation of Apache Arrow, the popular in-memory columnar format. // 欢迎来到 Apache Arrow 的 Rust 实现，这是一种流行的内存列式格式。"
[arrow.lib:rust-impl.cargo/crates]: https://crates.io/crates/arrow "(: cargo add -- arrow) (Apache-2.0) (1.9K SLoC) (88.1 KiB) Core functionality (memory layout, arrays, low level computations) // 核心功能（内存布局、数组、低级计算） (src: gh:apache/arrow-rs.git)"
[arrow-flight.cli:rust-impl.cargo/crates]: https://crates.io/crates/arrow-flight "(: cargo install -- arrow-flight) (Apache-2.0) (8.9K SLoC) (147 KiB) Support for Arrow-Flight IPC protocol // 支持 Arrow-Flight IPC 协议 (src: gh:apache/arrow-rs.git)"
[parquet.cli:rust-impl.cargo/crates]: https://crates.io/crates/parquet "(: cargo install -- parquet) (Apache-2.0) (69K SLoC) (724 KiB) Support for Parquet columnar file format // 支持 Parquet 列式文件格式 (src: gh:apache/arrow-rs.git)"
[parquet-derive.lib:rust-impl.cargo/crates]: https://crates.io/crates/parquet "(: cargo add -- parquet_derive) (Apache-2.0) (1,355 SLoC) (20.9 KiB) A crate for deriving RecordWriter/RecordReader for arbitrary, simple structs // 一个用于为任意、简单的结构体派生 RecordWriter/RecordReader 的 Crate (src: gh:apache/arrow-rs.git)"
[adbc-std.site/.site]: https://arrow.apache.org/adbc/current/ "ADBC is a set of APIs and libraries for Arrow-native access to databases. Execute SQL and Substrait (substrait.io) queries, query database catalogs, and more, all using Arrow data to eliminate unnecessary data copies, speed up access, and make it more convenient to build analytical applications. // ADBC 是一组用于 Arrow 原生访问数据库的 API 和库。执行 SQL 和 Substrait 查询，查询数据库目录等，全部使用 Arrow 数据来消除不必要的数据副本，加快访问速度，并使构建分析应用更加便捷。"
[adbc-std.src/gh]: https://github.com/apache/arrow-adbc.git "(Apache-2.0) (Languages: C# 43.1%, C++ 19.5%, Go 10.9%, Java 5.4%, Rust 4.8%, C 4.3%, Other 12.0%) Database connectivity API standard and libraries for Apache Arrow // Apache Arrow 数据库连接 API 标准及库 /// Like [Arrow Flight SQL](https://arrow.apache.org/docs/format/FlightSql.html 'Arrow Flight SQL is a protocol for interacting with SQL databases using the Arrow in-memory format and the Flight RPC framework. // Arrow Flight SQL 是一个使用 Arrow 内存格式和 Flight RPC 框架与 SQL 数据库交互的协议。 /// Generally, a database will implement the RPC methods according to the specification, but does not need to implement a client-side driver. A database client can use the provided Flight SQL client to interact with any database that supports the necessary endpoints. Flight SQL clients wrap the underlying Flight client to provide methods for the new RPC methods described here. // 通常，数据库会根据规范实现 RPC 方法，但不需要实现客户端驱动。数据库客户端可以使用提供的 Flight SQL 客户端与支持必要端点的任何数据库进行交互。Flight SQL 客户端封装了底层的 Flight 客户端，以提供此处描述的新 RPC 方法。'), ADBC is an Arrow-based way to work with databases. However, Flight SQL is a protocol defining a wire format and network transport as opposed to an API specification. Flight SQL requires a database to specifically implement support for it, while ADBC is a client API specification for wrapping existing database protocols which could be Arrow-native or not. Together, ADBC and Flight SQL offer a fully Arrow-native solution for clients and database vendors. // 与 Arrow Flight SQL 类似，ADBC 也是一种基于 Arrow 的数据库操作方式。然而，Flight SQL 是一个定义了网络传输格式和网络传输的协议，而不是 API 规范。Flight SQL 需要数据库专门实现对其的支持，而 ADBC 是一个客户端 API 规范，用于封装现有的数据库协议，这些协议可以是原生 Arrow 的，也可以不是。ADBC 和 Flight SQL 共同为客户端和数据库供应商提供了一种完全基于 Arrow 的解决方案。"
[parquet-std.site/apache.org]: https://parquet.apache.org/ "Apache Parquet is an open source, column-oriented data file format designed for efficient data storage and retrieval. It provides high performance compression and encoding schemes to handle complex data in bulk and is supported in many programming language and analytics tools. // Apache Parquet 是一种开放源代码的列式数据文件格式，专为高效数据存储和检索而设计。它提供高性能的压缩和编码方案，能够批量处理复杂数据，并在多种编程语言和分析工具中得到支持。"
[parquet-std.docs/.site]: https://parquet.apache.org/docs "We created Parquet to make the advantages of compressed, efficient columnar data representation available to any project in the Hadoop ecosystem. // 我们创建 Parquet 是为了让压缩、高效的列式数据表示的优势能够被 Hadoop 生态系统中的任何项目所使用。 /// Parquet is built from the ground up with complex nested data structures in mind, and uses the record shredding and assembly algorithm described in the Dremel paper. We believe this approach is superior to simple flattening of nested name spaces. // Parquet 从一开始就考虑了复杂的嵌套数据结构，并使用了 Dremel 论文中描述的记录分解和组装算法。我们认为这种方法优于简单的嵌套名称空间扁平化。 /// Parquet is built to support very efficient compression and encoding schemes. Multiple projects have demonstrated the performance impact of applying the right compression and encoding scheme to the data. Parquet allows compression schemes to be specified on a per-column level, and is future-proofed to allow adding more encodings as they are invented and implemented. // Parquet 旨在支持非常高效的压缩和编码方案。多个项目已经展示了将正确的压缩和编码方案应用于数据所带来的性能影响。Parquet 允许在每列级别指定压缩方案，并且为未来做好了准备，允许随着更多编码的发明和实现而添加更多编码。 /// Parquet is built to be used by anyone. The Hadoop ecosystem is rich with data processing frameworks, and we are not interested in playing favorites. We believe that an efficient, well-implemented columnar storage substrate should be useful to all frameworks without the cost of extensive and difficult to set up dependencies. // Parquet 旨在被任何人使用。Hadoop 生态系统中有丰富的数据处理框架，我们并不想偏袒任何一方。我们认为，一个高效、实现良好的列式存储底层应该对所有框架都有用，而不需要付出广泛且难以设置的依赖关系的成本。"
[parquet-std.src/gh]: https://github.com/apache/parquet-format.git "(Apache-2.0) (Languages: Thrift 84.3%, Shell 13.7%, Makefile 2.0%) Apache Parquet Format // Apache Parquet 格式 /// This repository contains the specification for Apache Parquet (parquet.apache.org) and Apache Thrift (thrift.apache.org) definitions to read and write Parquet metadata. // 该存储库包含 Apache Parquet 和 Apache Thrift 定义的规范，用于读取和写入 Parquet 元数据。 /// Apache Parquet is an open source, column-oriented data file format designed for efficient data storage and retrieval. It provides high performance compression and encoding schemes to handle complex data in bulk and is supported in many programming languages and analytics tools. // Apache Parquet 是一种开源的、列式数据文件格式，设计用于高效的数据存储和检索。它提供高性能的压缩和编码方案，用于批量处理复杂数据，并在许多编程语言和分析工具中得到支持。 /// Motivation // 动机 /// We created Parquet to make the advantages of compressed, efficient columnar data representation available to any project in the Hadoop ecosystem. // 我们创建 Parquet，是为了让压缩、高效的列式数据表示的优势能够为 Hadoop 生态系统中的任何项目所使用。 /// Parquet is built from the ground up with complex nested data structures in mind, and uses the [record shredding and assembly algorithm](https://github.com/julienledem/redelm/wiki/The-striping-and-assembly-algorithms-from-the-Dremel-paper 'The striping and assembly algorithms from the Dremel paper // Dremel 论文中的条带和组装算法') described in the [Dremel paper](https://research.google.com/pubs/archive/36632.pdf 'Dremel: Interactive Analysis of Web-Scale Datasets // Dremel：大规模数据集的交互式分析'). We believe this approach is superior to simple flattening of nested name spaces. // Parquet 从底层开始设计，专门用于处理复杂的嵌套数据结构，并使用了 Dremel 论文中描述的记录拆分和组装算法。我们相信这种方法优于简单地将嵌套命名空间扁平化。 /// Parquet is built to support very efficient compression and encoding schemes. Multiple projects have demonstrated the performance impact of applying the right compression and encoding scheme to the data. Parquet allows compression schemes to be specified on a per-column level, and is future-proofed to allow adding more encodings as they are invented and implemented. // Parquet 设计用于支持非常高效的压缩和编码方案。多个项目已经展示了将正确的压缩和编码方案应用于数据所带来的性能影响。Parquet 允许在列级别指定压缩方案，并且为未来做好了准备，可以随着更多编码的发明和实现而添加更多编码。 /// Parquet is built to be used by anyone. The Hadoop ecosystem is rich with data processing frameworks, and we are not interested in playing favorites. We believe that an efficient, well-implemented columnar storage substrate should be useful to all frameworks without the cost of extensive and difficult to set up dependencies. // Parquet 设计用于被任何人使用。Hadoop 生态系统中有丰富的数据处理框架，我们并不想偏袒任何一方。我们相信，一个高效、实现良好的列式存储底层应该对所有框架都有用，而无需付出大量且难以设置的依赖成本。"
