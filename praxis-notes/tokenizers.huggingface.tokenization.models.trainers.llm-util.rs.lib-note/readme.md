[core.src/gh]: https://github.com/huggingface/tokenizers.git "(Apache-2.0) (Languages: Rust 69.8%, Python 22.9%, Jupyter Notebook 4.2%, TypeScript 2.2%, JavaScript 0.4%) ğŸ’¥ Fast State-of-the-Art Tokenizers optimized for Research and Production // ğŸ’¥ ä¸“ä¸ºç ”å‘å’Œç”Ÿäº§ç¯å¢ƒä¼˜åŒ–çš„å¿«é€Ÿå…ˆè¿›åˆ†è¯å™¨ /// Provides an implementation of today's most used tokenizers, with a focus on performance and versatility. // æä¾›å½“ä»Šæœ€å¸¸ç”¨çš„åˆ†è¯å™¨çš„å®ç°ï¼Œé‡ç‚¹åœ¨äºæ€§èƒ½å’Œé€šç”¨æ€§ã€‚ /// Main features: // ä¸»è¦ç‰¹ç‚¹ï¼š /// - Train new vocabularies and tokenize, using today's most used tokenizers. // ä½¿ç”¨å½“ä»Šæœ€å¸¸ç”¨çš„åˆ†è¯å™¨è®­ç»ƒæ–°è¯æ±‡å¹¶è¿›è¡Œåˆ†è¯ã€‚ /// - Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes less than 20 seconds to tokenize a GB of text on a server's CPU. // ç”±äºé‡‡ç”¨äº† Rust è¯­è¨€ï¼Œé€Ÿåº¦æå¿«ï¼ˆè®­ç»ƒå’Œåˆ†è¯éƒ½å¾ˆå¿«ï¼‰ã€‚åœ¨æœåŠ¡å™¨ CPU ä¸Šï¼Œåˆ†è¯ 1GB çš„æ–‡æœ¬åªéœ€ä¸åˆ° 20 ç§’ã€‚ /// - Easy to use, but also extremely versatile. // æ˜“äºä½¿ç”¨ï¼Œè€Œä¸”ç”¨é€”æå…¶å¹¿æ³›ã€‚ /// - Designed for research and production. // ä¸“ä¸ºç ”å‘å’Œç”Ÿäº§è€Œè®¾è®¡ã€‚ /// - Normalization comes with alignments tracking. It's always possible to get the part of the original sentence that corresponds to a given token. // è§„èŒƒåŒ–åŒ…å«å¯¹é½è·Ÿè¸ªåŠŸèƒ½ã€‚å§‹ç»ˆå¯ä»¥è·å–åŸå§‹å¥å­ä¸­ä¸ç»™å®šæ ‡è®°å¯¹åº”çš„éƒ¨åˆ†ã€‚ /// - Does all the pre-processing: Truncate, Pad, add the special tokens your model needs. // å®Œæˆæ‰€æœ‰é¢„å¤„ç†ï¼šæˆªæ–­ã€å¡«å……ã€æ·»åŠ æ¨¡å‹æ‰€éœ€çš„ç‰¹æ®Šæ ‡è®°ã€‚"
[lib-rs<rs>.cargo/crates]: https://crates.io/crates/tokenizers "(: cargo add -- tokenizers) (Apache-2.0) (19K SLoC) (182 KiB) Provides an implementation of today's most used tokenizers, with a focus on performances and versatility. // æä¾›å½“ä»Šæœ€å¸¸ç”¨åˆ†è¯å™¨çš„å®ç°ï¼Œé‡ç‚¹åœ¨äºæ€§èƒ½å’Œé€šç”¨æ€§ã€‚ (src: gh:huggingface/tokenizers.git)"
[lib-py<rs>.pip/pypi]: https://pypi.org/project/tokenizers/ "(: pip install -- tokenizers) (License: Apache Software License) (Requires: Python >=3.9) Tokenizers // åˆ†è¯å™¨ /// Provides an implementation of today's most used tokenizers, with a focus on performance and versatility. // æä¾›å½“ä»Šæœ€å¸¸ç”¨çš„åˆ†è¯å™¨çš„å®ç°ï¼Œé‡ç‚¹åœ¨äºæ€§èƒ½å’Œé€šç”¨æ€§ã€‚ (src: gh:huggingface/tokenizers.git)"
[lib-ts<rs>.npm/npmjs]: https://npmjs.com/package/tokenizers "(: npm i -- tokenizers) (License: Apache-2.0) (Unpacked Size: 29.2 kB) (Total Files: 5) (~ tmpl from: @napi-rs/package-template)  (src: gh:huggingface/tokenizers.git)"
[lib-rb<rs>.gem/rubygem]: https://rubygems.org/gems/tokenizers "(: gem install -- tokenizers) (License: Apache-2.0) (Required Ruby Version: >= 3.2) Fast state-of-the-art tokenizers for Ruby // é€‚ç”¨äº Ruby çš„å¿«é€Ÿå…ˆè¿›çš„åˆ†è¯å™¨ (src: gh:ankane/tokenizers-ruby.git)"
[rb-bindlib.src/gh]: https://github.com/ankane/tokenizers-ruby.git "(Apache-2.0) (Languages: Rust 72.7%, Ruby 27.3%) Fast state-of-the-art tokenizers for Ruby // é€‚ç”¨äº Ruby çš„å¿«é€Ÿå…ˆè¿›çš„åˆ†è¯å™¨ /// This library follows the Tokenizers Python API (. hf-docs: tokenizers). You can follow Python tutorials and convert the code to Ruby in many cases. Feel free to open an issue if you run into problems. // æœ¬åº“éµå¾ª Tokenizers Python API ã€‚æ‚¨å¯ä»¥å‚è€ƒ Python æ•™ç¨‹ï¼Œåœ¨å¾ˆå¤šæƒ…å†µä¸‹å°†ä»£ç è½¬æ¢ä¸º Rubyã€‚å¦‚æœæ‚¨é‡åˆ°é—®é¢˜ï¼Œè¯·éšæ—¶æäº¤ issueã€‚"
[docs/hf]: https://huggingface.co/docs/tokenizers/index "Fast State-of-the-art tokenizers, optimized for both research and production // å¿«é€Ÿã€å…ˆè¿›çš„åˆ†è¯å™¨ï¼Œé’ˆå¯¹ç ”ç©¶å’Œç”Ÿäº§ç¯å¢ƒå‡è¿›è¡Œäº†ä¼˜åŒ–ã€‚ /// ğŸ¤— Tokenizers provides an implementation of todayâ€™s most used tokenizers, with a focus on performance and versatility. These tokenizers are also used in ğŸ¤— Transformers. // ğŸ¤— Tokenizers æä¾›äº†å½“ä»Šæœ€å¸¸ç”¨çš„åˆ†è¯å™¨çš„å®ç°ï¼Œé‡ç‚¹åœ¨äºæ€§èƒ½å’Œé€šç”¨æ€§ã€‚è¿™äº›åˆ†è¯å™¨ä¹Ÿç”¨äº ğŸ¤— Transformers ä¸­ã€‚ /// Main features: // ä¸»è¦ç‰¹ç‚¹ï¼š /// - Train new vocabularies and tokenize, using todayâ€™s most used tokenizers. // ä½¿ç”¨å½“ä»Šæœ€å¸¸ç”¨çš„åˆ†è¯å™¨è®­ç»ƒæ–°è¯æ±‡å¹¶è¿›è¡Œåˆ†è¯ã€‚ /// - Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes less than 20 seconds to tokenize a GB of text on a serverâ€™s CPU. // ç”±äºé‡‡ç”¨äº† Rust è¯­è¨€ï¼Œé€Ÿåº¦æå¿«ï¼ˆè®­ç»ƒå’Œåˆ†è¯éƒ½å¾ˆå¿«ï¼‰ã€‚åœ¨æœåŠ¡å™¨ CPU ä¸Šï¼Œåˆ†è¯ 1GB çš„æ–‡æœ¬åªéœ€ä¸åˆ° 20 ç§’ã€‚ /// - Easy to use, but also extremely versatile. // æ˜“äºä½¿ç”¨ï¼Œè€Œä¸”ç”¨é€”æå…¶å¹¿æ³›ã€‚ /// - Designed for both research and production. // ä¸“ä¸ºç§‘ç ”å’Œç”Ÿäº§è€Œè®¾è®¡ã€‚ /// - Full alignment tracking. Even with destructive normalization, itâ€™s always possible to get the part of the original sentence that corresponds to any token. // å®Œå…¨å¯¹é½è·Ÿè¸ªã€‚å³ä½¿é‡‡ç”¨ç ´åæ€§è§„èŒƒåŒ–ï¼Œä¹Ÿå§‹ç»ˆå¯ä»¥è·å–åŸå§‹å¥å­ä¸­ä¸ä»»ä½•æ ‡è®°å¯¹åº”çš„éƒ¨åˆ†ã€‚ /// - Does all the pre-processing: Truncation, Padding, add the special tokens your model needs. // å®Œæˆæ‰€æœ‰é¢„å¤„ç†ï¼šæˆªæ–­ã€å¡«å……ï¼Œæ·»åŠ æ¨¡å‹æ‰€éœ€çš„ç‰¹æ®Šæ ‡è®°ã€‚"
[quicktour/hf]: https://huggingface.co/docs/tokenizers/quicktour 'The main API of the library is the class Tokenizer, here is how we instantiate one with a BPE model: // è¯¥åº“çš„ä¸»è¦ API æ˜¯ Tokenizer class ï¼Œä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨ BPE æ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ª Tokenizer ç±»çš„æ–¹æ³•ï¼š /// (: from tokenizers import Tokenizer ;: from tokenizers.models import BPE ;: tokenizer = Tokenizer(BPE(unk_token="[UNK]")) ;: # py lib) (: use tokenizers::models::bpe::BPE ;: let mut tokenizer: TokenizerImpl<BPE,NormalizerWrapper,PreTokenizerWrapper,PostProcessorWrapper,DecoderWrapper> = TokenizerImpl::new(BPE::builder().unk_token("[UNK]".to_string()).build().ok_or("build error")) ;: // rust lib) (: { Tokenizer } = require("tokenizers") ;: { BPE } = require("tokenizers") ;: tokenizer = new Tokenizer(BPE.init({}, [], { unkToken: "[UNK]" })) ;: // nodejs lib) /// To train our tokenizer on the wikitext files, we will need to instantiate a [trainer]{.title-ref}, in this case a BpeTrainer // ä¸ºäº†åœ¨ç»´åŸºæ–‡æœ¬æ–‡ä»¶ä¸Šè®­ç»ƒæˆ‘ä»¬çš„åˆ†è¯å™¨ï¼Œæˆ‘ä»¬éœ€è¦ï¼š å®ä¾‹åŒ–ä¸€ä¸ª [trainer]{.title-ref}ï¼Œåœ¨æœ¬ä¾‹ä¸­ä¸º BpeTrainer /// (: from tokenizers.trainers import BpeTrainer ;: trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"]) ;: # py lib) (: use tokenizers::models::bpe::BpeTrainer ;: let mut trainer = BpeTrainer::builder().special_tokens(vec![AddedToken::from("[UNK]", true),AddedToken::from("[CLS]", true),AddedToken::from("[SEP]", true),AddedToken::from("[PAD]", true),AddedToken::from("[MASK]", true)]).build() ;: // rust lib) (: { bpeTrainer } = require("tokenizers") ;: trainer = bpeTrainer({ ecialTokens: ["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"] }) ;: // nodejs lib) /// ...'
[knows_by]: https://blog.cloudflare.com/cloudflares-most-efficient-ai-inference-engine/ "How we built the most efficient inference engine for Cloudflareâ€™s network // æˆ‘ä»¬å¦‚ä½•ä¸º Cloudflare ç½‘ç»œæ„å»ºæœ€é«˜æ•ˆçš„æ¨ç†å¼•æ“ /// ... /// After running most of our models on the widely used open source inference and serving engine vLLM, we figured out it didnâ€™t allow us to fully utilize the GPUs at the edge. Although it can run on a very wide range of hardware, from personal devices to data centers, it is best optimized for large data centers. When run as a dedicated inference server on powerful hardware serving a specific model, vLLM truly shines. However, it is much less optimized for dynamic workloads, distributed networks, and for the unique security constraints of running inference at the edge alongside other services. // åœ¨å¹¿æ³›ä½¿ç”¨çš„å¼€æºæ¨ç†å’ŒæœåŠ¡å¼•æ“ vLLM ä¸Šè¿è¡Œäº†æˆ‘ä»¬çš„å¤§éƒ¨åˆ†æ¨¡å‹åï¼Œæˆ‘ä»¬å‘ç°å®ƒæ— æ³•å……åˆ†åˆ©ç”¨è¾¹ç¼˜ç«¯çš„ GPUã€‚å°½ç®¡ vLLM å¯ä»¥åœ¨ä»ä¸ªäººè®¾å¤‡åˆ°æ•°æ®ä¸­å¿ƒçš„å„ç§ç¡¬ä»¶ä¸Šè¿è¡Œï¼Œä½†å®ƒé’ˆå¯¹å¤§å‹æ•°æ®ä¸­å¿ƒè¿›è¡Œäº†æœ€ä½³ä¼˜åŒ–ã€‚å½“ä½œä¸ºä¸“ç”¨æ¨ç†æœåŠ¡å™¨è¿è¡Œåœ¨é«˜æ€§èƒ½ç¡¬ä»¶ä¸Šï¼ŒæœåŠ¡äºç‰¹å®šæ¨¡å‹æ—¶ï¼ŒvLLM çš„æ€§èƒ½ç¡®å®éå¸¸å‡ºè‰²ã€‚ç„¶è€Œï¼Œå¯¹äºåŠ¨æ€å·¥ä½œè´Ÿè½½ã€åˆ†å¸ƒå¼ç½‘ç»œä»¥åŠåœ¨è¾¹ç¼˜ç«¯ä¸å…¶ä»–æœåŠ¡ä¸€èµ·è¿è¡Œæ¨ç†æ—¶æ‰€é¢ä¸´çš„ç‹¬ç‰¹å®‰å…¨é™åˆ¶ï¼ŒvLLM çš„ä¼˜åŒ–ç¨‹åº¦è¦ä½å¾—å¤šã€‚ /// Thatâ€™s why we decided to build something that will be able to meet the needs of Cloudflare inference workloads for years to come. Infire is an LLM inference engine, written in Rust, that employs a range of techniques to maximize memory, network I/O, and GPU utilization. It can serve more requests with fewer GPUs and significantly lower CPU overhead, saving time, resources, and energy across our network.  // å› æ­¤ï¼Œæˆ‘ä»¬å†³å®šæ„å»ºä¸€ä¸ªèƒ½å¤Ÿæ»¡è¶³ Cloudflare æœªæ¥æ•°å¹´æ¨ç†å·¥ä½œè´Ÿè½½éœ€æ±‚çš„è§£å†³æ–¹æ¡ˆã€‚Infire æ˜¯ä¸€ä¸ªç”¨ Rust ç¼–å†™çš„ LLM æ¨ç†å¼•æ“ï¼Œå®ƒé‡‡ç”¨äº†ä¸€ç³»åˆ—æŠ€æœ¯æ¥æœ€å¤§é™åº¦åœ°åˆ©ç”¨å†…å­˜ã€ç½‘ç»œ I/O å’Œ GPUã€‚å®ƒèƒ½å¤Ÿç”¨æ›´å°‘çš„ GPU å¤„ç†æ›´å¤šè¯·æ±‚ï¼Œå¹¶æ˜¾è‘—é™ä½ CPU å¼€é”€ï¼Œä»è€ŒèŠ‚çœæ•´ä¸ªç½‘ç»œçš„æ—¶é—´ã€èµ„æºå’Œèƒ½æºã€‚ /// ... /// After a request is deemed valid, Infire will pass it to the tokenizer, which transforms the raw text into a series of tokens, or numbers that the model can consume. Different models use different kinds of tokenizers, but the most popular ones use byte-pair encoding. For tokenization, we use HuggingFace's tokenizers crate. The tokenized prompts and params are then sent to the batcher, and scheduled for processing on the GPU, where they will be processed as vectors of numbers, called embeddings. // è¯·æ±‚è¢«åˆ¤å®šä¸ºæœ‰æ•ˆåï¼ŒInfire ä¼šå°†å…¶ä¼ é€’ç»™åˆ†è¯å™¨ï¼Œåˆ†è¯å™¨ä¼šå°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºä¸€ç³»åˆ—æ ‡è®°ï¼ˆæˆ–æ•°å­—ï¼‰ï¼Œä¾›æ¨¡å‹ä½¿ç”¨ã€‚ä¸åŒçš„æ¨¡å‹ä½¿ç”¨ä¸åŒçš„åˆ†è¯å™¨ï¼Œä½†æœ€å¸¸ç”¨çš„åˆ†è¯å™¨æ˜¯å­—èŠ‚å¯¹ç¼–ç ã€‚åˆ†è¯æ–¹é¢ï¼Œæˆ‘ä»¬ä½¿ç”¨ HuggingFace çš„ tokenizers crateã€‚åˆ†è¯åçš„æç¤ºå’Œå‚æ•°éšåä¼šè¢«å‘é€åˆ°æ‰¹å¤„ç†ç¨‹åºï¼Œå¹¶è°ƒåº¦åˆ° GPU ä¸Šè¿›è¡Œå¤„ç†ï¼Œæœ€ç»ˆç”Ÿæˆç§°ä¸ºåµŒå…¥ï¼ˆembeddingï¼‰çš„æ•°å­—å‘é‡ã€‚ /// ..."

