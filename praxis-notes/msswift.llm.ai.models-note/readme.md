[src/gh]: https://github.com/modelscope/ms-swift.git "(Apache-2.0) (Languages: Python 99.7%, Other 0.3%) Use PEFT or Full-parameter to CPT/SFT/DPO/GRPO 500+ LLMs (Qwen3, Qwen3-MoE, Llama4, InternLM3, GLM4, Mistral, Yi1.5, DeepSeek-R1, ...) and 200+ MLLMs (Qwen2.5-VL, Qwen2.5-Omni, Qwen2-Audio, Ovis2, InternVL3, Llava, MiniCPM-V-2.6, GLM4v, Xcomposer2.5, DeepSeek-VL2, Phi4, GOT-OCR2, ...). // ä½¿ç”¨ PEFT æˆ–å…¨å‚æ•° CPT/SFT/DPO/GRPO 500+ LLMsï¼ˆQwen3, Qwen3-MoE, Llama4, InternLM3, GLM4, Mistral, Yi1.5, DeepSeek-R1, ...ï¼‰å’Œ 200+ MLLMsï¼ˆQwen2.5-VL, Qwen2.5-Omni, Qwen2-Audio, Ovis2, InternVL3, Llava, MiniCPM-V-2.6, GLM4v, Xcomposer2.5, DeepSeek-VL2, Phi4, GOT-OCR2, ...ï¼‰ã€‚ /// SWIFT (Scalable lightWeight Infrastructure for Fine-Tuning) // SWIFT (å¯æ‰©å±•è½»é‡çº§åŸºç¡€è®¾æ–½ç”¨äºå¾®è°ƒ) /// ğŸ² ms-swift is an official framework provided by the ModelScope community for fine-tuning and deploying large language models and multi-modal large models. It currently supports the training (pre-training, fine-tuning, human alignment), inference, evaluation, quantization, and deployment of 450+ large models and 150+ multi-modal large models. These large language models (LLMs) include models such as Qwen3, Qwen3-MoE, Qwen2.5, InternLM3, GLM4, Mistral, DeepSeek-R1, Yi1.5, TeleChat2, Baichuan2, and Gemma2. The multi-modal LLMs include models such as Qwen2.5-VL, Qwen2-Audio, Llama3.4, Llava, InternVL2.5, MiniCPM-V-2.6, GLM4v, Xcomposer2.5, Yi-VL, DeepSeek-VL2, Phi3.5-Vision, and GOT-OCR2. // ms-swift æ˜¯ ModelScope ç¤¾åŒºæä¾›çš„å®˜æ–¹æ¡†æ¶ï¼Œç”¨äºå¾®è°ƒå’Œéƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€å¤§å‹æ¨¡å‹ã€‚å®ƒç›®å‰æ”¯æŒ 450 å¤šä¸ªå¤§å‹æ¨¡å‹å’Œ 150 å¤šä¸ªå¤šæ¨¡æ€å¤§å‹æ¨¡å‹çš„è®­ç»ƒï¼ˆé¢„è®­ç»ƒã€å¾®è°ƒã€äººå·¥å¯¹é½ï¼‰ã€æ¨ç†ã€è¯„ä¼°ã€é‡åŒ–ä»¥åŠéƒ¨ç½²ã€‚è¿™äº›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åŒ…æ‹¬ Qwen3ã€Qwen3-MoEã€Qwen2.5ã€InternLM3ã€GLM4ã€Mistralã€DeepSeek-R1ã€Yi1.5ã€TeleChat2ã€Baichuan2 å’Œ Gemma2 ç­‰æ¨¡å‹ã€‚å¤šæ¨¡æ€ LLMs åŒ…æ‹¬ Qwen2.5-VLã€Qwen2-Audioã€Llama3.4ã€Llavaã€InternVL2.5ã€MiniCPM-V-2.6ã€GLM4vã€Xcomposer2.5ã€Yi-VLã€DeepSeek-VL2ã€Phi3.5-Vision å’Œ GOT-OCR2 ç­‰æ¨¡å‹ã€‚ /// ğŸ” Additionally, ms-swift incorporates the latest training technologies, including lightweight techniques such as LoRA, QLoRA, Llama-Pro, LongLoRA, GaLore, Q-GaLore, LoRA+, LISA, DoRA, FourierFt, ReFT, UnSloth, and Liger, as well as human alignment training methods like DPO, GRPO, RM, PPO, KTO, CPO, SimPO, and ORPO. ms-swift supports acceleration of inference, evaluation, and deployment modules using vLLM and LMDeploy, and it supports model quantization with technologies like GPTQ, AWQ, and BNB. Furthermore, ms-swift offers a Gradio-based Web UI and a wealth of best practices. // ğŸ” æ­¤å¤–ï¼Œms-swift é›†æˆäº†æœ€æ–°çš„è®­ç»ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬è½»é‡çº§æŠ€æœ¯å¦‚ LoRAã€QLoRAã€Llama-Proã€LongLoRAã€GaLoreã€Q-GaLoreã€LoRA+ã€LISAã€DoRAã€FourierFtã€ReFTã€UnSloth å’Œ Ligerï¼Œä»¥åŠäººç±»å¯¹é½è®­ç»ƒæ–¹æ³•å¦‚ DPOã€GRPOã€RMã€PPOã€KTOã€CPOã€SimPO å’Œ ORPOã€‚ms-swift æ”¯æŒä½¿ç”¨ vLLM å’Œ LMDeploy åŠ é€Ÿæ¨ç†ã€è¯„ä¼°å’Œéƒ¨ç½²æ¨¡å—ï¼Œå¹¶æ”¯æŒä½¿ç”¨ GPTQã€AWQ å’Œ BNB ç­‰æŠ€æœ¯è¿›è¡Œæ¨¡å‹é‡åŒ–ã€‚æ­¤å¤–ï¼Œms-swift æä¾›åŸºäº Gradio çš„ Web UI å’Œä¸°å¯Œçš„æœ€ä½³å®è·µã€‚ /// Why choose ms-swift? // ä¸ºä»€ä¹ˆé€‰æ‹© ms-swiftï¼Ÿ /// - ğŸ Model Types: Supports 450+ pure text large models, 150+ multi-modal large models, as well as All-to-All multi-modal models, sequence classification models, and embedding models, covering the entire process from training to deployment. // ğŸ æ¨¡å‹ç±»å‹ï¼šæ”¯æŒ 450+çº¯æ–‡æœ¬å¤§å‹æ¨¡å‹ï¼Œ150+å¤šæ¨¡æ€å¤§å‹æ¨¡å‹ï¼Œä»¥åŠå…¨å…¨å¤šæ¨¡æ€æ¨¡å‹ã€åºåˆ—åˆ†ç±»æ¨¡å‹å’ŒåµŒå…¥æ¨¡å‹ï¼Œæ¶µç›–ä»è®­ç»ƒåˆ°éƒ¨ç½²çš„æ•´ä¸ªæµç¨‹ã€‚ /// - Dataset Types: Comes with 150+ pre-training, fine-tuning, human alignment, multi-modal datasets, and supports custom datasets. // æ•°æ®é›†ç±»å‹ï¼šåŒ…å« 150+é¢„è®­ç»ƒã€å¾®è°ƒã€äººå·¥å¯¹é½ã€å¤šæ¨¡æ€æ•°æ®é›†ï¼Œå¹¶æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†ã€‚ /// - Hardware Support: Compatible with CPU, RTX series, T4/V100, A10/A100/H100, Ascend NPU, MPS, etc. // ç¡¬ä»¶æ”¯æŒï¼šå…¼å®¹ CPUã€RTX ç³»åˆ—ã€T4/V100ã€A10/A100/H100ã€Ascend NPUã€MPS ç­‰ã€‚ /// - ğŸŠ Lightweight Training: Supports lightweight fine-tuning methods like LoRA, QLoRA, DoRA, LoRA+, ReFT, RS-LoRA, LLaMAPro, Adapter, GaLore, Q-Galore, LISA, UnSloth, Liger-Kernel. // ğŸŠ è½»é‡çº§è®­ç»ƒï¼šæ”¯æŒè½»é‡çº§å¾®è°ƒæ–¹æ³•ï¼Œå¦‚ LoRAã€QLoRAã€DoRAã€LoRA+ã€ReFTã€RS-LoRAã€LLaMAProã€Adapterã€GaLoreã€Q-Galoreã€LISAã€UnSlothã€Liger-Kernelã€‚ /// - Distributed Training: Supports distributed data parallel (DDP), device_map simple model parallelism, DeepSpeed ZeRO2/ZeRO3, FSDP, and other distributed training techniques. // åˆ†å¸ƒå¼è®­ç»ƒï¼šæ”¯æŒåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼ˆDDPï¼‰ã€è®¾å¤‡æ˜ å°„ç®€å•æ¨¡å‹å¹¶è¡Œã€DeepSpeed ZeRO2/ZeRO3ã€FSDP ä»¥åŠå…¶ä»–åˆ†å¸ƒå¼è®­ç»ƒæŠ€æœ¯ã€‚ /// - Quantization Training: Supports training quantized models like BNB, AWQ, GPTQ, AQLM, HQQ, EETQ. // é‡åŒ–è®­ç»ƒï¼šæ”¯æŒè®­ç»ƒé‡åŒ–æ¨¡å‹ï¼Œå¦‚ BNBã€AWQã€GPTQã€AQLMã€HQQã€EETQã€‚ /// - RLHF Training: Supports human alignment training methods such as DPO, GRPO, RM, PPO, KTO, CPO, SimPO, ORPO for both pure text and multi-modal large models. // RLHF è®­ç»ƒï¼šæ”¯æŒ DPOã€GRPOã€RMã€PPOã€KTOã€CPOã€SimPOã€ORPO ç­‰äººç±»å¯¹é½è®­ç»ƒæ–¹æ³•ï¼Œé€‚ç”¨äºçº¯æ–‡æœ¬å’Œå¤šæ¨¡æ€å¤§å‹æ¨¡å‹ã€‚ /// - ğŸ“ Multi-Modal Training: Supports training on different modalities like images, videos, and audio, for tasks like VQA, captioning, OCR, and grounding. // ğŸ“ å¤šæ¨¡æ€è®­ç»ƒï¼šæ”¯æŒåœ¨å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ç­‰ä¸åŒæ¨¡æ€ä¸Šè¿›è¡Œè®­ç»ƒï¼Œç”¨äº VQAã€å­—å¹•ã€OCR å’Œå®šä½ç­‰ä»»åŠ¡ã€‚ /// - Interface Training: Provides capabilities for training, inference, evaluation, quantization through an interface, completing the whole large model pipeline. // æ¥å£è®­ç»ƒï¼šé€šè¿‡æ¥å£æä¾›è®­ç»ƒã€æ¨ç†ã€è¯„ä¼°ã€é‡åŒ–ç­‰åŠŸèƒ½ï¼Œå®Œæˆæ•´ä¸ªå¤§æ¨¡å‹ç®¡é“ã€‚ /// - Plugin and Extension: Supports custom model and dataset extensions, as well as customization of components like loss, metric, trainer, loss-scale, callback, optimizer. // æ’ä»¶å’Œæ‰©å±•ï¼šæ”¯æŒè‡ªå®šä¹‰æ¨¡å‹å’Œæ•°æ®é›†æ‰©å±•ï¼Œä»¥åŠç»„ä»¶å¦‚æŸå¤±ã€åº¦é‡ã€è®­ç»ƒå™¨ã€æŸå¤±ç¼©æ”¾ã€å›è°ƒã€ä¼˜åŒ–å™¨çš„è‡ªå®šä¹‰ã€‚ /// - ğŸ‰ Toolbox Capabilities: Offers not only training support for large models and multi-modal large models but also covers the entire process of inference, evaluation, quantization, and deployment. // ğŸ‰ å·¥å…·ç®±åŠŸèƒ½ï¼šä¸ä»…æä¾›å¤§å‹æ¨¡å‹å’Œå¤šæ¨¡æ€å¤§å‹æ¨¡å‹çš„è®­ç»ƒæ”¯æŒï¼Œè¿˜æ¶µç›–æ¨ç†ã€è¯„ä¼°ã€é‡åŒ–ä»¥åŠéƒ¨ç½²çš„æ•´ä¸ªè¿‡ç¨‹ã€‚ /// - Inference Acceleration: Supports inference acceleration engines like PyTorch, vLLM, LmDeploy, and provides OpenAI API for accelerating inference, deployment, and evaluation modules. // æ¨ç†åŠ é€Ÿï¼šæ”¯æŒ PyTorchã€vLLMã€LmDeploy ç­‰æ¨ç†åŠ é€Ÿå¼•æ“ï¼Œå¹¶æä¾› OpenAI API ä»¥åŠ é€Ÿæ¨ç†ã€éƒ¨ç½²å’Œè¯„ä¼°æ¨¡å—ã€‚ /// - Model Evaluation: Uses EvalScope as the evaluation backend and supports evaluation on 100+ datasets for both pure text and multi-modal models. // æ¨¡å‹è¯„ä¼°ï¼šä½¿ç”¨ EvalScope ä½œä¸ºè¯„ä¼°åç«¯ï¼Œæ”¯æŒå¯¹ 100+æ•°æ®é›†è¿›è¡Œçº¯æ–‡æœ¬å’Œå¤šæ¨¡æ€æ¨¡å‹çš„è¯„ä¼°ã€‚ /// - Model Quantization: Supports AWQ, GPTQ, and BNB quantized exports, with models that can use vLLM/LmDeploy for inference acceleration and continue training. // æ¨¡å‹é‡åŒ–ï¼šæ”¯æŒ AWQã€GPTQ å’Œ BNB é‡åŒ–å¯¼å‡ºï¼Œæ”¯æŒä½¿ç”¨ vLLM/LmDeploy è¿›è¡Œæ¨ç†åŠ é€Ÿå¹¶ç»§ç»­è®­ç»ƒçš„æ¨¡å‹ã€‚"

[qs.zh/docs]: https://swift.readthedocs.io/zh-cn/latest/GetStarted/%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B.html
