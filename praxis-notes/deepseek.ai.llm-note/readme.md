
[site]: https://deepseek.com/
[chat/site]: https://chat.deepseek.com/
[api.plat/site]: https://platform.deepseek.com/

[site/hf]: https://huggingface.co/deepseek-ai "DeepSeek (深度求索), founded in 2023, is a Chinese company dedicated to making AGI a reality. /// Unravel the mystery of AGI with curiosity. Answer the essential question with long-termism."
[site/gh]: https://github.com/deepseek-ai


[integration.src/gh]: https://github.com/deepseek-ai/awesome-deepseek-integration.git "(CC0-1.0) Awesome DeepSeek Integrations /// Integrate the DeepSeek API into popular softwares. Access DeepSeek Open Platform to get an API key. // DeepSeek 实用集成 /// 将 DeepSeek 大模型能力轻松接入各类软件。访问 DeepSeek 开放平台来获取您的 API key。"

[R1.src/gh]: https://github.com/deepseek-ai/DeepSeek-R1.git "(MIT) We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning. With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. However, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. To support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models. // 我们介绍了第一代推理模型， DeepSeek-R1-Zero 和 DeepSeek-R1 。 DeepSeek-R1-Zero 是通过大规模增强学习（RL）训练的模型，而没有监督微调（SFT）作为初步的步骤，在推理上表现出了显着的表现。使用 RL ， DeepSeek-R1-Zero 自然而然地出现了许多强大而有趣的推理行为。但是， DeepSeek-R1-Zero 遇到了挑战，例如无尽的重复，不良的可读性和语言混合。为了解决这些问题并进一步提高推理性能，我们介绍了 DeepSeek-R1 ，该 deepSeek-r1 在 RL 之前包含了冷启动数据。 DeepSeek-R1 在数学，代码和推理任务中实现与 OpenAI-O1 相当的性能。为了支持研究社区，我们已经开源的 DeepSeek-R1-Zero ， DeepSeek-R1 和六个密集的型号，该模型与 Llama 和 Qwen 的 DeepSeek-R1 蒸馏出来。 DeepSeek-R1-Distill-Qwen-32b 在各种基准测试中胜过 OpenAi-O1-Mini ，为密集模型实现了新的最新结果。"
[R1.coll/hf]: https://huggingface.co/collections/deepseek-ai/deepseek-r1-678e1e131c0169c0bc89728d "(各个版本的 R1 模型： R1, R1-Zero, 以及各蒸馏。)"
[R1.src/hf]: https://huggingface.co/deepseek-ai/DeepSeek-R1 "(Model size: 685B params) (Tensor type: BF16·F8_E4M3·F32) (MIT)"
[R1-Zero.src/hf]: https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero "(Model size: 685B params) (Tensor type: BF16·F8_E4M3·F32) (MIT)"
[R1-Distill-Llama-70B.src/hf]: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B "(Model size: 70.6B params) (Tensor type: BF16) (MIT, LLAMA-3.3)"
[R1-Distill-Qwen-32B.src/hf]: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B "(Model size: 32.8B params) (Tensor type: BF16) (MIT, Apache-2.0)"
[R1-Distill-Qwen-14B.src/hf]: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B "(Model size: 14.8B params) (Tensor type: BF16) (MIT, Apache-2.0)"
[R1-Distill-Llama-8B.src/hf]: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B "(Model size: 8.03B params) (Tensor type: BF16) (MIT, LLAMA-3.1)"
[R1-Distill-Qwen-7B.src/hf]: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B "(Model size: 7.62B params) (Tensor type: BF16) (MIT, Apache-2.0)"
[R1-Distill-Qwen-1.5B.src/hf]: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B "(Model size: 1.78B params) (Tensor type: BF16) (MIT, Apache-2.0)"

[Coder.coll/hf]: https://huggingface.co/collections/deepseek-ai/deepseek-coder-65f295d7d8a0a29fe39b4ec4
[Coder-V2/hf]: https://huggingface.co/collections/deepseek-ai/deepseekcoder-v2-666bf4b274a5f556827ceeca
[Math/hf]: https://huggingface.co/collections/deepseek-ai/deepseek-math-65f2962739da11599e441681
[Prover/hf]: https://huggingface.co/collections/deepseek-ai/deepseek-prover-66beb212ae70890c90f24176

[janus.src/gh]: https://github.com/deepseek-ai/Janus.git "(MIT, DEEPSEEK-1.0) (Languages: Python 98.5%, Makefile 1.5%) Janus-Series: Unified Multimodal Understanding and Generation Models"

[ios-shortcuts/icloud]: https://www.icloud.com/shortcuts/b75899492ead45ef9a47cfce89334bf0 "深度求索"
[r1.ollama/ollama]: https://ollama.com/library/deepseek-r1 "(: ollama run deepseek-r1:latest # 0a8c26691023 • 4.7GB) DeepSeek's first-generation of reasoning models with comparable performance to OpenAI-o1, including six dense models distilled from DeepSeek-R1 based on Llama and Qwen."
[r1-llama-8b.gguf/lmstudio]: https://lmstudio.ai/model/deepseek-r1-llama-8b "(4.92 GB) (huggingface.co: lmstudio-community/DeepSeek-R1-Distill-Llama-8B-GGUF) DeepSeek R1 distilled into Llama 8B: a powerful reasoning model in a small package"
[r1-qwen-7b.gguf/lmstudio]: https://lmstudio.ai/model/deepseek-r1-qwen-7b "(4.68 GB) (huggingface.co: lmstudio-community/DeepSeek-R1-Distill-Qwen-7B-GGUF) DeepSeek R1 distilled into Qwen 7B: a powerful reasoning model in a small package"

[twitter]: https://twitter.com/deepseek_ai
