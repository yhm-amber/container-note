
[site]: https://deepseek.com/
[chat/site]: https://chat.deepseek.com/
[api.plat/site]: https://platform.deepseek.com/

[site/hf]: https://huggingface.co/deepseek-ai "DeepSeek (深度求索), founded in 2023, is a Chinese company dedicated to making AGI a reality. /// Unravel the mystery of AGI with curiosity. Answer the essential question with long-termism."
[site/gh]: https://github.com/deepseek-ai


[integration.src/gh]: https://github.com/deepseek-ai/awesome-deepseek-integration.git "(CC0-1.0) Awesome DeepSeek Integrations /// Integrate the DeepSeek API into popular softwares. Access DeepSeek Open Platform to get an API key. // DeepSeek 实用集成 /// 将 DeepSeek 大模型能力轻松接入各类软件。访问 DeepSeek 开放平台来获取您的 API key。"

[R1.src/gh]: https://github.com/deepseek-ai/DeepSeek-R1.git "(MIT) We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning. With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. However, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. To support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models. // 我们介绍了第一代推理模型， DeepSeek-R1-Zero 和 DeepSeek-R1 。 DeepSeek-R1-Zero 是通过大规模增强学习（RL）训练的模型，而没有监督微调（SFT）作为初步的步骤，在推理上表现出了显着的表现。使用 RL ， DeepSeek-R1-Zero 自然而然地出现了许多强大而有趣的推理行为。但是， DeepSeek-R1-Zero 遇到了挑战，例如无尽的重复，不良的可读性和语言混合。为了解决这些问题并进一步提高推理性能，我们介绍了 DeepSeek-R1 ，该 deepSeek-r1 在 RL 之前包含了冷启动数据。 DeepSeek-R1 在数学，代码和推理任务中实现与 OpenAI-O1 相当的性能。为了支持研究社区，我们已经开源的 DeepSeek-R1-Zero ， DeepSeek-R1 和六个密集的型号，该模型与 Llama 和 Qwen 的 DeepSeek-R1 蒸馏出来。 DeepSeek-R1-Distill-Qwen-32b 在各种基准测试中胜过 OpenAi-O1-Mini ，为密集模型实现了新的最新结果。"
[R1.coll/hf]: https://huggingface.co/collections/deepseek-ai/deepseek-r1-678e1e131c0169c0bc89728d "(各个版本的 R1 模型： R1, R1-Zero, 以及各蒸馏。)"
[R1.model/hf]: https://huggingface.co/deepseek-ai/DeepSeek-R1 "(Model size: 685B params) (Tensor type: BF16·F8_E4M3·F32) (MIT)"
[R1-Zero.model/hf]: https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero "(Model size: 685B params) (Tensor type: BF16·F8_E4M3·F32) (MIT)"
[R1-Distill-Llama-70B.model/hf]: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B "(Model size: 70.6B params) (Tensor type: BF16) (MIT, LLAMA-3.3)"
[R1-Distill-Qwen-32B.model/hf]: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B "(Model size: 32.8B params) (Tensor type: BF16) (MIT, Apache-2.0)"
[R1-Distill-Qwen-14B.model/hf]: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B "(Model size: 14.8B params) (Tensor type: BF16) (MIT, Apache-2.0)"
[R1-Distill-Llama-8B.model/hf]: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B "(Model size: 8.03B params) (Tensor type: BF16) (MIT, LLAMA-3.1)"
[R1-Distill-Qwen-7B.model/hf]: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B "(Model size: 7.62B params) (Tensor type: BF16) (MIT, Apache-2.0)"
[R1-Distill-Qwen-1.5B.model/hf]: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B "(Model size: 1.78B params) (Tensor type: BF16) (MIT, Apache-2.0)"






[OCR.src/gh]: https://github.com/deepseek-ai/DeepSeek-OCR.git "(MIT) (Languages: Python 100.0%) Contexts Optical Compression // 上下文光学压缩"
[OCR.paper/arxiv]: https://arxiv.org/abs/2510.18234 "We present DeepSeek-OCR as an initial investigation into the feasibility of compressing long contexts via optical 2D mapping. DeepSeek-OCR consists of two components: DeepEncoder and DeepSeek3B-MoE-A570M as the decoder. Specifically, DeepEncoder serves as the core engine, designed to maintain low activations under high-resolution input while achieving high compression ratios to ensure an optimal and manageable number of vision tokens. Experiments show that when the number of text tokens is within 10 times that of vision tokens (i.e., a compression ratio < 10x), the model can achieve decoding (OCR) precision of 97%. Even at a compression ratio of 20x, the OCR accuracy still remains at about 60%. This shows considerable promise for research areas such as historical long-context compression and memory forgetting mechanisms in LLMs. Beyond this, DeepSeek-OCR also demonstrates high practical value. On OmniDocBench, it surpasses GOT-OCR2.0 (256 tokens/page) using only 100 vision tokens, and outperforms MinerU2.0 (6000+ tokens per page on average) while utilizing fewer than 800 vision tokens. In production, DeepSeek-OCR can generate training data for LLMs/VLMs at a scale of 200k+ pages per day (a single A100-40G). // 我们提出 DeepSeek-OCR 作为一项初步研究，探索通过光学二维映射压缩长上下文的可行性。DeepSeek-OCR 包含两个组件：DeepEncoder 和 DeepSeek3B-MoE-A570M 作为解码器。具体而言，DeepEncoder 作为核心引擎，设计用于在高清输入下保持低激活状态，同时实现高压缩率，以确保视觉 token 数量达到最优且易于管理。实验表明，当文本 token 数量是视觉 token 数量的 10 倍以内（即压缩率 < 10x）时，模型可以实现 97% 的解码（OCR）精度。即使在 20x 的压缩率下，OCR 准确率仍保持在约 60%。这为历史长上下文压缩和 LLMs 中的记忆遗忘机制等研究领域展现了相当大的潜力。除此之外，DeepSeek-OCR 也展现出很高的实用价值。在 OmniDocBench 上，它使用仅 100 个视觉 token 就超越了 GOT-OCR2.0（每页 256 个 token），同时使用少于 800 个视觉 token 就优于 MinerU2.0（平均每页 6000+ 个 token）。 在生产环境中，DeepSeek-OCR 能够以每天 20 万页（单台 A100-40G）的规模为 LLMs/VLMs 生成训练数据。"
[OCR.model/hf]: https://huggingface.co/deepseek-ai/DeepSeek-OCR "(Model size: 3B params) (Tensor type: BF16) (MIT) (ArXiv: 2510.18234)"
[OCR.site/<some-unofficial-A>]: https://deepseek-ocr.ai/ "(This site is not affiliated with DeepSeek.com. // 本网站与 DeepSeek.com 无关。) Features // 特性 /// Everything you need to turn images and PDFs into usable text. // 将图像和 PDF 转换为可用文本所需的一切。 /// - Fast extraction // 快速提取 ///: Processes pages quickly and keeps formatting stable for consistent results. // 快速处理页面并保持格式稳定，以获得一致的结果。 /// - Works on long documents // 适用于长文档 ///: Handles multi‑page PDFs and large scans without skipping content. // 可处理多页 PDF 和大文件扫描，不会遗漏内容。 /// - Lower cost with fewer tokens // 使用更少的 token 降低成本 ///: Efficient processing uses fewer tokens so you pay less. // 高效处理使用更少的 token，因此支付更少。 /// - Accurate tables and layout // 精确的表格和布局 ///: Extracts clean CSV tables and preserves page structure where it matters. // 提取干净的 CSV 表格，并在重要位置保留页面结构。 /// - Handles complex layouts // 处理复杂的布局 ///: Understands columns, headers, captions, and figures for clean output. // 理解列、标题、说明文字和图表，以获得清晰的输出。 /// - Multilingual text // 多语言文本 ///: Recognizes English, Chinese, and mixed documents with proper punctuation. // 识别英语、中文和混合文档，并正确使用标点符号。"
[OCR.try/<some-unofficial-A>]: https://.deepseek-ocr.ai/app "(This site is not affiliated with DeepSeek.com. // 本网站与 DeepSeek.com 无关。)"








[Coder.coll/hf]: https://huggingface.co/collections/deepseek-ai/deepseek-coder-65f295d7d8a0a29fe39b4ec4
[Coder-V2.coll/hf]: https://huggingface.co/collections/deepseek-ai/deepseekcoder-v2-666bf4b274a5f556827ceeca
[Math.coll/hf]: https://huggingface.co/collections/deepseek-ai/deepseek-math-65f2962739da11599e441681
[Prover.coll/hf]: https://huggingface.co/collections/deepseek-ai/deepseek-prover-66beb212ae70890c90f24176

[janus.src/gh]: https://github.com/deepseek-ai/Janus.git "(MIT, DEEPSEEK-1.0) (Languages: Python 98.5%, Makefile 1.5%) Janus-Series: Unified Multimodal Understanding and Generation Models // Janus 系列: 统一的多模态理解和生成模型 /// Janus-Pro is an advanced version of the previous work Janus. Specifically, Janus-Pro incorporates (1) an optimized training strategy, (2) expanded training data, and (3) scaling to larger model size. With these improvements, Janus-Pro achieves significant advancements in both multimodal understanding and text-to-image instruction-following capabilities, while also enhancing the stability of text-to-image generation. // Janus-Pro 是先前作品 Janus 的高级版本。具体而言， Janus-Pro 结合了 (1) 优化的训练策略， (2) 扩展的训练数据， (3) 扩展到更大的模型大小。通过这些改进， Janus-Pro 在多模态的理解和文本对图像遵循能力方面都取得了重大进步，同时还提高了文本对图像生成的稳定性。 /// Janus is a novel autoregressive framework that unifies multimodal understanding and generation. It addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder’s roles in understanding and generation, but also enhances the framework’s flexibility. Janus surpasses previous unified model and matches or exceeds the performance of task-specific models. The simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models. // 贾努斯（Janus）是一个新颖的自动回归框架，可以统一多模态的理解和产生。它通过将视觉编码解码为单独的路径来解决以前方法的局限性，同时仍利用单个统一的变压器体系结构进行处理。脱钩不仅减轻了视觉编码器在理解和生成中的作用之间的冲突，而且还可以增强框架的灵活性。 Janus 超过了以前的统一模型，并超过了特定于任务模型的性能。 Janus 的简单性，高灵活性和有效性使其成为下一代统一多模态模型的有力候选。 /// JanusFlow introduces a minimalist architecture that integrates autoregressive language models with rectified flow, a state-of-the-art method in generative modeling. Our key finding demonstrates that rectified flow can be straightforwardly trained within the large language model framework, eliminating the need for complex architectural modifications. Extensive experiments show that JanusFlow achieves comparable or superior performance to specialized models in their respective domains, while significantly outperforming existing unified approaches across standard benchmarks. This work represents a step toward more efficient and versatile vision-language models. // Janusflow 推出了一种简约的体系结构，该体系结构将自回归语言模型与整流流程（一种生成建模的最新方法）集成在一起。我们的关键发现表明，可以在大语言模型框架中直接训练整流的流程，从而消除了对复杂的建筑修改的需求。广泛的实验表明， Janusflow 具有与其各自域中的专业模型相当或出色的性能，同时显着超过了跨标准基准的现有统一方法。这项工作代表了迈向更高效，更通用的视觉语言模型的一步。"
[Janus-Pro-7B.model/hf]: https://huggingface.co/deepseek-ai/Janus-Pro-7B "(transformers model with pipeline type any-to-any) (Sequence Length: 4096) Janus-Pro is a unified understanding and generation MLLM, which decouples visual encoding for multimodal understanding and generation. Janus-Pro is constructed based on the DeepSeek-LLM-1.5b-base/DeepSeek-LLM-7b-base. // Janus-Pro 是一种统一的理解和生成 MLLM ，它将视觉编码解耦为多模态的理解和生成。 Janus-Pro 基于 DeepSeek-LLM-1.5b-base/DeepSeek-LLM-7b-base 建造。 /// For multimodal understanding, it uses the SigLIP-L (huggingface.co: timm/ViT-L-16-SigLIP-384) as the vision encoder, which supports 384 x 384 image input. For image generation, Janus-Pro uses the tokenizer from here (gh: FoundationVision/LlamaGen.git) with a downsample rate of 16. // 为了进行多模态理解，它将 SigLIP-L (huggingface.co: timm/ViT-L-16-SigLIP-384) 用作视觉编码器，它支持 384 x 384 图像输入。对于图像生成，Janus-Pro 从这里 (gh: FoundationVision/LlamaGen.git) 使用令牌，下样本率为 16 。"
[Janus-Pro-1B.model/hf]: https://huggingface.co/deepseek-ai/Janus-Pro-1B "(transformers model with pipeline type any-to-any) (Sequence Length: 4096) Janus-Pro is a unified understanding and generation MLLM, which decouples visual encoding for multimodal understanding and generation. Janus-Pro is constructed based on the DeepSeek-LLM-1.5b-base/DeepSeek-LLM-7b-base. // Janus-Pro 是一种统一的理解和生成 MLLM ，它将视觉编码解耦为多模态的理解和生成。 Janus-Pro 基于 DeepSeek-LLM-1.5b-base/DeepSeek-LLM-7b-base 建造。 /// For multimodal understanding, it uses the SigLIP-L (huggingface.co: timm/ViT-L-16-SigLIP-384) as the vision encoder, which supports 384 x 384 image input. For image generation, Janus-Pro uses the tokenizer from here (gh: FoundationVision/LlamaGen.git) with a downsample rate of 16. // 为了进行多模态理解，它将 SigLIP-L (huggingface.co: timm/ViT-L-16-SigLIP-384) 用作视觉编码器，它支持 384 x 384 图像输入。对于图像生成，Janus-Pro 使用此 (gh: FoundationVision/LlamaGen.git) 令牌，下样本率为 16 。"
[Janus-1.3B.model/hf]: https://huggingface.co/deepseek-ai/Janus-1.3B "(transformers model with pipeline type any-to-any) (Sequence Length: 4096) Janus is a unified understanding and generation MLLM, which decouples visual encoding for multimodal understanding and generation. Janus is constructed based on the DeepSeek-LLM-1.3b-base which is trained on an approximate corpus of 500B text tokens. For multimodal understanding, it uses the SigLIP-L (huggingface.co: timm/ViT-L-16-SigLIP-384) as the vision encoder, which supports 384 x 384 image input. For image generation, Janus uses the tokenizer from here (gh: FoundationVision/LlamaGen.git) with a downsample rate of 16. // Janus 是一种统一的理解和生成 MLLM ，它解耦了视觉编码的多模式理解和生成。 Janus 基于 DeepSeek-LLM-1.3b-base 经大约 500B 文本的 token 训练而成。为了进行多模态理解，它将 SigLIP-L (huggingface.co: timm/ViT-L-16-SigLIP-384) 用作视觉编码器，它支持 384 x 384 图像输入。对于图像生成， Janus 使用此 (gh: FoundationVision/LlamaGen.git) 令牌，下样本速率为 16 。"
[JanusFlow-1.3B.model/hf]: https://huggingface.co/deepseek-ai/JanusFlow-1.3B "(transformers model with pipeline type any-to-any) (Sequence Length: 4096) JanusFlow is a unified understanding and generation MLLM, which decouples visual encoding for multimodal understanding and generation, which is constructed based on DeepSeek-LLM-1.3b-base. For multimodal understanding, it uses the SigLIP-L (huggingface.co: timm/ViT-L-16-SigLIP-384) as the vision encoder, which supports 384 x 384 image input. For image generation, JanusFlow uses rectified flow and SDXL-VAE (huggingface.co: stabilityai/sdxl-vae) to generate 384 x 384 images. The provided checkpoint is the EMA checkpoint after pre-training and supervised fine-tuning. // Janusflow 是一种统一的理解和生成 MLLM ，它将视觉编码解耦用为多模态理解和生成，基于 DeepSeek-LLM-1.3b-base 训练而成。为了进行多模态理解，它将 SigLIP-L (huggingface.co: timm/ViT-L-16-SigLIP-384) 用作视觉编码器，它支持 384 x 384 图像输入。对于图像生成， Janusflow 使用矫正流和  SDXL-VAE (huggingface.co: stabilityai/sdxl-vae) 生成 384 x 384 图像。检查点提供的是预培训和监督微调后的 EMA 检查点。"

[januspro.paper/ghrepo]: https://github.com/deepseek-ai/Janus/blob/main/janus_pro_tech_report.pdf "Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling // Janus-Pro: 通过数据和模型缩放的统一多模态理解和生成"
[janus.paper/arxiv]: https://arxiv.org/abs/2410.13848 "Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation // Janus: 将视觉编码解耦，以进行统一的多模态理解和生成"
[janusflow.paper/arxiv]: https://arxiv.org/abs/2411.07975 "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation // Janusflow: 统一自动化和整流流程，以进行统一的多模态理解和生成"

[Janus-Pro-7B.show/hf]: https://huggingface.co/spaces/deepseek-ai/Janus-Pro-7B
[Janus-1.3B.show/hf]: https://huggingface.co/spaces/deepseek-ai/Janus-1.3B
[JanusFlow-1.3B.show/hf]: https://huggingface.co/spaces/deepseek-ai/JanusFlow-1.3B

[ios-shortcuts/icloud]: https://www.icloud.com/shortcuts/b75899492ead45ef9a47cfce89334bf0 "深度求索"
[r1.ollama/ollama]: https://ollama.com/library/deepseek-r1 "(: ollama run deepseek-r1:latest # 0a8c26691023 • 4.7GB) DeepSeek's first-generation of reasoning models with comparable performance to OpenAI-o1, including six dense models distilled from DeepSeek-R1 based on Llama and Qwen."
[r1-llama-8b.gguf/lmstudio]: https://lmstudio.ai/model/deepseek-r1-llama-8b "(4.92 GB) (huggingface.co: lmstudio-community/DeepSeek-R1-Distill-Llama-8B-GGUF) DeepSeek R1 distilled into Llama 8B: a powerful reasoning model in a small package"
[r1-qwen-7b.gguf/lmstudio]: https://lmstudio.ai/model/deepseek-r1-qwen-7b "(4.68 GB) (huggingface.co: lmstudio-community/DeepSeek-R1-Distill-Qwen-7B-GGUF) DeepSeek R1 distilled into Qwen 7B: a powerful reasoning model in a small package"

[twitter]: https://twitter.com/deepseek_ai
