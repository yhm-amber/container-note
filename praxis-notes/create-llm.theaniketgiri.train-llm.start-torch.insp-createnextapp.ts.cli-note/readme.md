[src/gh]: https://github.com/theaniketgiri/create-llm.git "(MIT) (Languages: TypeScript 77.3%, JavaScript 18.8%, Shell 1.9%, Makefile 1.6%, Dockerfile 0.4%) The fastest way to build and start training your own LLM. CLI tool that scaffolds production-ready PyTorch training projects in seconds. Like create-next-app but for language models. // 构建并开始训练您自己的语言模型的最快方法。这款命令行工具可在几秒钟内搭建可用于生产环境的 PyTorch 训练项目。类似于 create-next-app，但专为语言模型而设计。 /// **CLI tool for scaffolding LLM Creation and training ** // 用于搭建 LLM 框架和进行培训的 CLI 工具 /// Create production-ready LLM training projects in seconds. Similar to create-next-app but for training custom language models. // 几秒钟即可创建可用于生产环境的语言模型训练项目。类似于 create-next-app，但用于训练自定义语言模型。 /// Why create-llm? // 为何选择 create-llm ？ /// Training a language model from scratch requires: // 从零开始训练语言模型需要： /// - Model architecture (GPT, BERT, T5...) // 模型架构（GPT、BERT、T5……） /// - Data preprocessing pipeline // 数据预处理流程 /// - Tokenizer training // 分词器训练 /// - Training loop with callbacks // 带有回调函数的训练循环 /// - Checkpoint management // 检查点管理 /// - Evaluation metrics // 评估指标 /// - Text generation // 文本生成 /// - Deployment tools // 部署工具 /// - create-llm provides all of this in one command. // create-llm 命令在一个命令中即可实现所有这些功能。 /// ## Features // 特征 /// ### Right-Sized Templates // 合适的模板 /// Choose from 4 templates optimized for different use cases: // 从 4 个针对不同使用场景优化的模板中进行选择： /// - NANO (1M params) - Learn in 2 minutes on any laptop // NANO （100 万参数）——在任何笔记本电脑上 2 分钟即可学会 /// - TINY (6M params) - Prototype in 15 minutes on CPU // 微型 （600 万参数）——CPU 端 15 分钟即可完成原型 /// - SMALL (100M params) - Production models in hours // 小型 （1 亿参数）- 生产模型只需数小时即可完成 /// - BASE (1B params) - Research-grade in days // 基础 （10 亿参数）- 研究级，耗时数天 /// ### Complete Toolkit // 完整工具包 /// Everything you need out of the box: // 开箱即用，所需物品一应俱全： /// - PyTorch training infrastructure // PyTorch 训练基础设施 /// - Data preprocessing pipeline // 数据预处理流程 /// - Tokenizer training (BPE, WordPiece, Unigram) // 分词器训练（BPE、WordPiece、Unigram） /// - Checkpoint management with auto-save // 检查点管理及自动保存 /// - TensorBoard integration for real-time monitoring // TensorBoard 集成用于实时监控 /// - Interactive chat interface // 交互式聊天界面 /// - Model comparison tools // 模型比较工具 /// - Deployment scripts // 部署脚本 /// ### Smart Defaults // 智能默认值 /// Intelligent configuration that: // 智能配置，具体如下： /// - Auto-detects vocab size from tokenizer // 自动从分词器检测词汇量 /// - Automatically handles sequence length mismatches // 自动处理序列长度不匹配 /// - Warns about model/data size mismatches // 警告模型/数据大小不匹配 /// - Detects overfitting during training // 检测训练过程中的过拟合情况 /// - Suggests optimal hyperparameters // 建议最优超参数 /// - Handles cross-platform paths // 处理跨平台路径 /// - Provides detailed diagnostic messages for errors // 提供详细的错误诊断信息 /// ### Plugin System // 插件系统 /// Optional integrations: // 可选集成： /// - WandB - Experiment tracking // WandB - 实验跟踪 /// - HuggingFace - Model sharing // HuggingFace - 模型分享"
[cli.npm/npmjs]: https://npmjs.com/package/create-llm "(: npm i -- create-llm) (: npx create-llm your-awesome-llm ;: cd your-awesome-llm ;: pip install -r requirements.txt ;: python training/train.py) (: npx create-llm # Interactive Setup // 交互式设置) (License: MIT) (Unpacked Size: 658 kB) (Total Files: 106)  (src: gh:theaniketgiri/create-llm.git)"

