[src/gh]: https://github.com/vllm-project/vllm.git "(Apache-2.0) (Languages: Python 86.0%, Cuda 7.6%, C++ 4.7%, Shell 0.8%, C 0.4%, CMake 0.4%, Other 0.1%) A high-throughput and memory-efficient inference and serving engine for LLMs // 一个用于 LLMs 的高吞吐量和内存高效的推理和服务器引擎 /// vLLM is a fast and easy-to-use library for LLM inference and serving. // vLLM 是一个用于 LLM 推理和服务的快速且易于使用的库。 /// Originally developed in the Sky Computing Lab at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry. // 最初由加州大学伯克利分校的天空计算实验室开发，vLLM 已经演变成一个由学术界和工业界共同贡献的社区驱动项目。 /// vLLM is fast with: // vLLM 的速度体现在： /// - State-of-the-art serving throughput // 最先进的 serving 吞吐量 /// - Efficient management of attention key and value memory with PagedAttention // 使用 PagedAttention 高效管理注意力键值内存 /// - Continuous batching of incoming requests // 持续批处理传入请求 /// - Fast model execution with CUDA/HIP graph // 使用 CUDA/HIP 图快速执行模型 /// - Quantizations: GPTQ, AWQ, AutoRound, INT4, INT8, and FP8 // 量化：GPTQ、AWQ、AutoRound、INT4、INT8 和 FP8 /// - Optimized CUDA kernels, including integration with FlashAttention and FlashInfer // 优化 CUDA 内核，包括与 FlashAttention 和 FlashInfer 的集成 /// - Speculative decoding // 推测解码 /// - Chunked prefill // 分块预填充 /// vLLM is flexible and easy to use with: // vLLM 具有灵活性和易用性，支持： /// - Seamless integration with popular Hugging Face models // 与流行的 Hugging Face 模型无缝集成 /// - High-throughput serving with various decoding algorithms, including parallel sampling, beam search, and more // 使用多种解码算法进行高吞吐量服务，包括并行采样、集束搜索等 /// - Tensor, pipeline, data and expert parallelism support for distributed inference // 张量、流水线、数据以及专家并行支持分布式推理 /// - Streaming outputs // 流式输出 /// - OpenAI-compatible API server // 兼容 OpenAI 的 API 服务器 /// - Support for NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, Arm CPUs, and TPU. Additionally, support for diverse hardware plugins such as Intel Gaudi, IBM Spyre and Huawei Ascend. // 支持 NVIDIA GPU、AMD CPU 和 GPU、Intel CPU 和 GPU、PowerPC CPU、Arm CPU 以及 TPU。此外，还支持多种硬件插件，如 Intel Gaudi、IBM Spyre 和华为 Ascend。 /// - Prefix caching support // 前缀缓存支持 /// - Multi-LoRA support // 多 LoRA 支持 /// vLLM seamlessly supports most popular open-source models on HuggingFace, including: // vLLM 无缝支持 HuggingFace 上大多数流行的开源模型，包括： /// - Transformer-like LLMs (e.g., Llama) // Transformer 类 LLMs（例如，Llama） /// - Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3) // 专家混合 LLMs（例如，Mixtral、Deepseek-V2 和 V3） /// - Embedding Models (e.g., E5-Mistral) // 嵌入模型（例如，E5-Mistral） /// - Multi-modal LLMs (e.g., LLaVA) // 多模态 LLMs（例如，LLaVA）"
[paper/arxiv]: https://arxiv.org/abs/2309.06180 "[Submitted on 12 Sep 2023] { Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC) } (doi: 10.48550/arXiv.2309.06180) Efficient Memory Management for Large Language Model Serving with PagedAttention /// 基于 PagedAttention 的大语言模型服务高效内存管理 /// High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4  with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at this https URL // 对大型语言模型（LLMs）的高吞吐量服务需要同时批处理足够多的请求。然而，现有系统面临挑战，因为每个请求的键值缓存（KV 缓存）内存巨大且动态变化。如果管理不当，这种内存可能因碎片化和冗余重复而被显著浪费，从而限制批处理大小。为解决这个问题，我们提出了 PagedAttention，这是一种受操作系统中的经典虚拟内存和分页技术启发的注意力算法。在其基础上，我们构建了 vLLM，一个 LLM 服务系统，该系统实现了（1）KV 缓存内存的近零浪费和（2）在请求内部和跨请求之间灵活共享 KV 缓存，以进一步减少内存使用。我们的评估表明，与 FasterTransformer 和 Orca 等最先进系统相比，vLLM 在相同延迟水平下将流行 LLMs 的吞吐量提高了 2-4   。随着序列更长、模型更大以及解码算法更复杂，这种改进效果更为显著。vLLM 的源代码可在以下 https URL 公开获取。"
[cli.pip/pypi]: https://pypi.org/project/vllm/ "(: uv pip install --torch-backend=auto -- vllm) (License Expression: Apache-2.0) (Requires: Python >=3.8) A high-throughput and memory-efficient inference and serving engine for LLMs // 一个用于 LLMs 的高吞吐量和内存高效的推理和服务器引擎 (src: gh:vllm-project/vllm.git)"
[docs/.site]: https://docs.vllm.ai/ "(: uv pip install --torch-backend=auto -- vllm # auto, such as: nv-cuda ;: uv pip install -- vllm-tpu # run vLLM on Google TPUs ;: docker run -it --rm --network=host --group-add=render --ipc=host --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --device /dev/kfd --device /dev/dri -v <path/to/your/models>:/app/models -e HF_HOME=/app/models -- docker.io/rocm/vllm-dev:main # AMD ROCm) (: vllm serve -- MiniMaxAI/MiniMax-M2 # serv on: http://localhost:8000, OpenAI API protocol.) (: from vllm import LLM, SamplingParams ;: prompts = ['Hello', 'Where is PRC', 'The future of AI is'] ;: sampling_params = SamplingParams(temperature=0.8, top_p=0.95) # temperature: if more hot (such as 1.0), more creative; if more cold (such as 0.0 for greedy decoding means choose most-probab token with no any random sampling), more deterministic. # top_p=0.95: nucleus sampling, Limited sampling set: Tokens are accumulated one by one from bigger to smaller and stop while acc > top_p, so bigger top_p bigger sampling set bigger creative. ;: llm = LLM(model='MiniMaxAI/MiniMax-M2') ;: outputs = llm.generate(prompts, sampling_params) # lib api demo)"
[site/ai]: https://vllm.ai/ "The High-Throughput and Memory-Efficient inference and serving engine for LLMs // 专为 LLMs 设计的高吞吐量和内存高效的推理与服务引擎 /// Easy, fast, and cost-efficient LLM serving for everyone. // 为所有人提供简单、快速且成本效益高的 LLM 服务 /// - Easy // 简单 ///: Deploy the widest range of open-source models on any hardware. Includes a drop-in OpenAI-compatible API for instant integration. // 在任何硬件上部署最广泛的开源模型。包含即插即用的 OpenAI 兼容 API，实现即时集成。 /// - Fast // 快速 ///: Maximize throughput with PagedAttention. Advanced scheduling and continuous batching ensure peak GPU utilization. // 通过 PagedAttention 最大化吞吐量。高级调度和持续批处理确保 GPU 的峰值利用率。 /// - Cost Efficient // 成本高效 ///: Slash inference costs by maximizing hardware efficiency. We make high-performance LLMs affordable and accessible to everyone. // 通过最大化硬件效率来降低推理成本。我们让高性能 LLMs 变得对每个人都负担得起且易于获取。"
[cli.oci/dockerhub]: https://hub.docker.com/r/vllm/vllm-openai '(: podman run --device nvidia.com/gpu=all -v ~/.cache/huggingface:/root/.cache/huggingface --env "HF_TOKEN=$HF_TOKEN" -p 8000:8000 --ipc=host -- docker.io/vllm/vllm-openai:latest --model MiniMaxAI/MiniMax-M2)'


