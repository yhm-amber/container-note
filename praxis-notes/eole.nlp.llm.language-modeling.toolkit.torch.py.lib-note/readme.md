[src/gh]: https://github.com/eole-nlp/eole.git "(MIT) (Languages: Python 94.8%, Shell 5.2%) Open language modeling toolkit based on PyTorch initially spun-off of OpenNMT-py // åŸºäº PyTorch çš„å¼€æ”¾è¯­è¨€å»ºæ¨¡å·¥å…·åŒ…æœ€åˆæ˜¯ä» OpenNMT-py åˆ†æ”¯å‡ºæ¥çš„ /// We aim to maintain the research-friendly approach of the original project while including latest architectures (LLMs) and various other techniques. Our goal is to provide a comprehensive yet compact and modular codebase for experimenting with various types of language models (encoder, decoder, seq2seq). // æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¿æŒåŸå§‹é¡¹ç›®çš„ç§‘ç ”å‹å¥½æ€§ï¼ŒåŒæ—¶åŒ…å«æœ€æ–°çš„æ¶æ„ï¼ˆLLMsï¼‰å’Œå„ç§å…¶ä»–æŠ€æœ¯ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æä¾›ä¸€ä¸ªå…¨é¢ã€ç´§å‡‘ä¸”æ¨¡å—åŒ–çš„ä»£ç åº“ï¼Œç”¨äºå®éªŒå„ç§ç±»å‹çš„è¯­è¨€æ¨¡å‹ï¼ˆç¼–ç å™¨ã€è§£ç å™¨ã€seq2seqï¼‰ã€‚ /// HF Models supported // æ”¯æŒ HF æ¨¡å‹ /// - tencent/HunyuanOCR End-to-End OCR model by Tencent. Uses more image token vs Deepseek but smaller LM. Results are impressive. (see recipe) // è…¾è®¯/HunyuanOCR ç»ˆç«¯ OCR æ¨¡å‹ç”±è…¾è®¯å¼€å‘ã€‚ç›¸æ¯” Deepseekï¼Œä½¿ç”¨æ›´å¤šå›¾åƒæ ‡è®°ä½†è¯­è¨€æ¨¡å‹æ›´å°ã€‚ç»“æœä»¤äººå°è±¡æ·±åˆ»ã€‚ï¼ˆå‚è§é£Ÿè°±ï¼‰ /// - deepseek-ai/DeepSeek-OCR For now takes any image and rescales to 1024x1024 before processing - Gundam mode not implemented yet) // deepseek-ai/DeepSeek-OCR ç›®å‰æ¥å—ä»»ä½•å›¾åƒï¼Œå¹¶åœ¨å¤„ç†å‰å°†å…¶ç¼©æ”¾åˆ° 1024x1024 - å°šæœªå®ç°é«˜è¾¾å§†æ¨¡å¼) /// - tencent/Hunyuan-MT-7B SOTA NMT at WMT25, better than Towerplus-9B and EuroLLM-9B // è…¾è®¯/Hunyuan-MT-7B åœ¨ WMT25 ä¸Šè¾¾åˆ° SOTA NMT æ°´å¹³ï¼Œä¼˜äº Towerplus-9B å’Œ EuroLLM-9B /// - Qwen/Qwen2/3 Non VL family. Includes Qwen3-30B-A3B // Qwen/Qwen2/3 éè§†è§‰è¯­è¨€æ¨¡å‹å®¶æ—ã€‚åŒ…æ‹¬ Qwen3-30B-A3B /// - google/gemma-3-27b-it All Gemma3 family - supports text and image input // google/gemma-3-27b-it æ‰€æœ‰ Gemma3 ç³»åˆ— - æ”¯æŒæ–‡æœ¬å’Œå›¾åƒè¾“å…¥ /// - Mistral-3.1-24B-instruct supports all Mistral AI models (text and image input) - includes Ministral, Mixtral, Mathstral // Mistral-3.1-24B-instruct æ”¯æŒæ‰€æœ‰ Mistral AI æ¨¡å‹ï¼ˆæ–‡æœ¬å’Œå›¾åƒè¾“å…¥ï¼‰- åŒ…æ‹¬ Minstralã€Mixtralã€Mathstral /// - meta-llama/Llama-3.X models // meta-llama/Llama-3.X æ¨¡å‹ /// - microsoft/Phi-2/3 models // microsoft/Phi-2/3 æ¨¡å‹ /// Of course you can train your own architecture (Decoder only, Encoder Only, or EncoderDecoder Model) // å½“ç„¶ä½ å¯ä»¥è®­ç»ƒè‡ªå·±çš„æ¶æ„ï¼ˆä»…è§£ç å™¨ã€ä»…ç¼–ç å™¨æˆ–ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼‰ /// Latest developments // æœ€æ–°è¿›å±• /// - high inference speed using Flash Attention and Vllm RMSNorm kernel. // ä½¿ç”¨ Flash Attention å’Œ Vllm RMSNorm å†…æ ¸å®ç°çš„é«˜æ¨ç†é€Ÿåº¦ã€‚ /// - prefixLM + split prompt/answer in src/tgt optional method to feed your data // åœ¨ src/tgt ä¸­å¯é€‰çš„å‰ç¼€ LM + åˆ†éš”æç¤º/ç­”æ¡ˆçš„æ–¹æ³•æ¥è¾“å…¥æ‚¨çš„æ•°æ®ã€‚ /// - Pure-BF16 Training thanks to Kahan Summation implemented here // å¾—ç›Šäºåœ¨æ­¤å¤„å®ç°çš„ Kahan æ±‚å’Œï¼Œå¯ä»¥è¿›è¡Œçº¯ BF16 è®­ç»ƒã€‚ /// - Web-based (Google translator-like) interface featuring the latest Hunyuan-MT-7B or EuroLLM-8B-Instruct LLM // åŸºäº Web çš„ï¼ˆç±»ä¼¼ Google ç¿»è¯‘å™¨ï¼‰ç•Œé¢ï¼Œæ”¯æŒæœ€æ–°çš„ Hunyuan-MT-7B æˆ– EuroLLM-8B-Instruct LLMã€‚ /// - Estimator layer which enables to rescore multiple beams in the same model. Read article here and here // é‡è¯„åˆ†å±‚ï¼Œå¯åœ¨åŒä¸€æ¨¡å‹ä¸­é‡è¯„åˆ†å¤šä¸ªæŸã€‚é˜…è¯»æ–‡ç« æ­¤å¤„å’Œæ­¤å¤„ /// - Support Hugging Face Tokenizers for better compatiblity // æ”¯æŒ Hugging Face Tokenizers ä»¥æé«˜å…¼å®¹æ€§ /// - Replicate CometKiwi(XL/XXL) Encoder+Estimator models // å¤åˆ¶ CometKiwi(XL/XXL) Encoder+Estimator æ¨¡å‹ /// Key Features // ä¸»è¦åŠŸèƒ½ /// - Versatile Training and Inference: Train from scratch, finetune, and infer models of various architectures including Transformer Encoder/Decoder/EncoderDecoder and RNN EncoderDecoder. // çµæ´»çš„è®­ç»ƒå’Œæ¨ç†ï¼šæ”¯æŒä»å¤´å¼€å§‹è®­ç»ƒã€å¾®è°ƒå’Œæ¨ç†å„ç§æ¶æ„çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬ Transformer Encoder/Decoder/EncoderDecoder å’Œ RNN EncoderDecoderã€‚ /// - Dynamic Data Transforms: Apply on-the-fly transformations in the dataloading logic for both training and inference. // åŠ¨æ€æ•°æ®è½¬æ¢ï¼šåœ¨è®­ç»ƒå’Œæ¨ç†çš„æ•°æ®åŠ è½½é€»è¾‘ä¸­åº”ç”¨å³æ—¶è½¬æ¢ã€‚ /// - Comprehensive LLM Support: Includes converters for Llama, Mistral, Phi, Gemma ... // å…¨é¢çš„ LLM æ”¯æŒï¼šåŒ…å« Llamaã€Mistralã€Phiã€Gemma ç­‰çš„è½¬æ¢å™¨... /// - Advanced Quantization: Support for 8-bit and 4-bit quantization, along with LoRA adapters, with or without checkpointing, as well as mixed precision (FP16). // é«˜çº§é‡åŒ–ï¼šæ”¯æŒ 8 ä½å’Œ 4 ä½é‡åŒ–ï¼Œä»¥åŠ LoRA é€‚é…å™¨ï¼Œå¯å¸¦æˆ–ä¸å¸¦æ£€æŸ¥ç‚¹ï¼Œä»¥åŠæ··åˆç²¾åº¦ï¼ˆFP16ï¼‰ã€‚ /// - Efficient Finetuning: Finetune 7B and 13B models on a single RTX 24GB GPU using 4-bit quantization. // é«˜æ•ˆå¾®è°ƒï¼šä½¿ç”¨ 4 ä½é‡åŒ–åœ¨å•ä¸ª RTX 24GB GPU ä¸Šå¾®è°ƒ 7B å’Œ 13B æ¨¡å‹ã€‚ /// - Flexible Inference: Perform inference in 4-bit or 8-bit using the same layer quantization methods as in finetuning. // çµæ´»æ¨ç†ï¼šä½¿ç”¨ä¸å¾®è°ƒç›¸åŒçš„å±‚é‡åŒ–æ–¹æ³•ï¼Œä»¥ 4 ä½æˆ– 8 ä½è¿›è¡Œæ¨ç†ã€‚ /// - Tensor Parallelism: Enable tensor parallelism for both training and inference when models exceed the memory capacity of a single GPU. // å¼ é‡å¹¶è¡Œï¼šå½“æ¨¡å‹è¶…è¿‡å•ä¸ª GPU çš„å†…å­˜å®¹é‡æ—¶ï¼Œå¯ç”¨è®­ç»ƒå’Œæ¨ç†çš„å¼ é‡å¹¶è¡Œã€‚"
[site/ghio]: https://eole-nlp.github.io/eole/ "Open language modeling toolkit based on PyTorch /// - âš™ï¸ Open Models // âš™ï¸ å¼€æ”¾æ¨¡å‹ ///: This project, although not as broad in scope as some, aims at maximizing code reusability while supporting diverse architectures and pretrained models. The challenge here is to factorize while not over-complexifyng things too much. // å°½ç®¡è¿™ä¸ªé¡¹ç›®åœ¨èŒƒå›´ä¸Šä¸å¦‚ä¸€äº›é¡¹ç›®å¹¿æ³›ï¼Œä½†å®ƒæ—¨åœ¨æœ€å¤§åŒ–ä»£ç çš„å¯é‡ç”¨æ€§ï¼ŒåŒæ—¶æ”¯æŒå¤šç§æ¶æ„å’Œé¢„è®­ç»ƒæ¨¡å‹ã€‚è¿™é‡Œçš„æŒ‘æˆ˜æ˜¯åœ¨ä¸ä½¿äº‹æƒ…è¿‡äºå¤æ‚çš„æƒ…å†µä¸‹è¿›è¡Œæ¨¡å—åŒ–ã€‚ /// - ğŸ§± Simplicity and Modularity // ğŸ§± ç®€æ´æ€§å’Œæ¨¡å—åŒ– ///: A single entry-point to call runnables and tools. Pre-defined converters, extendable modules and architectures. // ä¸€ä¸ªå•ä¸€çš„å…¥å£ç‚¹æ¥è°ƒç”¨å¯è¿è¡Œç¨‹åºå’Œå·¥å…·ã€‚é¢„å®šä¹‰çš„è½¬æ¢å™¨ã€å¯æ‰©å±•çš„æ¨¡å—å’Œæ¶æ„ã€‚ /// - ğŸ’¨ Speed and Efficiency // ğŸ’¨ é€Ÿåº¦ä¸æ•ˆç‡ ///: Developed on reasonable hardware. Aimed at making models accessible in restricted resources or frugal environments. // åŸºäºåˆç†ç¡¬ä»¶å¼€å‘ã€‚æ—¨åœ¨ä½¿æ¨¡å‹åœ¨èµ„æºå—é™æˆ–èŠ‚ä¿­çš„ç¯å¢ƒä¸­ä¹Ÿèƒ½ä½¿ç”¨ã€‚"
[knowsby]: https://github.com/OpenNMT/OpenNMT-py.git "å…¬å‘Šï¼šOpenNMT-py å·²ä¸å†ç§¯æç»´æŠ¤ã€‚ /// We started a new project Eole available on Github // æˆ‘ä»¬å¯åŠ¨äº†ä¸€ä¸ªæ–°çš„é¡¹ç›® Eoleï¼Œå¯åœ¨ Github ä¸Šè·å– /// It is a spin-off of OpenNMT-py in terms of features but we revamped a lot of stuff. // å®ƒåœ¨åŠŸèƒ½ä¸Šæ˜¯ä» OpenNMT-py åˆ†æ”¯å‡ºæ¥çš„ï¼Œä½†æˆ‘ä»¬å¯¹å…¶è¿›è¡Œäº†å¤§é‡æ”¹è¿›ã€‚"

