[src/gh]: https://github.com/simplescaling/s1.git "(Languages: Python 96.1%, Jupyter Notebook 2.4%, Shell 1.3%, Other 0.2%) s1: Simple test-time scaling"
[arxiv]: https://arxiv.org/abs/2501.19393 "Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending \"Wait\" multiple times to the model's generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1-32B exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1-32B with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at this https URL // 测试时缩放是一种有前景的新语言建模方法，它通过额外的测试时计算来提高性能。最近，OpenAI 的 o1 模型展示了这种能力，但没有公开其方法，导致许多复制工作。我们寻求实现测试时缩放和强大推理性能的最简单方法。首先，我们根据三个通过消融验证的标准——难度、多样性和质量——精心制作了一个包含 1,000 个问题及其推理轨迹的小数据集 s1K。其次，我们开发了预算强制方法，通过强制终止模型的思考过程或通过多次添加“等待”到模型生成时尝试结束来延长它，以控制测试时的计算。这可以使模型再次检查其答案，通常可以纠正错误的推理步骤。在 s1K 上对 Qwen2.5-32B-Instruct 语言模型进行监督微调并配备预算强制后，我们的 s1-32B 模型在竞赛数学问题上的表现超过了 o1-preview 高达 27%（MATH 和 AIME24）。此外，使用预算强制对 s1-32B 进行缩放允许在不进行测试时干预的情况下外推其性能：从 AIME24 的 50%提高到 57%。 我们的模型、数据和代码在此 https URL 处开源"
