[fullstack-impl.src/gh]: https://github.com/karpathy/nanochat.git "(Languages: Python 88.6%, Rust 5.3%, HTML 4.2%, Shell 1.9%) The best ChatGPT that $100 can buy. // 100 美元能买到的最佳 ChatGPT。 /// This repo is a full-stack implementation of an LLM like ChatGPT in a single, clean, minimal, hackable, dependency-lite codebase. nanochat is designed to run on a single 8XH100 node via scripts like speedrun.sh, that run the entire pipeline start to end. This includes tokenization, pretraining, finetuning, evaluation, inference, and web serving over a simple UI so that you can talk to your own LLM just like ChatGPT. nanochat will become the capstone project of the course LLM101n being developed by Eureka Labs. // 这个仓库是一个单栈实现，类似于 ChatGPT 的 LLM，代码库干净、极简、可定制、依赖性低。nanochat 设计为通过 speedrun.sh 等脚本在单个 8XH100 节点上运行整个流程，从开始到结束。这包括分词、预训练、微调、评估、推理，以及通过简单的 UI 进行 Web 服务，以便你可以像 ChatGPT 一样与自己的 LLM 交谈。nanochat 将成为 Eureka Labs 开发的课程 LLM101n 的收官项目。 /// Unsurprisingly, $100 is not enough to train a highly performant ChatGPT clone. In fact, LLMs are famous for their multi-million dollar capex. For our purposes, I think there are two more scales of interest. First is the ~$300 tier d26 model (i.e. depth=26) that trains in ~12 hours, which slightly outperforms GPT-2 CORE score. Second is the $1000 tier (~41.6 hours), just because it's a nice round number. But both of these are not yet fully supported and therefore not attached here in the master branch yet. // 毫不奇怪，100 美元不足以训练一个高性能的 ChatGPT 克隆。事实上，LLMs 以其数百万美元的资本支出而闻名。就我们的目的而言，我认为有两个更值得关注的规模。首先是~300 美元的 d26 模型（即深度=26），训练时间约为 12 小时，略微优于 GPT-2 CORE 分数。其次是 1000 美元的级别（约 41.6 小时），仅仅因为它是一个很整齐的数字。但这两个都不完全支持，因此尚未在主分支中添加。 /// nanochat is nowhere finished. The goal is to improve the state of the art in micro models that are accessible to work with end to end on budgets of < $1000 dollars. Accessibility is about overall cost but also about cognitive complexity - nanochat is not an exhaustively configurable LLM \"framework\"; there will be no giant configuration objects, model factories, or if-then-else monsters in the code base. It is a single, cohesive, minimal, readable, hackable, maximally-forkable \"strong baseline\" codebase designed to run start to end and produce a concrete ChatGPT clone and its report card. // nanochat 还没有完全完成。目标是改进预算低于 1000 美元的可端到端操作的微型模型的状态。可访问性不仅关乎总体成本，还关乎认知复杂度 - nanochat 不是一个可彻底配置的 LLM \"框架\"；代码库中不会有巨大的配置对象、模型工厂或 if-then-else 怪物。它是一个单一、连贯、极简、可读、可修改、可最大程度分叉的 \"强基线\" 代码库，旨在端到端运行，并生成一个具体的 ChatGPT 克隆及其成绩单。"
[chat/nano-gpt.com]: https://nano-gpt.com/conversation "- Leave no trace // 不留痕迹 ///: Chats are saved on your device. We strictly inform providers not to train models on your data // 聊天记录会保存在您的设备上。我们严格告知服务提供商不要使用您的数据进行模型训练 /// - Every top AI model available // 目前所有顶尖 AI 模型 ///: We add models as quickly as they are available. Order is based on independent leaderboards // 我们尽快添加新模型。排序基于独立排行榜 /// - Let auto model help you // 让自动模型助你一臂之力 ///: Use the Auto Model which automatically selects the best suited model for your query if you are unsure what model to use // 如果你不确定使用哪个模型，可以使用自动模型，它会自动选择最适合你查询的模型 /// You are using our free model. This model is limited, but you're welcome to use it as much as you want! To use all the top models, please add to your balance. // 你正在使用我们的免费模型。这个模型有限制，但你可以随意使用！要使用所有顶级模型，请充值。"
[nanogpt.src/gh]: https://github.com/karpathy/nanoGPT.git "(MIT) (Languages: Python 100.0%) The simplest, fastest repository for training/finetuning medium-sized GPTs. // 用于训练/微调中大型 GPT 的最简单、最快的仓库。 /// The simplest, fastest repository for training/finetuning medium-sized GPTs. It is a rewrite of minGPT that prioritizes teeth over education. Still under active development, but currently the file train.py reproduces GPT-2 (124M) on OpenWebText, running on a single 8XA100 40GB node in about 4 days of training. The code itself is plain and readable: train.py is a ~300-line boilerplate training loop and model.py a ~300-line GPT model definition, which can optionally load the GPT-2 weights from OpenAI. That's it. // 用于训练/微调中等规模 GPT 的最简单、最快的仓库。它是 minGPT 的重写版本，优先考虑性能而非教育。目前仍在积极开发中，但当前的文件 train.py 在 OpenWebText 上复现了 GPT-2（124M），在单个 8XA100 40GB 节点上训练约需 4 天。代码本身简洁易读： train.py 是一个约 300 行的训练循环模板， model.py 是一个约 300 行的 GPT 模型定义，可以可选地从 OpenAI 加载 GPT-2 权重。就这样。"
