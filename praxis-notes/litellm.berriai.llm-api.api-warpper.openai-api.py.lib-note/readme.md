[src/gh]: https://github.com/BerriAI/litellm.git "(MIT) (Languages: Python 86.9%, TypeScript 12.1%, HTML 0.9%, JavaScript 0.1%, Shell 0.0%, Makefile 0.0%) Python SDK, Proxy Server (AI Gateway) to call 100+ LLM APIs in OpenAI (or native) format, with cost tracking, guardrails, loadbalancing and logging. [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, VLLM, NVIDIA NIM] // Python SDKï¼Œä»£ç†æœåŠ¡å™¨ï¼ˆAI ç½‘å…³ï¼‰ï¼Œç”¨äºè°ƒç”¨ 100 å¤šé¡¹ LLM APIï¼Œæ”¯æŒ OpenAIï¼ˆæˆ–åŸç”Ÿï¼‰æ ¼å¼ï¼Œå…·æœ‰æˆæœ¬è·Ÿè¸ªã€å®‰å…¨é™åˆ¶ã€è´Ÿè½½å‡è¡¡å’Œæ—¥å¿—è®°å½•åŠŸèƒ½ã€‚[Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, VLLM, NVIDIA NIM] /// ğŸš… LiteLLM /// Call all LLM APIs using the OpenAI format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.] // ä½¿ç”¨ OpenAI æ ¼å¼è°ƒç”¨æ‰€æœ‰ LLM API [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq ç­‰] /// LiteLLM manages: // LiteLLM ç®¡ç†ï¼š /// - Translate inputs to provider's completion, embedding, and image_generation endpoints // å°†è¾“å…¥è½¬æ¢ä¸ºæä¾›å•†çš„ completion ã€ embedding å’Œ image_generation ç«¯ç‚¹ /// - Consistent output, text responses will always be available at ['choices'][0]['message']['content'] // ä¸€è‡´æ€§çš„è¾“å‡ºï¼Œæ–‡æœ¬å“åº”å°†å§‹ç»ˆåœ¨ ['choices'][0]['message']['content'] å¯ç”¨ /// - Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - Router // è·¨å¤šä¸ªéƒ¨ç½²çš„é‡è¯•/å›é€€é€»è¾‘ï¼ˆä¾‹å¦‚ Azure/OpenAIï¼‰- è·¯ç”±å™¨ /// - Set Budgets & Rate limits per project, api key, model LiteLLM Proxy Server (LLM Gateway) // ä¸ºæ¯ä¸ªé¡¹ç›®ã€API å¯†é’¥å’Œæ¨¡å‹è®¾ç½®é¢„ç®—å’Œé€Ÿç‡é™åˆ¶ï¼ŒLiteLLM ä»£ç†æœåŠ¡å™¨ï¼ˆLLM ç½‘å…³ï¼‰"
[lib.pip/pypi]: https://pypi.org/project/litellm/ "(: pip install -- litellm) (License: MIT License (MIT)) (Requires: Python <4.0, >=3.9) (Provides-Extra: [caching], [extra-proxy], [mlflow], [proxy], [semantic-router], [utils]) Library to easily interface with LLM API providers // ç”¨äºè½»æ¾ä¸ LLM API æä¾›è€…è¿›è¡Œæ¥å£çš„åº“"
[site/ai]: https://litellm.ai/ "AI Gateway to provide model access, fallbacks and spend tracking across 100+ LLMs. All in the OpenAI format. // AI ç½‘å…³æä¾›è·¨ 100 å¤š LLMs çš„æ¨¡å‹è®¿é—®ã€å›é€€æ–¹æ¡ˆå’Œæ”¯å‡ºè¿½è¸ªã€‚å…¨éƒ¨é‡‡ç”¨ OpenAI æ ¼å¼ã€‚ /// LiteLLM makes it easy for Platform teams to give developers LLM access // LiteLLM ä½¿å¹³å°å›¢é˜Ÿèƒ½å¤Ÿè½»æ¾åœ°ä¸ºå¼€å‘è€…æä¾› LLM è®¿é—®"
[docs/.site]: https://docs.litellm.ai/ "How to use LiteLLM // å¦‚ä½•ä½¿ç”¨ LiteLLM /// You can use litellm through either: // æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä½¿ç”¨ litellmï¼š /// - LiteLLM Proxy Server - Server (LLM Gateway) to call 100+ LLMs, load balance, cost tracking across projects // LiteLLM ä»£ç†æœåŠ¡å™¨ - æœåŠ¡å™¨ï¼ˆLLM ç½‘å…³ï¼‰ï¼Œç”¨äºè°ƒç”¨ 100 å¤šä¸ª LLMï¼Œè´Ÿè½½å‡è¡¡ï¼Œè·¨é¡¹ç›®æˆæœ¬è·Ÿè¸ª /// - LiteLLM python SDK - Python Client to call 100+ LLMs, load balance, cost tracking // LiteLLM python SDK - Python å®¢æˆ·ç«¯ï¼Œç”¨äºè°ƒç”¨ 100 å¤šä¸ª LLMï¼Œè´Ÿè½½å‡è¡¡ï¼Œæˆæœ¬è·Ÿè¸ª"

