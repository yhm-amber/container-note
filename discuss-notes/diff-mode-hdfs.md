# on OS, or not (only) on OS?

## 存储方案

前提：最终平台应当以使用围绕 HDFS 的技术为主。


存储解决方案：


| 序号  | 主要技术             | 类别     | 读写性能            | 是否支持被自动创建 | 是否存在副本、弹性、快照等高级特性 | 该技术涉及的 License                  | 业界地位                                                               |
|-----|------------------|--------|-----------------|-----------|-------------------|---------------------------------|--------------------------------------------------------------------|
| `0` | 手动创建 PV          | 本地卷    | 最好              | ❌         | ❌                 | Apache-2.0 License              | 这是 Kubernetes 自身的功能，不支持自动按需创建，需要预先手动创建。                            |
| `1` | OpenEBS Local    | 本地卷    | 几乎与 `0` 相同      | ✔         | ❌                 | Apache-2.0 License              | 非常知名，KubeSphare（8k star）的集群安装工具默认会为产品的新手安装它作为默认存储类。                |
| `2` | OpenEBS cStor    | 数据存储引擎 | 大幅低于 `0` 和 `1`  | ✔         | ✔                 | Apache-2.0 License              | --                                                                 |
| `3` | Ceph in Rook     | 数据存储引擎 | 略好于 `2`         | ✔         | ✔                 | Apache-2.0 License、GPL、LGPL、BSD | Ceph在社区中是长期很受欢迎的存储引擎，有多家企业推进对它性能的优化以打击同类付费产品。                      |
| `4` | Rancher Longhorn | 数据存储引擎 | 略好于 `3`         | ✔         | ✔                 | Apache-2.0 License              | Rancher Lab 出品的存储引擎，现在属于 SUSE 旗下项目，目前是其开源 IaaS 产品 Harvester 的组件之一。 |
| `5` | NFS              | 网络文件系统 | 完全依赖网速与其底层实现的瓶颈 | ✔         | ⭕（非本地存储不考虑这些）     | --                              | 这是个接口，背后可能是各种实现。                                                   |

分布式数据存储引擎还有： cubeFS、seeweedfs。但二者与 hdfs 有一定功能重叠（而且某些方面的能力还好过 HDFS ），故不加入。

另外，还有存储方案是 hostPath Volume ，它需要事先创建目录（并且需要分析应给目录的合理的）

数据存储组件的调度方案：


| 序号  | 调度方案设计                  | 是否允许存在漂移 | 实现所需要做的工作                                                                                                                           | 卷/存储方案           | 使用场景                                                                                                                                                                  | 存储组件的性能                                                        | 迁移、副本等高级功能                                                | 是否建议 |
|-----|-------------------------|----------|-------------------------------------------------------------------------------------------------------------------------------------|------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------|-----------------------------------------------------------|------|
| `0` | 禁止角色的调度                 | ❌        | 简单：设置 K8s 禁止跨节点调度 Pod ，并为每个 Pod 配置固定的本地卷。                                                                                           | 手动创建 PV          | 测试，或者确定节点更换 TDH 6 。                                                                                                                                                   | 基本就是 HDFS 自身的性能                                                | 完全依靠 HDFS 完成                                              | ❌    |
| `1` | 允许调度，但只允许手动指定调度位置       | ❌        | 较复杂：基于用户界面要支持有限制地修改 Pod / Deploy / Sts 的 Yaml ， 或实现一个封装好的 HDFS 自定义资源（CRD），或者实现 HDFS 的自定义存储类。旧的数据卷可以一并迁移，也可以不迁移，用HDFS自身的副本功能来变相实现迁移。 | OpenEBS Local    | 在多数组件都必须使用 HDFS 存储时，推荐此方案。少部分组件（如身为元数据库的 MySQL）可以另外直接基于，具有副本功能的存储类作为自身存储支持。                                                                                           | 只要不主动触发调度，基本就是 HDFS 自身的性能                                      | 完全依靠 HDFS 完成；但，调度到另外的节点就等同于迁移，因此可以简化迁移功能的实现。              | ✔    |
| `3` | 允许自动调度，存储卷基于本地的分布式存储引擎  | ✔        | 复杂：应当记录这个数据副本是哪个 Datanode 数据的副本，并且需要有办法动态地根据此来限制 Pod 的节点亲和范围。                                                                       | Rancher Longhorn | 在这个方案里，实际并不是 HDFS 而是另外的引擎成为统一的分布式存储的提供者。HDFS 变成了上层应用之一。这会让 HDFS 的优势不能得到发挥。                                                                                            | 在 HDFS 固有的性能瓶颈前提下，新增了由这个分布式存储引擎（如 Longhorn ）带来的性能瓶颈，整体性能会大幅下降。 | 完全依靠这个分布式存储引擎（如 Longhorn ）完成，HDFS作为存储引擎的功能不会得到充分发挥。       | ❌    |
| `4` | 允许自动调度，存储卷基于网络文件系统（NFS） | ✔        | 简单：配置 NFS 的存储类并以配置的方式使用。                                                                                                            | nfs              | 这个方案里，HDFS的大部分优势特性都不能得到发挥，不论是数据的可靠还是它身为一个分布式文件系统特有的读写性能。如果不打算发挥 HDFS 的这些能力，并且具有可以这样使用的高性能网络存储，则可以像这样使用。该方案中，HDFS组件将失去除保持它原有接口外的任何价值，如果Spark等支持直接对接 NFS ，为何还需要 HDFS 呢？ | 性能瓶颈是 NFS 的性能瓶颈加上 HDFS 的性能瓶颈。                                  | 和 `3` 一样，实际上HDFS的这方面功能成了摆设。数据的实际的副本与弹性存储的能力取决于 NFS 背后的实现。 | ❌    |

另外一些组合：


| 存储方案                                                  | 是否允许服务跨节点恢复 | 序号  | 问题                                                                                                                               | 总结                                                   |
|-------------------------------------------------------|-------------|-----|----------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------|
| 手动创建 PV                                               | ✔           | `0` | 你不知道允许跨节点的服务调度会把服务搞去哪台节点，因此你不可能做到所谓提前准备。如果没有准备，在别的节点上恢复的服务将会不能完成恢复，一直等待出现可以被它使用的 PV ，这还增加了错误地绑定 PV 的可能。                          | 没有使二者可以组合使用的场景。                                      |
| OpenEBS Local                                         | ✔           | `1` | 如果允许自动恢复服务时可跨节点，当服务恢复到新的节点时，相当于出现了前面说过的【迁移】。这会不必要地影响到HDFS的正常性能。简单说来，正常的HDFS使用中难道会频繁地作节点迁移吗？不会的，因为HDFS并不是被设计来做这种事情的。              | 因此上面设计成了只是让用户需要迁移时，利用 K8s 的固有功能与自动分配的本机卷，来便捷地实现角色迁移。 |
| OpenEBS cStor / Ceph in Rook / Rancher Longhorn / nfs | ✔           | `2` | 这不会有什么问题，cStor/Ceph/Longhorn本身是一个分布式的存储引擎，nfs则是不受本集群故障影响的网络接口。但这样会使得总体性能瓶颈变为cStor/Ceph/Longhorn/nfs瓶颈加上HDFS瓶颈，并且会让真实世界的机器做多余的事情。 | 问题主要方面已经不是会像 `0` 一样造成故障，主要问题已经变成：更大的硬件资源消耗与更小的性能瓶颈。  |
| OpenEBS cStor / Ceph in Rook / Rancher Longhorn / nfs | ❌           | `3` | 比起上面的 `2` ，这样当然更不会有什么问题。唯一的问题就是，让机器白白做了更多的事情，最大可达的性能（即瓶颈）却要降低。并且，在这样的存储引擎下，你可以享用的灵活调度带来的自动防灾、自动恢复的能力，也被没有必要地舍弃了。                 | 除了与 `2` 相同的部分外，禁止跨节点调度带来的功能上的灵活性降低也会在这里出现。           |





## 运行时选择

### 不同的方案

有哪些方案

| num_code | desc               | 虚拟化 | 容器 | +示例 |
|----------|--------------------|-----|----|----
| 0        | 在裸金属上无隔离地部署        | 无   | 无  | 即裸机部署、不用容器 |
| 1        | 在裸金属上的虚拟机里无隔离地部署   | 有   | 无  | 把云虚机当正常主机使用、不用容器 |
| 2        | 在裸金属上直接作有隔离无虚拟化的部署 | 无   | 有  | 裸金属上编排容器，容器不是强隔离容器 |
| 3        | 在裸金属上作有虚拟化有隔离的部署   | 有   | 有  | 把docker（或者别的普通容器）换成kata,或在云虚机上再用k8s+普通容器 |

虚拟化又分轻量和标准两种。二者特点：

| 特点             | 轻量虚拟化                                                                                          | 标准虚拟化                                                                              |
|----------------|------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------|
| 隔离             | 强                                                                                              | 强                                                                                  |
| 启动速度           | 快                                                                                              | 慢                                                                                  |
| 运行消耗（虚拟化带来的部分） | 少                                                                                              | 多                                                                                  |
| 用到了该技术的实例      | `sysml/lightvm` `firecracker-microvm/firecracker` `openeuler-mirror/stratovirt` `Tinyx` `Kata` | `vmware` `qemu` `Xen` `KVM` `openeuler-mirror/stratovirt` `virtualBox` `Multipass` |

虚拟化和容器的特点：

| 特点        | 虚拟化   | 容器                                                           |
|-----------|-------|--------------------------------------------------------------|
| 隔离        | 强     | 弱                                                            |
| 进程外部可见    | 不可    | 可                                                            |
| 内核独立     | 有     | 无                                                            |
| 启动速度（理论上） | 较慢    | 较快                                                           |
| 安全        | 强     | 弱                                                            |
| 用到了该技术的实例 | （见上表） | `Kata` `iSula` `Docker` `Containerd` `Podman` `LXC` `OpenVZ` |

容器和虚拟机的区别：

- 容器启动的是一个操作系统（OS）：即把一个可用的操作系统去掉内核以及承载内核需要的一切剩下的部分。
- 虚拟机启动的是完整的硬件：轻量虚拟化和标准虚拟化的区别在于前者向上提供的硬件更少。


| 虚拟化接口          | Micro-VM       | Normal-VM      | Container   | Process       |
|----------------|----------------|----------------|-------------|---------------|
| CPU            | ⭕              | ⭕              | ❌           | ❌             |
| Flat Memory    | ⭕              | ❌              | ❌           | ❌             |
| OverLay Memory | ❌              | ⭕              | ❌           | ❌             |
| Virtio Device  | ⭕              | ⭕              | ❌           | ❌             |
| PCI System     | ❌              | ⭕              | ❌           | ❌             |
| Kernel         | ⭕              | ⭕              | ❌           | ❌             |
| OS (*nix)      | ⭕              | ⭕              | ⭕           | ❌             |
| Process (*nix) | ⭕              | ⭕              | ⭕           | ⭕             |
| e.g.           | （见上文【轻量虚拟化】字段） | （见上文【标准虚拟化】字段） | （见上文【容器】字段） | `wasm & wasi` |

### 不只是不同

> *`Docker: To be, or not to be …`*
> 

#### 进程 or OS

上面的 Process 字段和 Container 字段其实是可以融合的。这事儿要追溯到 Docker 创始人的那两条引起广泛讨论的推文，简单说就是，创始人先说 *如果当年就有如今 `wasm` 和 `wasi` 这俩玩意的话他的 Docker 根本就没有出现的必要* ，在被粉丝怼了后又补充到 *自己的意思不是说未来的 Docker 要被二者取代掉而是说未来不仅仅有 `Linux Container` 、 `Windows Container` 还会有 `WASM Container` ，而它们都可以被 Docker 管理* 。

……不过，身为容器运行时管理器的 `docker` 自己，倒是快要从 `kubernetes` 社区被踢出了，[详](https://kubesphere.com.cn/blogs/dockershim-out-of-kubernetes/)。并不是由于 `wasm` ，而是在于谷歌建立并主导的 Kubernetes 社区。其技术与管理角度的主要原因就在于，它的**接口上**的实现导致它同 `kubernetes` 交互的时候必须多一个虚拟层，叫 `docker-shim` 。这对性能有影响，更多是对 `kubernetes` 的维护有影响，因为这个 Shim 就是由后者的社区来维护的，而现在比起 Docker ， K8s 显然才是主流，因而它的社区当然也就不想再做这个维护了。

另外， Docker 也是一个比较重的容器，启动后的底噪（一些文章里有这个说法、我理解为除去镜像本身造成的消耗以外的消耗）会比较多，内存占用有 100MiB 左右，相比起来，欧拉的 StratoVirt 虽然是虚拟机，但却只有 4MiB （这个水平和 AWS 的 FireCracker 差不多且二者也都是虚拟机）。

> 说点别的：
> -—
> 所谓 *XX已死、XX永生* 的情况在信息技术领域的迭代里还挺常见的，比如 *Lisp 已死、 Lisp 永生* 或者现在也可以有 *Docker 已死、 Docker 永生* 。  
> 
> - POSIX 接口方面：在操作单个容器生命周期的命令行界面上，那些想要（且可能最终真的会）取代 Docker 的软件，它们的命令的使用（也就是 POSIX 接口）都多少被设计成了 Docker 的形状，其中 `podman` 甚至直接就是冲着 `alias docker=podman` 来做的。  
> - 别的方面：**未来的容器很可能不一定是一个（像现在的容器这样的）完整的OS，而是只有单一的进程（WASM进程）**。这带来的理论性能提升将是巨大的，并且隔离性也仍然具备。而这件事毕竟是 Docker 创始人先说出来的，谁知道这家公司以后会有啥转变呢。（另外这样一来， Java 一直想做但后来越发难以做好的那个梦想，就可以在 WASM 身上得以实现了。。。）  

总结： *进程和 OS 未来可能都会统一到容器之下而成为不同的细分的容器种类——毕竟 Docker 发明的镜像的概念确实特别好用（在交付的意义上）* 。

（其实 Docker 的容器最初被设计出来的时候就是要让容器当镜像使的。而且，不带依赖地把运行中的普通进程暂停并变为可持久化的数据这种事，应该是在 Docker 之前就可以做得到了）

#### 强隔离 or 轻量

有一个项目叫 Kata ，也是开源项目，它致力的目标就是 *比虚拟机更安全、比容器更轻快* ，其原理就是把容器放在了轻量级虚拟机里。其实这种做法在很多云上的实践里早就有了，只不过那些方案里的虚拟机一般都是标准虚拟机，会很重很重，而这个使用的是轻虚机，并且同容器部分作了整合。一般容器的架构，所有的彼此隔离的运行时要共享内核，这就意味着更差的隔离性，容器在宿主机仍需要内核的依赖，而依靠内核来攻击（从而使上层的隔离也实质上失去效果）也变成了可能（特别是内核比较旧的话）。

[参考](https://zhuanlan.zhihu.com/p/166305347)：

> 许多负责大规模容器运行的运维人员将容器“嵌套”在虚拟机中，从逻辑上将其与运行在同一主机上的其他进程隔离开，但在虚拟机中运行容器会丧失容器的速度和敏捷性优势。Intel 和 Hyper.sh（已加入蚂蚁集团）的开发人员意识到了这个问题，同时开始独立研发解决方案，两家公司都希望容器可以摆脱传统虚机的所有包袱，换言之，就是开发“面向云原生的虚拟化”技术：  
> 
> - 来自 Intel 开源技术中心的工程师在 Intel Clear Containers 项目中运用 Intel Virtualization Technology（Intel VT）技术来强化性能和安全隔离；  
> - 与此同时，Hyper.sh 的工程师采用相似的策略启动了开源项目 runV，将容器放在一个安全“沙箱”中，通过支持多种 CPU 架构和管理程序，侧重于开发技术中立的解决方案；  
> 
> 2017年，两家公司将项目合并，互为补充，创建了开源项目 Kata Containers。Intel 和 Hyper.sh 联合开发者社区，希望在各方的共同努力下，兼顾性能和兼容性，在为终端用户提供出色应用体验的同时，加速开发新的功能特性以满足未来新兴用例的需求。Kata Containers 成为 OpenStack 基金会（OSF）除 OpenStack 之外的首个托管项目，该项目于2017年12月在北美 KubeCon 上正式公开亮相，社区座右铭是“快如容器，稳似虚机”。 **其实质是，通过 Kata Containers 让每个容器/pod 采用其单独的内核，运行在一个轻量级的虚拟机中**。由于每个容器/pod 现在都运行在专属虚拟机中，恶意代码无法再利用共享内核来访问邻近的容器，因此，容器即服务(CaaS)供应商能够更安全的提供在裸金属上运行容器的服务。由于容器之间的硬件隔离，Kata Containers 允许互不信任的租户，**甚至生产应用及未经认证的生产应用程序都能在同一集群内安全运行**。  
> 

衍生了 FireCracker 、 StratoVirt 等轻虚机的 Rust-Vmm 也是在该社区的推动下建立的。而 rust-vmm 直接被 Kubernetes 调度也是可以做到的……

总结： *两种不同层级的隔离手段也早就可以融合起来——甚至比 Docker 还快、轻* 。

#### so ?

开头我并未写出【0】【1】【2】【3】四个方案，在功能或者要点上的分别什么情况。

因为这个没法说。按说，理论上，又要虚拟机的隔离、又要容器的包装，肯定比只是容器或只是虚机的消耗都要大的。然而，结合二者优点的实现已经存在在真实世界里了，甚至比【2】的一些旧的实现还要好不少。这基本都是归功于 rust-vmm 带来的高度的轻量化。

——被启动的是一个机器，但由于里面东西实在太少了，所以比一个不太精简的系统要快了不少。

## like Node, or like Process?

编排方式也可以有不同的选择：让一个基本编排单位像什么？

- 像原来的节点那样？
- 像原来的进程那样？

本文主要讨论在大规模分布式数据处理的场景下，我们可以怎样去选择。

*（PS. 由于在 Swarm 和 Mesos 相继因为各自的原因凉了以后，谷歌的 Kubernetes 已经没什么差别很大的竞品了。所以，这里就不像上面那样对编排系统的软件选型作出对比了……（ R.I.P. ） 不过，正所谓「败者成为余烬、胜者重燃」， Kubernetes 确实成了所谓的「事实标准」，但，同它有一样使用方式的**实现却可能完全不同**的东西，必然会相继诞生——且也早已有诞生，这也是为什么另外那两个会凉掉。）*

### to be ... , or ... ?

方案选择

#### 方案

**由于一个容器的启动就是一个OS的启动**（至少即便是在目前也是基本如此），因此，像如 ***一个容器（组）扮演一个虚拟节点*** 这样的笨拙一些的思路也是自然而然的。当然，**这样用的话其实很不符合 *容器* 被设计出来的初衷**。不过我想先从这里开始说。

Kubernetes 的基本编排单位是 *容器组* （又叫 Pod 。下文 Pod 都指 *容器组* ），也就是一组容器。那么，我们可以：

- 让其中的一个容器扮演原来的完整的单个OS的功能，并增加同组内的容器来辅助这个OS的一些工作。——下文称此为【方案一】。
- 让一个容器组扮演原本的单个OS，里面的每个组件都分别只是放在它们各自的容器中。——下文称此为【方案二】。
- 还有一种方案，是分布式应用的每个角色对应一个 StatefulSet 或 Deployment ，而每个角色的实例对应的就是这二者生成的 Pod 了。——下文称此为【方案三】。

其中：

| points   | 【方案一】                                                                                  | 【方案二】                                                                                                                                                                                                                                                                | 【方案三】                        |
|----------|----------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------|
| 应用之间的隔离性 | 差，容器内的一切都是无隔离地布置。                                                                      | 好，确保「该在一起的都在一起」的同时又有起码的隔离。                                                                                                                                                                                                                                           | 好（而且相对于【二】更加过头了）             |
| 资源复用的情况  | 同一个容器内的运行时依赖都是同一份，如果这个依赖是服务，则只需要运行一个就好。                                                | 虽然存储空间不会产生多余的占用（在镜像分层合理的情况下）、且依赖如果被设计为非常驻应用（即用一次创建一次进程的那种比如JVM）的话，倒是也不会比【一】多出什么资源的消耗，但问题是，很多现有的软件都是以常驻服务的形式成为另外软件的依赖的。                                                                                                                                               | 同【二】（但是会有相比【二】跟多（而且多余）的网络抽象） |
| 开发是否容易   | 比较容易，开发人员甚至不需要太深入地了解容器和编排系统的众多特性，只需要用其中一小部分就行了——毕竟这是个不错的懒人方案。                          | 不太容易。在密集IO的分布式数据处理场景，现有的解决方案是 Hadoop 生态的那些，而按照这个方案的原则来，相当于很多部分要被替换掉才行，比如Yarn和K8s的功能重叠问题。为此，**哪怕不改造（或甚至重新实现）现有的 Hadoop 生态——对K8s这样的新的编排系统的充分熟悉也一定是无法避免的**。                                                                                                             | 容易，介于【一】【二】之间。               |
| 架构是否合理   | 不合理，很不合理。分布式应用很多都自行实现了分布式，而这个方案就是为了省事儿的，如果费力去基于这个方案，想要把有功能重叠的东西（也就是多余的部分）剔除，则不如干脆换【二】。 | 相对于【一】合理不少。它能在确保数据本地性（就是通常说的 *移动数据不如移动计算* ）的同时（因为一个Pod里的所有容器一定是在同一个实际的物理节点上的），又能让这种分布式数据处理的工作具备新的优点：有隔离（**至少计算任务的进程和分布式软件本身的进程可以不用混在一块了从而能确保不安全的情况向外波及传播**）、操作与维护逻辑更加友好（至少不须再记忆都要在哪里配置什么能有什么效果而是用K8s统一的API去访问它们；各种终端前端的实现也就可以省心了）、上下游对接开发难度也能跟着下降（还是因为只需关心统一的API），等。 | 介于【一】【二】之间。                  |
| 迁移（节点）   | 同【二】                                                                                   | 这个设计一般是要一个Pod就在一个真实节点上的。它的迁移完全要基于Pod们内部的分布式应用对于容灾的支持。方便但不够灵活（一个Pod是基本调度单位所以这样一迁相当于迁一整个被扮演的节点。）                                                                                                                                                                       | 一个角色实例是一个Pod，因此迁移可以非常灵活      |
| 数据本地     | 天然支持（仅在存储类使用本地卷时）。                                                                     | 天然支持（仅在存储类使用本地卷时）。                                                                                                                                                                                                                                                   | 可以（仅在存储类使用本地卷时），但需要手动设计调度逻辑。 |

#### 【一】、【二】

我尝试过【方案一】，因为它是最简单的。需要自行制作镜像，镜像设计为要把 `init` 作 `1` 号进程使用，并基于此编写 K8s 的资源调度定义（YAML）。

我是用 CDH 作的尝试，部署两种 StatefulSet 类型的资源作为主从两种节点，其中从节点有多个副本，而一个Pod就扮演旧的分布式数据处理场景下的一个「节点」。**从节点的Pod只有一个容器，主节点的Pod有两个，第二个容器用来初始化主节点、并持续维护主节点HOSTS文件中对从节点的记录与主节点对从节点的免密SSH登录。**（初始化其实就是安装软件——这里其实就已经违背容器一般的使用方式了，因为我把软件的所有安装包单独放到了称之为「源」的镜像里，以镜像的方式管理所有软件包；而正常用法其实应该是，每个CDH中的分布式软件（如ZK、HDFS等）都要有自己的镜像、CDH的Manager也要有自己的镜像，启动的容器里应当是完成了安装工作的，**但这么弄就不是【方案一】而是【方案二】了**。）

——不过，这个尝试**仍然有意义**，那就是，如果我想要把某个分布式软件（如ZK、HDFS等）单独做出镜像来，无非是把【方案一】的我的尝试里的「整个CDH」给换成「其中某个分布式软件的开源版」就好了：节点间，差异部分放入调度定义（YAML）、相同部分放入镜像。——**重要的是，镜像里、我可以脱离包管理器来完成这个工作！——因此也不需再把 `init` 作 `1` 号进程了、并且也避免了使用时在生成容器中还要再执行安装命令。**——这刚好可以作为另外的方案的开端。（而且镜像制作上其实比【方案一】更简单些。。。）

我找到了一个叫 HokStack 的开源项目，它有点像【方案二】，然而当我安上它以后，处处都是错误，根本不能用，遂未再研究它。

#### 【二】、【三】

在 Helm 上，比较普遍的做法，似乎并不是这里的【一】或【二】的任何一个，而是【三】。其最大的困难在于数据本地性，但也不是不可实现：加入用某款**分布式文件系统**（HDFS/OZONE/MINIO/SEAWEEDFS）作**唯一且统一的**本地存储的抽象，并确保，**分布式计算引擎**（SPARK/FLINK/GLOW/GLEAM）可以很好地同这个文件系统交互，即配合编排系统（K8S）**能够确保切分后的小任务都被调度到它要用的数据的附近（就是数据可以通过尽可能少的网络转发——最好不需要通过网络转发）**（比如若一个SPARK的JOB容器是被插入到对应DATANODE的Pod内就算不需通过网络转发做「数据的移动」了）。

相对于【二】的优劣势的详细情况：

- 优势在于角色分布的灵活，分布式软件的装删管理也可以只是通过编排系统就可以完成，而不需额外实现什么。（甚至用上控制器以后安装和删除某分布式软件的运行实例只需要删除或创建特定KIND资源的配置了——**其配置复杂度被下降到了和K8SPod一样的层次！**）
- 劣势在于，不同容器组之间的交互，即便在同一个真实节点（或者虚机），也是仍要走网络的。但数据本地性仍然可以通过上面说的「DataNode 的 Pod 能够被插入 Job 容器」解决，所以其实是很小的劣势。

而不论【二】还是【三】，相对于【一】的优势，就是容器本身被设计出来的优势：因为有了隔离，所以可以尝试最轻量化的部署，**从而使用基于轻VM的强隔离的容器时，这轻量的优势也就能得以发挥了！**


### so How ?

*怎麽辦？*

#### 安装

*具体的大体步骤*

##### 一： Kubernetes

首先要有一个编排系统。

这里我用 [fanux/sealos](https://github.com/fanux/sealos) （版本是 `v3.3.9-rc.7` ）示例，因为这个工具可以一条命令就装一个 K8s 。这个作者还有一个叫 sealer 的项目，使用了全新的理念（把自己视为一个集群的 Docker 了属于是），不过版本号还比较低（目前是 `v0.5.2` ），故暂且还是用 sealos 做 Kubernetes 最初安装的工作。

##### 二： Kubesphare

Kubernetes 的熟悉成本是巨大的，而 [Kubesphare](https://kubesphere.io) 这个项目可以极大减小这个成本。它有一个界面，以及其它很多功能。这一层的代替品还可以有 Rancher 、 Kuboard 、 Dashboard ，前两者也是功能不错的作品。

另外，Kubesphare 的 [kubesphere/kubekey](https://github.com/kubesphere/kubekey) 项目提供的小工具 `kk` 也可以完成 *安装一个 Kubernetes* 的工作。通过这个命令行工具 `kk` ， Kubesphare 可以直接在裸金属上（或云虚机上）连同 Kubernetes 一同安装。

版本目前截止到 `3.2.1` ，目前在不使用 `kk` 安装 KubeSphare 时，需要已经有一个装好了 Kubernetes 的集群，并且其中已经配置好了**默认 StorageClass** 。

##### 三： Container

其实， *容器运行时* 的话，不管是用 `sealos` 还是用 `kk` 安装 Kubernetes 都会被一并安装。前者安装 `1.20.X` 以上版本的 Kubernetes 时会以 [containerd](https://github.com/containerd/containerd) 作为编排系统所管集群的运行时，须注意其中没有 `docker` 命令，相似的命令是 `nerdctl` 。

如果需要容器有强隔离性，可以安装 Kata 。**选用了【2】方案时强烈建议用 Kata** ，方案【3】也可以使用。对于前者， Kata 可以兼顾【2】的形式与【3】的隔离性；对于后者，在【3】的前提下再用 Kata ，多出来的一层轻量虚拟化也是代价小于收益的。

##### 四： plugins and so on

对于 `kubectl` 命令来说，只要有 `kubectl-xxx` 的命令可以在当前命令行有意义地执行，那么 `kubectl xxx` 这个 `kubectl` 的子命令就也能够执行。

这就是所谓的 *插件* 了，它是单节点安装的东西，并且是可以脱离容器的，因此安装插件需要务必确保其安全性。

还有一些不是插件但也要单节点安装的，上面提到过的 `sealos` 、 `kk` 、 `sealer` 这些，也是要单节点安装的，**可以是在容器内的（这取决于各软件对此的支持）**。这几个属于安装整个集群的，因此当然是单机使用的工具。还有比较有名的比如管理应用安装的，比如 `helm` ，也是单机的工具。

这些可以在需要用时再安装。

##### 五： Application

上面都是分布式编排系统的安装，下面就可以使用了。

安装应用就是一种使用。这里以【三】举例，具体的做法就是，通过 `helm` 安装某个应用的 Operator 从而获得新的控制器定义，效果就是添加了新的 KIND 。然后通过 `helm` 再应用使用了该 KIND 的模板。

Helm 的一个好处就是它可以达到一个原子化的效果，就是说，一个应用一般都会有多个部分（NS/SVC/STS/CRT/PVC/...），而它们中只要一个是可能失败的，就所有都不会被安装，从而避免出现安装一半的情况。（不完整的安装或不完整的卸载会造成毫无意义的资源浪费。）

另外，还有可以管理应用的工具，叫 [Kustomize](https://kustomize.io) 。它跟 Helm 不一样， Helm 只能通过拼接字符串的方式得到需要的 YAML （集群资源定义），而 Kustomize 则能够通过操作数据结构一样的方式去定义需要的 YAML （集群资源定义），而这个操作数据结构的手段依然是编写 YAML （这个是 Kustomize 标准的资源定义了）。 Helm 比较适合应用整体打包发布， Kustomize 更适合敏捷开发。（特定版本的 Kustomize 现在已经被封装到 Kubernetes 的 `kubectl` 的子命令中了。）

##### 六： apply

*当然，如果是第一次创建而不是更新某个资源定义，请还是使用 `kubectl` 的 `create` 子命令……*

一般来说， `kubectl apply -f file` 可以把内容中定义了 Kubernetes 资源的 `file` 在 Kubernetes 集群中加以应用。

其中 `file` 位置如果写成短横线，像这样： `kubectl apply -f -` ，那么这个内容将会从标准输入（ `/dev/stdin` ）读取——可以只是执行它，然后编写临时的资源定义对象的代码，然后按键盘 `^D` 表示完成编辑，刚刚临时编辑的资源定义就可以得到应用。

这是最普遍的应用方式。

资源定义一般使用 `yaml` 格式表示，也可以是 `json` 格式， Kubernetes 读取的 `yaml` 格式其实也是先转成 `json` 格式再识别。**要注意，这个转换逻辑每个地方可能都小有差别，这是 YAML 语言自身的问题（ [REF](https://github.com/voml/voml-language/blob/master/docs/cn/migrate/From%20YAML.md) ），所以即便遇到「语法明明没错但就是报错语法错误」了也是完全不奇怪的。**

一个 YAML 块，以 `---` 开头并以 `...` 结束，单个文件里如果只有一个块则都可以省略，有多个的时候可以省略 `...` 。

另外也可以使用 `kubectl apply -k dir` 来一键应用一个 Kustomize 。这也算是一种更合理的批量应用资源定义的方式。

##### 七： image

不论是 Docker 命令行，还是在编排系统编写资源定义，本质上都是**在定义怎么去使用镜像生成容器**。

所以，如何定义上面说到的 YAML （其实也包括了 Helm 会生成的 YAML ），根本上还是取决于被使用的镜像，到底是被设计成了**要被怎么使用**（或者说可被怎么使用）的。

如何制作镜像，在 KubeSphare 上应该有相应的工具链，命令行界面上可以通过镜像制作工具（比如 `docker` `nerdctl` 这些就能承担这个工作）去制作。

有了镜像，就能有使用镜像的方式，就能有 Kubernetes 资源定义；有多个这样的定义的话，就能抽取变化的部分来做成 Helm 模板或者 Kustomize 的 YAML ，就能按照设定好的使用方式去使用镜像了。

一般来说，一个应用，随部署场景变化最容易变化的部分，应当在 Helm 这一层被抽取，不会随此变化的部分应该加入镜像内部，而能够完成这个加入的，一般就是通过 KIND 为 ConfigMap 的 Kubernetes 资源了。

#### 使用

*概述各部分细节*

##### 0x00 镜像

要种地，就要先有地。要用镜像，就要先有镜像。

###### 图层

镜像（这里特指 Docker 这类容器管理概念下的镜像）是什么？

*可以想象一张图片，它有很多层，每层都有透明和不透明的部分，所有层叠加起来，每层不透明的都要把下层覆盖，然后从上往下就看到完整的图片。*

——而镜像就是这样一张完整图片。

访问一个镜像（👈为了使用）所需要的，实际上就是知道，它都有哪些图层、以及这图层们要有怎样的顺序。如此，这整体就可作为被访问镜像，如同从上往下看那张图一样，被「叠加着显示」出来了。

而存储一个镜像的**数据的部分**，实际也只需要：让所有图层都要有一个唯一标识，然后存储这镜像的那个图层的序列，就好了（除去这些还要有镜像的唯一标识等等）；**然后就只需要再存下所有的图层就行了**。

拉取镜像时，图层是真正被下载、存储的东西。如果同时拉去两个镜像用到了**共同的图层**，那么这个**真正在下载该图层的进程**实际上也只有（也只应有）**一个**。同样道理，如果对一个拉取中的镜像再执行拉取命令，实际上的下载也并不会就此变多。**拉取如此，存储也是**如此。不考虑副本策略的事情，一个图层，不论被复用多少次，也都只会被拉去一次、且存在一份。

以上，就是对 *容器镜像* 的一个重要特征的概述。它被 Docker 创新出来，而这也正是 Docker 的力量所在。 Docker 的开创性功绩就在于，它第一次解决了**一直没被广泛解决**的交付的问题。（不过这个说法应当是仅限于 \*nix 体系下的。去掉这个限制的话，这种解决方案的最初出现，应当正是 *闭包* 最初出现的时候。）

具体在你安装好的 `docker` 软件的实现上，每一层其实都被放在特定路径下的特定的文件夹里头，这文件夹名字就是一个「图层」的 *哈希* （ HASH Code ），同时这也是该图层的唯一标识。

镜像被启动后，就会成为容器，对应原先的程序（一个文件）和进程（一个被加载进内存和CPU上的进行中的工作）。**容器比镜像多 *一层* ，叫「可写层」**，实如其名，镜像的各个「图层」是**只读、不可变**的，被启动的容器表现为一个 *操作系统* （OS），操作系统一定是有状态的，在原镜像基础上**变化的那部分持久化**状态，就会被放在这个叫「可写层」的地方。实现上，就像镜像的图层在文件夹里，容器比起所用镜像的变化部分，也在一个文件夹里，文件夹的名称就是容器的唯一标识，其中也就是用来存放前面提到的「可写层」里的内容了。

容器可以被提交为镜像。而这个提交，本质上就是：

1. 这个可写层，要视为一个整体来得到它的唯一标识、唯一标识会是啥样**完全取决于内容**会是啥样（这就是 *哈希* ）；
2. 然后带着这个唯一标识，该层所有数据都被复制一份，到镜像图层们的路径下的它该在的那个文件夹里头；
3. 同时新增一个镜像的记录，其表示内容的部分就是，旧镜像的 *图层标识序列* 上再加上一条刚刚新增的 *图层标识* 后所形成的一个新的序列。

###### 命令

命令的话，大概像这样：

- 使用配置文件（类似于 `makefile` 的 `Dockerfile` ）创建镜像： `docker build` ，它对应的就是还没 Docker 之前的 `make` 命令。
- 使用镜像创建容器： `docker create` 、 `docker run` 等，对应于还没 Docker 之前，就是使用程序创建进程（也就是通俗说的跑起一个程序）。
- 直接调试或改变运行中的容器： `docker exec` 、 `docker cp` 等，对应于还没 Docker 之前这个不太好对应，这就好比是你想办法把一个运行中的进程直接编辑了，也就是编辑了已经被加载进内存里的内容……
- 提交一个容器为镜像： `docker commit` ，就像上一条一样不好对应，这个就好比，你不光把一个进程直接编辑了，你还把这个运行在半途的程序暂停并变成了文件，然后你又要拿这个文件当你做好的程序到处去用……（不过真别说，有个叫 CRIU 的开源软件就是做这个事儿的……后来它被 OpenVZ 、 LXC 、 Docker 等软件所使用（集成），也在社区的帮助下获得了巨大的发展。）

除此之外还有很多，这些都是 *子命令* 。

想看 `docker` 都有哪些子命令就执行 `docker --help` ，想看子命令怎么用（包括它还有没有子子命令），就 `docker 子命令 --help` 。对于 `kubectl` 、 `nerdctl` 、 `sealos` 等，皆同理。（前面提到过的 `kubectl` 的插件其实就是它被增加的子命令。）

另外，从上面的对应可以看出来，最妥当的镜像**创建**方式，自然也是用 `build` 子命令了。 `exec` 和 `commit` 这俩子命令，虽然是另一种创建的手段，但比起 `build` 还是不那么合适的，因为它不能留下标准的过程，中间也其实是不可控的，与其说是创建镜像的方式，不如说是对已存在的容器的丰富使用罢了，它就好比通过改造运行中的进程的内容并重新保存成文件，然后用这个行为上类似于「运维」的工作，来替代通常被称为「编程」的那个工作——而之所以，工业实践中没有多少这样做的，就是因为，它不能保证自己只是做了自己要做的改变、也不能留下关于「到底都发生了哪些改变」的全部信息。

*（不过，反过来说，如果能够保证留下所有改变的记录且改变是完全可控的，这种直接改变运行实例的「好像运维一样」的工作就可以被称之为「编程」了，而它也是更好用的。这其实就是基于原型的面向对象和基于类的面向对象的设计区别，后者的死板为很多不麻烦的事带来不必要的麻烦（设计模式）、而前者比后者好用（这需要功能也成为第一类数据（即 first-class ））。）*

##### 0x01 容器

容器是用镜像加上各项可配参数创建，就好像进程是程序加上参数和环境所创建。容器就是运行中的镜像。

上面已经说过一些对应关系，主要的就是，镜像对应程序文件、容器对应「跑起来的程序」。

*「跑起来的程序」是通俗但模糊的说法，它应该叫进程。执行程序（文件）其实就是用它创建一个进程**实例**。进程就是用程序创建的实例。（「实例」这两个字可以结合譬如 Java 里的「实例」也即是「对象」来理解—— \*nix 系统上的一个可执行文件，其实也就类似于 Java 里的一个 Class 。）*

上面说的只是**对应关系**，我也可以说镜像对应 Java 的类文件、容器对应 Java 的对象实例，这是用来体现镜像和容器是啥关系的描述，并不是说镜像就是类容器就是对象，毕竟至少 Docker 都不是用 Java 制作的。。。

而如果说等价关系的话，**容器并不等价于一般的 Linux 进程**，就好像，**镜像也不等于一般的 Linux 可执行文件**。相比而言，容器或镜像都是更加丰富的，容器不仅仅有内存中的状态，也会有容器内部的持久化的状态（可写层），这是任何 Linux 进程都不能够有的，而镜像里也不只有可执行文件，它是**一大堆东西**，只是**被容器管理软件（如 Docker ）给封装成了单个的东西**，从而能够被**像操作单个文件一样操作单个镜像整体**。

###### 后台（ `-d` ）

在 Linux 里面，其实没有比较完整的让进程以「后台」这样的状态去运行的办法。有一个结尾符是 `&` ，它和 `;` （默认）的区别就在于它会让被它结束的整体以「后台」这种模式运行。**但这个进程依然是当前 SHell 的 *子进程* ，并且因而也能够通过 `jobs` 命令看到。最难受的其实是它的标准输出标准错误就是用当前进程的，这就会造成命令行撕裂**。

——可以通过 `disown -r` 把自己运行中的后台进程的父进程向当前进程的父进程方向交过去，也可以在执行时就通过 `(xxx xx &)` 的写法让 `xxx xx` 命令所创建的进程的父进程直接就是系统进程（进程号 `1` ），另外还有用 `nohup` 或 `screen` 管理后台进程的办法。

**但这都不是太好的办法，我们只想让「后台」的东西同时跑着、又能够检查到它的状态、又不需要为它的标准输出标准错误放在哪儿这种管理文件的标准感到困扰**，所幸，在启动容器（基于特定的方式「执行了镜像」从而创建了容器）的时候，这种事情可以得到统一的解决：只需要在 `run` 子命令后加上 `-d` 选项，它就会被以后台的形式启动，**并且可以直接通过 `logs` 子命令去查看被启动进程的标准输出标准错误、而且它还不会因为当前 SHell 的关闭而被一并关闭！可以说是放后台这个操作需要达到的目的都能很好达到了**。

而且，在这条命令后面再加上 `&` 也没关系，这也只不过是令一个启动目标进程的启动器进程在当前 SHell 的后台（ `jobs` 可看到的那里）执行而已，但这个进程也只是通知容器管理软件，最终做事的是与当前 SHell 没有父子进程关系的容器管理软件。

**如果不是强隔离的容器（譬如 Kata 那种），容器内的进程在容器外就也是可见的。**

###### 交互 （ `-t` `-i` ）

在一般的 \*nix SHell 上，都会有管道（ `|` ）这么个东西。每一个被启动的进程，也都有其**各自**的 *标准输入* 、 *标准输出* 、 *标准错误* 。而在 \*nix SHell 上，管道（指匿名管道）就是用于把一个进程的标准输出对接另一个进程的标准输入的工具，它让前者的标准输出接口产生的 *数据* 直接被交给后者的标准输入接口。

Bash 可以被以不同模式启动，如果只是执行 `bash` ，然后就可以**不断重复**一个 `输入命令 -> 等待执行 -> 执行完毕 -> 取得命令提示符然后又能输入命令` 的流程的话，这就是 *交互式* 模式了。不是这个模式的话，你不会看到命令提示符（一般是箭头的形状或者一对中括号总之是各种形状的一个用来提示你可以继续输入命令的**打印**）， `bash` 进程会读取你给它指定的脚本内容（可能是文件也可能是 `-c` 直接读取字符串）并**自行**一条条接执行下去，**直到没有可执行的内容了就关闭自己**。

在一般使用 `bash` 的时候，交互模式还是非交互模式启动这个 `bash` 进程是被自动判断的，但启动容器的时候，这件事就需要手动指定了，使用的选项就是 `-t` 以打开交互模式和 `-i` 以**将被启动进程的标准输入对接到外面的命令的标准输入上**。

*「 `xxx` 容器」意思就是「以 `xxx` 镜像创建的容器」。就像「 `xxx` 进程」意思就是「以 `xxx` 程序（命令）创建的进程」。**另外容器不是单纯的进程**这点上面也提到过了。*

示例：

*下面的命令请自行补充 `--rm` 选项，为了突出重点我就不写了。这选项是指定容器若停止就自动删除。**否则你会看到停止的容器还都在那里。。。而它们的每个可写层至少要占用宿主机目录上的一个文件夹**。*

这是一个启动 `busybox` 容器并在该容器内创建 `cat` 进程的命令（没被放后台）：

~~~ sh
docker run -- busybox cat
~~~

它当然不会有任何效果。如果后面是 `cat /etc/hosts` 的话倒是会显示出容器内这个文件的内容。不过这不是我们要做的。

现在请试下这个：

~~~ sh
docker run -it -- busybox cat
~~~

然后随便输入什么，回车，你会发现，字符界面又把你刚刚那一行多打印了一次。

**然后它会等待你继续输入**。这就是因为，交互模式被通过 `-t` 打开了；而你的输入之所以能够被这个 `busybox` 容器**内**的 `cat` 进程**接收得到**，就是因为在 `run` 子命令后有 `-i` 选项存在。

下面可以通过这些例子，具体了解一下二者分别的效果：

- `echo mudada | docker run -i -- busybox cat` ：借助这容器内的 `cat` 进程打印一下 `mudada` 。（这个效果不能在后台容器身上达成）
- `docker run --name tdcat -td -- busybox cat` ：这会启动一个后台运行的容器，其内部的 `1` 号进程是 `cat` 。不主动再开，里面就会只有这一个进程，**这也会导致容器的持续运行**，因为内部的 `cat` 进程是以交互模式启动，会持续等待内容输入——然而却什么都等不到，因为并没有 `-i` ，故不存在途径把数据传递给这个 `cat` 进程的标准输入接口（因为这个接口并没有给予途径使之可被访问）。（如果不是后台模式的话就能直观验证一下里面的进程能否对你的输入有所反应了）


###### 管理容器

如果容器内没有进程，容器本身也就不是运行的了。管理容器的工作，可以视为管理容器内进程的工作。
