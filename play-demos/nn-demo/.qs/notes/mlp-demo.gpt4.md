## Axon Demo (GPT-4)

> (10) ç»™æˆ‘ä¸€ä¸ª Axon ç¤ºä¾‹

ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ **Elixir + Axon** æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹çš„ç®€å•ç¤ºä¾‹ï¼Œå±•ç¤ºäº†å¦‚ä½•å®šä¹‰æ¨¡å‹ç»“æ„ã€åˆå§‹åŒ–å‚æ•°å¹¶è¿›è¡Œå‰å‘æ¨ç†ã€‚

---

### ğŸ§  Axon ç¤ºä¾‹ï¼šæ„å»ºä¸€ä¸ªç®€å•çš„å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰

```elixir
# å¼•å…¥ Axon å’Œ Nx åº“
Mix.install([
  {:axon, "~> 0.7.0"},
  {:nx, "~> 0.6.0"}
])

# å®šä¹‰æ¨¡å‹ç»“æ„
model =
  Axon.input("input", shape: {nil, 784}) # è¾“å…¥å±‚ï¼Œ784ç»´ï¼ˆå¦‚ MNIST å›¾åƒï¼‰
  |> Axon.dense(128, activation: :relu) # ç¬¬ä¸€å±‚å…¨è¿æ¥ + ReLU
  |> Axon.batch_norm()                  # æ‰¹å½’ä¸€åŒ–
  |> Axon.dropout(rate: 0.5)            # Dropout é˜²æ­¢è¿‡æ‹Ÿåˆ
  |> Axon.dense(64, activation: :tanh)  # ç¬¬äºŒå±‚å…¨è¿æ¥ + Tanh
  |> Axon.dense(10)                     # è¾“å‡ºå±‚ï¼ˆ10ç±»ï¼‰
  |> Axon.activation(:softmax)         # Softmax æ¿€æ´»ç”¨äºåˆ†ç±»

# åˆå§‹åŒ–æ¨¡å‹å‚æ•°
params = Axon.init(model, Nx.random_uniform({1, 784}))

# æ‰§è¡Œå‰å‘æ¨ç†
output = Axon.predict(model, params, Nx.random_uniform({1, 784}))

IO.inspect(output)
```

---

### ğŸ“Œ è¯´æ˜

- `Axon.input/2`ï¼šå®šä¹‰è¾“å…¥å¼ é‡ï¼Œ`nil` è¡¨ç¤ºæ‰¹å¤§å°åŠ¨æ€ã€‚
- `Axon.dense/2`ï¼šå®šä¹‰å…¨è¿æ¥å±‚ã€‚
- `Axon.activation/2`ï¼šæ·»åŠ æ¿€æ´»å‡½æ•°ã€‚
- `Axon.init/2`ï¼šåˆå§‹åŒ–æ¨¡å‹å‚æ•°ã€‚
- `Axon.predict/3`ï¼šæ‰§è¡Œå‰å‘æ¨ç†ã€‚

---

### ğŸ“Š å¯è§†åŒ–æ¨¡å‹ç»“æ„ï¼ˆå¯é€‰ï¼‰

```elixir
Axon.Display.as_table(model, Nx.template({1, 784}, :f32)) |> IO.puts()
```

è¿™å°†è¾“å‡ºä¸€ä¸ªè¡¨æ ¼ï¼Œå±•ç¤ºæ¯ä¸€å±‚çš„è¾“å…¥è¾“å‡ºå½¢çŠ¶ã€å‚æ•°æ•°é‡ç­‰ä¿¡æ¯ã€‚

---

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº† Axon çš„å£°æ˜å¼å»ºæ¨¡é£æ ¼ï¼Œç±»ä¼¼äº Keras æˆ– PyTorch Lightningï¼Œä½†åŸºäº Elixir çš„å¹¶å‘å’Œå‡½æ•°å¼ç‰¹æ€§æ„å»ºã€‚å¦‚æœä½ å¸Œæœ›æˆ‘å¸®ä½ æ‰©å±•è¿™ä¸ªç¤ºä¾‹ï¼Œæ¯”å¦‚æ·»åŠ è®­ç»ƒå¾ªç¯æˆ–åŠ è½½æ•°æ®é›†ï¼Œä¹Ÿå¯ä»¥ç»§ç»­å‘Šè¯‰æˆ‘ï¼


## PyTorch Demo (GPT-4)

> (11) æˆ‘èƒ½ä»¥è¿™ç§ç»“æ„æ¥åœ¨ pytorch æ„å»ºæ¨¡å‹å—

æ˜¯çš„ï¼Œä½ å¯ä»¥åœ¨ PyTorch ä¸­ä»¥ç±»ä¼¼ Axon çš„ç»“æ„åŒ–æ–¹å¼æ¥æ„å»ºæ¨¡å‹ã€‚è™½ç„¶ PyTorch æœ¬èº«æ˜¯é¢å‘å¯¹è±¡çš„ï¼ˆé€šè¿‡ç»§æ‰¿ `nn.Module`ï¼‰ï¼Œä½†ä½ å¯ä»¥ä½¿ç”¨ä¸€äº›é«˜çº§å°è£…æˆ–å‡½æ•°å¼é£æ ¼æ¥æ¨¡æ‹Ÿ Axon çš„å£°æ˜å¼å»ºæ¨¡æ–¹å¼ã€‚

---

### âœ… PyTorch ä¸­æ¨¡æ‹Ÿ Axon é£æ ¼çš„æ–¹å¼

#### 1. **ä½¿ç”¨ Sequential æ„å»ºæ¨¡å‹**

ç±»ä¼¼ Axon çš„é“¾å¼ç»“æ„ï¼š

```python
import torch
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(784, 128),
    nn.ReLU(),
    nn.BatchNorm1d(128),
    nn.Dropout(0.5),
    nn.Linear(128, 64),
    nn.Tanh(),
    nn.Linear(64, 10),
    nn.Softmax(dim=1)
)
```

#### 2. **ä½¿ç”¨å‡½æ•°å¼ APIï¼ˆæ¨èç»“åˆ PyTorch Lightning æˆ– TorchFXï¼‰**

ä½ å¯ä»¥å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥æ„å»ºæ¨¡å‹ç»“æ„ï¼š

```python
def build_model():
    layers = [
        nn.Linear(784, 128),
        nn.ReLU(),
        nn.BatchNorm1d(128),
        nn.Dropout(0.5),
        nn.Linear(128, 64),
        nn.Tanh(),
        nn.Linear(64, 10),
        nn.Softmax(dim=1)
    ]
    return nn.Sequential(*layers)

model = build_model()
```

#### 3. **ä½¿ç”¨ PyTorch Lightning ç®€åŒ–è®­ç»ƒæµç¨‹**

```python
import pytorch_lightning as pl

class LitModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.model = build_model()
        self.loss_fn = nn.CrossEntropyLoss()

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.loss_fn(logits, y)
        return loss

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=1e-3)
```

---

### ğŸ§  æ€»ç»“å¯¹æ¯”

| ç‰¹æ€§ | Axon | PyTorch |
|------|------|---------|
| æ„å»ºæ–¹å¼ | å‡½æ•°å¼é“¾å¼ç»“æ„ | é¢å‘å¯¹è±¡æˆ–å‡½æ•°å¼ |
| çµæ´»æ€§ | é«˜ | é«˜ |
| å¯è¯»æ€§ | é«˜ | é«˜ï¼ˆä½¿ç”¨ `Sequential` æˆ–å‡½æ•°å°è£…ï¼‰ |
| è®­ç»ƒå°è£… | å†…ç½®è®­ç»ƒ API | å¯ç”¨ Lightning ç®€åŒ– |

---

å¦‚æœä½ å¸Œæœ›æˆ‘å¸®ä½ æŠŠ Axon ç¤ºä¾‹å®Œæ•´è½¬æ¢æˆ PyTorch Lightning é¡¹ç›®ç»“æ„ï¼Œä¹Ÿå¯ä»¥å‘Šè¯‰æˆ‘ä½ å¸Œæœ›æ”¯æŒå“ªäº›åŠŸèƒ½ï¼ˆæ¯”å¦‚è®­ç»ƒã€éªŒè¯ã€æ¨ç†ã€ä¿å­˜æ¨¡å‹ç­‰ï¼‰ã€‚æ˜¯å¦éœ€è¦æˆ‘ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„ PyTorch é¡¹ç›®æ¨¡æ¿ï¼Ÿ

